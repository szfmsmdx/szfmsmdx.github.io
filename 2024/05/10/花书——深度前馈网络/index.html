<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 7.1.1">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">

<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext">

<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.1/css/all.min.css" integrity="sha256-wiz7ZSCn/btzhjKDQBms9Hx4sSeUYsDrTLg7roPstac=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"szf.cool","root":"/","images":"/images","scheme":"Gemini","darkmode":false,"version":"8.19.2","exturl":false,"sidebar":{"position":"left","display":"hide","padding":12,"offset":12},"copycode":{"enable":false,"style":null},"fold":{"enable":false,"height":500},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":true,"nav":null,"count":true,"text":true},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"},"path":"/search.xml","localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false}}</script><script src="/js/config.js"></script>

    <meta name="description" content="研读花书，皆为拙记，如有错误还望各位不惜笔墨，不啬赐教。本节主要包括深度前馈网络部分内容">
<meta property="og:type" content="article">
<meta property="og:title" content="花书——深度前馈网络">
<meta property="og:url" content="http://szf.cool/2024/05/10/%E8%8A%B1%E4%B9%A6%E2%80%94%E2%80%94%E6%B7%B1%E5%BA%A6%E5%89%8D%E9%A6%88%E7%BD%91%E7%BB%9C/">
<meta property="og:site_name" content="Szf&#39;s blog ^ ^">
<meta property="og:description" content="研读花书，皆为拙记，如有错误还望各位不惜笔墨，不啬赐教。本节主要包括深度前馈网络部分内容">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://szfmsmdx.oss-cn-hangzhou.aliyuncs.com/blog_pic/202405131401989.png">
<meta property="og:image" content="https://szfmsmdx.oss-cn-hangzhou.aliyuncs.com/blog_pic/202405141144738.png">
<meta property="og:image" content="https://szfmsmdx.oss-cn-hangzhou.aliyuncs.com/blog_pic/202405141144042.png">
<meta property="og:image" content="https://szfmsmdx.oss-cn-hangzhou.aliyuncs.com/blog_pic/202405141145424.png">
<meta property="og:image" content="https://szfmsmdx.oss-cn-hangzhou.aliyuncs.com/blog_pic/202405141214607.png">
<meta property="og:image" content="https://szfmsmdx.oss-cn-hangzhou.aliyuncs.com/blog_pic/202405161548317.png">
<meta property="og:image" content="https://szfmsmdx.oss-cn-hangzhou.aliyuncs.com/blog_pic/202405161548922.png">
<meta property="og:image" content="https://szfmsmdx.oss-cn-hangzhou.aliyuncs.com/blog_pic/202405161617141.png">
<meta property="og:image" content="https://szfmsmdx.oss-cn-hangzhou.aliyuncs.com/blog_pic/202405161617068.png">
<meta property="og:image" content="https://szfmsmdx.oss-cn-hangzhou.aliyuncs.com/blog_pic/202405161619644.png">
<meta property="og:image" content="https://szfmsmdx.oss-cn-hangzhou.aliyuncs.com/blog_pic/202405161644450.png">
<meta property="og:image" content="https://szfmsmdx.oss-cn-hangzhou.aliyuncs.com/blog_pic/202405161745844.png">
<meta property="og:image" content="https://szfmsmdx.oss-cn-hangzhou.aliyuncs.com/blog_pic/202405161747716.png">
<meta property="og:image" content="https://szfmsmdx.oss-cn-hangzhou.aliyuncs.com/blog_pic/202405161748140.png">
<meta property="og:image" content="https://szfmsmdx.oss-cn-hangzhou.aliyuncs.com/blog_pic/202405161805492.png">
<meta property="og:image" content="https://szfmsmdx.oss-cn-hangzhou.aliyuncs.com/blog_pic/202405161810183.png">
<meta property="og:image" content="https://szfmsmdx.oss-cn-hangzhou.aliyuncs.com/blog_pic/202405161812666.png">
<meta property="og:image" content="https://szfmsmdx.oss-cn-hangzhou.aliyuncs.com/blog_pic/202405161825716.png">
<meta property="og:image" content="https://szfmsmdx.oss-cn-hangzhou.aliyuncs.com/blog_pic/202405161825012.png">
<meta property="og:image" content="https://szfmsmdx.oss-cn-hangzhou.aliyuncs.com/blog_pic/202405161834171.png">
<meta property="og:image" content="https://szfmsmdx.oss-cn-hangzhou.aliyuncs.com/blog_pic/202405161834812.png">
<meta property="article:published_time" content="2024-05-10T03:46:21.000Z">
<meta property="article:modified_time" content="2024-05-18T05:33:14.048Z">
<meta property="article:author" content="Szfmsmdx">
<meta property="article:tag" content="Deep Learning">
<meta property="article:tag" content="花书">
<meta property="article:tag" content="机器学习">
<meta property="article:tag" content="前馈神经网络">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://szfmsmdx.oss-cn-hangzhou.aliyuncs.com/blog_pic/202405131401989.png">


<link rel="canonical" href="http://szf.cool/2024/05/10/%E8%8A%B1%E4%B9%A6%E2%80%94%E2%80%94%E6%B7%B1%E5%BA%A6%E5%89%8D%E9%A6%88%E7%BD%91%E7%BB%9C/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"zh-CN","comments":true,"permalink":"http://szf.cool/2024/05/10/%E8%8A%B1%E4%B9%A6%E2%80%94%E2%80%94%E6%B7%B1%E5%BA%A6%E5%89%8D%E9%A6%88%E7%BD%91%E7%BB%9C/","path":"2024/05/10/花书——深度前馈网络/","title":"花书——深度前馈网络"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>花书——深度前馈网络 | Szf's blog ^ ^</title>
  







<link rel="dns-prefetch" href="comment.szf.cool">
  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
<style>.darkmode--activated{--body-bg-color:#282828;--content-bg-color:#333;--card-bg-color:#555;--text-color:#ccc;--blockquote-color:#bbb;--link-color:#ccc;--link-hover-color:#eee;--brand-color:#ddd;--brand-hover-color:#ddd;--table-row-odd-bg-color:#282828;--table-row-hover-bg-color:#363636;--menu-item-bg-color:#555;--btn-default-bg:#222;--btn-default-color:#ccc;--btn-default-border-color:#555;--btn-default-hover-bg:#666;--btn-default-hover-color:#ccc;--btn-default-hover-border-color:#666;--highlight-background:#282b2e;--highlight-foreground:#a9b7c6;--highlight-gutter-background:#34393d;--highlight-gutter-foreground:#9ca9b6}.darkmode--activated img{opacity:.75}.darkmode--activated img:hover{opacity:.9}.darkmode--activated code{color:#69dbdc;background:0 0}button.darkmode-toggle{z-index:9999}.darkmode-ignore,img{display:flex!important}.beian img{display:inline-block!important}</style></head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">Szf's blog ^ ^</p>
      <i class="logo-line"></i>
    </a>
      <p class="site-subtitle" itemprop="description">Yesterday you said tomorrow...</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="搜索" role="button">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup"><div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off" maxlength="80"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close" role="button">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div class="search-result-container no-result">
  <div class="search-result-icon">
    <i class="fa fa-spinner fa-pulse fa-5x"></i>
  </div>
</div>

    </div>
  </div>

</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%AD%A6%E4%B9%A0%E5%BC%82%E6%88%96xor"><span class="nav-number">1.</span> <span class="nav-text">学习异或（XOR）</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%9F%BA%E4%BA%8E%E6%A2%AF%E5%BA%A6%E7%9A%84%E5%AD%A6%E4%B9%A0"><span class="nav-number">2.</span> <span class="nav-text">基于梯度的学习</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BB%A3%E4%BB%B7%E5%87%BD%E6%95%B0"><span class="nav-number">2.1.</span> <span class="nav-text">代价函数</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BD%BF%E7%94%A8%E6%9C%80%E5%A4%A7%E4%BC%BC%E7%84%B6%E5%AD%A6%E4%B9%A0%E6%9D%A1%E4%BB%B6%E5%88%86%E5%B8%83"><span class="nav-number">2.1.1.</span> <span class="nav-text">使用最大似然学习条件分布</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%AD%A6%E4%B9%A0%E6%9D%A1%E4%BB%B6%E7%BB%9F%E8%AE%A1%E9%87%8F"><span class="nav-number">2.1.2.</span> <span class="nav-text">学习条件统计量</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%BE%93%E5%87%BA%E5%8D%95%E5%85%83"><span class="nav-number">2.2.</span> <span class="nav-text">输出单元</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%94%A8%E4%BA%8E%E9%AB%98%E6%96%AF%E8%BE%93%E5%87%BA%E5%88%86%E5%B8%83%E7%9A%84%E7%BA%BF%E6%80%A7%E5%8D%95%E5%85%83"><span class="nav-number">2.2.1.</span> <span class="nav-text">用于高斯输出分布的线性单元</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%94%A8%E4%BA%8Ebernoulli%E4%BC%AF%E5%8A%AA%E5%88%A9%E8%BE%93%E5%87%BA%E5%88%86%E5%B8%83%E7%9A%84sigmoid%E5%8D%95%E5%85%83"><span class="nav-number">2.2.2.</span> <span class="nav-text">用于Bernoulli(伯努利)输出分布的sigmoid单元</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%94%A8%E4%BA%8E-multinoulli-%E8%BE%93%E5%87%BA%E5%88%86%E5%B8%83%E7%9A%84-softmax-%E5%8D%95%E5%85%83"><span class="nav-number">2.2.3.</span> <span class="nav-text">用于 Multinoulli
输出分布的 softmax 单元</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E9%9A%90%E8%97%8F%E5%8D%95%E5%85%83"><span class="nav-number">2.3.</span> <span class="nav-text">隐藏单元</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#relu%E5%8F%8A%E5%85%B6%E6%89%A9%E5%B1%95"><span class="nav-number">2.3.1.</span> <span class="nav-text">ReLU及其扩展</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#logistic-sigmoid%E4%B8%8E%E5%8F%8C%E6%9B%B2%E6%AD%A3%E5%88%87%E5%87%BD%E6%95%B0"><span class="nav-number">2.4.</span> <span class="nav-text">logistic
sigmoid与双曲正切函数</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%85%B6%E4%BB%96%E9%9A%90%E8%97%8F%E5%8D%95%E5%85%83"><span class="nav-number">2.5.</span> <span class="nav-text">其他隐藏单元</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E6%9E%B6%E6%9E%84%E8%AE%BE%E8%AE%A1"><span class="nav-number">3.</span> <span class="nav-text">架构设计</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%B8%87%E8%83%BD%E8%BF%91%E4%BC%BC%E6%80%A7%E8%B4%A8%E5%92%8C%E6%B7%B1%E5%BA%A6"><span class="nav-number">3.1.</span> <span class="nav-text">万能近似性质和深度</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%85%B6%E4%BB%96%E6%9E%B6%E6%9E%84%E4%B8%8A%E7%9A%84%E8%80%83%E8%99%91"><span class="nav-number">3.2.</span> <span class="nav-text">其他架构上的考虑</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E5%92%8C%E5%85%B6%E4%BB%96%E7%9A%84%E5%BE%AE%E5%88%86%E7%AE%97%E6%B3%95"><span class="nav-number">4.</span> <span class="nav-text">反向传播和其他的微分算法</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%AE%A1%E7%AE%97%E5%9B%BE"><span class="nav-number">4.1.</span> <span class="nav-text">计算图</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%BE%AE%E7%A7%AF%E5%88%86%E4%B8%AD%E7%9A%84%E9%93%BE%E5%BC%8F%E6%B3%95%E5%88%99"><span class="nav-number">4.2.</span> <span class="nav-text">微积分中的链式法则</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E9%80%92%E5%BD%92%E5%9C%B0%E4%BD%BF%E7%94%A8%E9%93%BE%E5%BC%8F%E6%B3%95%E5%88%99%E5%AE%9E%E7%8E%B0%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD"><span class="nav-number">4.3.</span> <span class="nav-text">递归地使用链式法则实现反向传播</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%85%A8%E8%BF%9E%E6%8E%A5-mlp-%E4%B8%AD%E7%9A%84%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD"><span class="nav-number">4.4.</span> <span class="nav-text">全连接 MLP 中的反向传播</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%AC%A6%E5%8F%B7%E5%88%B0%E7%AC%A6%E5%8F%B7%E7%9A%84%E5%AF%BC%E6%95%B0"><span class="nav-number">4.5.</span> <span class="nav-text">符号到符号的导数</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%B8%80%E8%88%AC%E5%8C%96%E7%9A%84%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD"><span class="nav-number">4.6.</span> <span class="nav-text">一般化的反向传播</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BB%A5mlp%E4%B8%BA%E4%BE%8B%E7%9A%84%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD"><span class="nav-number">4.7.</span> <span class="nav-text">以MLP为例的反向传播</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%85%B6%E4%BB%96%E9%97%AE%E9%A2%98"><span class="nav-number">4.8.</span> <span class="nav-text">其他问题</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%95%8C%E4%BB%A5%E5%A4%96%E7%9A%84%E5%BE%AE%E5%88%86"><span class="nav-number">4.9.</span> <span class="nav-text">深度学习界以外的微分</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E9%AB%98%E9%98%B6%E5%BE%AE%E5%88%86"><span class="nav-number">4.10.</span> <span class="nav-text">高阶微分</span></a></li></ol></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Szfmsmdx"
      src="/images/avatar.jpg">
  <p class="site-author-name" itemprop="name">Szfmsmdx</p>
  <div class="site-description" itemprop="description">To be a pure man</div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">17</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">5</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">29</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author animated">
      <span class="links-of-author-item">
        <a href="https://github.com/szfmsmdx" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;szfmsmdx" rel="noopener me" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:szfmsmdx@163.com" title="E-Mail → mailto:szfmsmdx@163.com" rel="noopener me" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>
  <div class="cc-license animated" itemprop="license">
    <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/zh-CN" class="cc-opacity" rel="noopener" target="_blank"><img src="https://cdnjs.cloudflare.com/ajax/libs/creativecommons-vocabulary/2020.11.3/assets/license_badges/small/by_nc_sa.svg" alt="Creative Commons"></a>
  </div>

        </div>
      </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://szf.cool/2024/05/10/%E8%8A%B1%E4%B9%A6%E2%80%94%E2%80%94%E6%B7%B1%E5%BA%A6%E5%89%8D%E9%A6%88%E7%BD%91%E7%BB%9C/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpg">
      <meta itemprop="name" content="Szfmsmdx">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Szf's blog ^ ^">
      <meta itemprop="description" content="To be a pure man">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="花书——深度前馈网络 | Szf's blog ^ ^">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          花书——深度前馈网络
        </h1>

        

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2024-05-10 11:46:21" itemprop="dateCreated datePublished" datetime="2024-05-10T11:46:21+08:00">2024-05-10</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2024-05-18 13:33:14" itemprop="dateModified" datetime="2024-05-18T13:33:14+08:00">2024-05-18</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Deep-Learning/" itemprop="url" rel="index"><span itemprop="name">Deep Learning</span></a>
        </span>
    </span>

  
  
  <span class="post-meta-item">
    
    <span class="post-meta-item-icon">
      <i class="far fa-comment"></i>
    </span>
    <span class="post-meta-item-text">Waline：</span>
  
    <a title="waline" href="/2024/05/10/%E8%8A%B1%E4%B9%A6%E2%80%94%E2%80%94%E6%B7%B1%E5%BA%A6%E5%89%8D%E9%A6%88%E7%BD%91%E7%BB%9C/#waline" itemprop="discussionUrl">
      <span class="post-comments-count waline-comment-count" data-path="/2024/05/10/%E8%8A%B1%E4%B9%A6%E2%80%94%E2%80%94%E6%B7%B1%E5%BA%A6%E5%89%8D%E9%A6%88%E7%BD%91%E7%BB%9C/" itemprop="commentCount"></span>
    </a>
  </span>
  
  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>12k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>21 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    


    <div class="post-body" itemprop="articleBody"><blockquote class="blockquote-center">
<p>研读花书，皆为拙记，如有错误还望各位不惜笔墨，不啬赐教。本节主要包括<strong>深度前馈网络</strong>部分内容</p>

</blockquote>
<span id="more"></span>
<p><strong>深度前馈网络（deep feedforward network）</strong>，也叫作
<strong>前馈神经网络（feedforwardneural network）</strong>或者
<strong>多层感知机（multilayer perceptron,
MLP）</strong>，是典型的深度学习模型。</p>
<blockquote>
<p>目标是近似某个函数 <span class="math inline">\(f^*\)</span></p>
<p>例如对于分类器， <span class="math inline">\(y=f^*(x)\)</span> 将输入
x 映射到一个类别 y</p>
<p>前馈网络定义了一个映射 <span
class="math inline">\(y=f(x;\theta)\)</span> ，并且学习 <span
class="math inline">\(\theta\)</span>
的值，使他能够达到最佳的函数的近似</p>
</blockquote>
<p>这种模型被称为 <strong>前向（feedforward）</strong>的，是因为信息流过
x 的函数，流经用于定义 f 的中间计算过程，最终到达输出
y。在模型的输出和模型本身之间没有
<strong>反馈（feedback）</strong>连接。当前馈神经网络被扩展成包含反馈连接时，它们被称为
<strong>循环神经网络（recurrent neural network）</strong></p>
<p>前馈神经网络被称作
<strong>网络（network）</strong>是因为它们通常用许多不同函数复合在一起来表示。该模型与一个<strong>有向无环图</strong>相关联，而图描述了函数是如何复合在一起的。比如有三个函数
<span class="math inline">\(f^{(1)}、f^{(2)}、f^{(3)}\)</span>
，连接在一个链上，那么他们分别称为网络的 first layer、second layer
...</p>
<p>链的全长称为模型的 深度（depth）。正是因为这个术语才出现了
‘‘深度学习’’ 这个名字。前馈网络的最后一层被称为 <strong>输出层（output
layer）</strong>。</p>
<p>在训练过程中，我们让 <span class="math inline">\(f(x)\)</span> 去匹配
<span class="math inline">\(f^*(x)\)</span> 的值，其中训练样本 x
伴随着一个标签 <span class="math inline">\(y\approx f^*(x)\)</span></p>
<blockquote>
<p>训练样本直接指明了输出层在每一点 x 上必须做什么；它必须产生一个接近 y
的值。但是训练数据并没有直接指明<font color=red>其他层</font>应该怎么做。因为训练数据并没有给出这些层中的每一层所需的输出，所以这些层被称为
<strong>隐藏层（hidden layer）</strong>。</p>
</blockquote>
<p>网络中的每个隐藏层通常都是向量值的。这些隐藏层的维数决定了模型的
<strong>宽度（width）</strong>。向量的每个元素都可以被视为起到类似一个神经元的作用。除了将层想象成向量到向量的单个函数，我们也可以把层想象成由许多并行操作的
<strong>单元（unit）</strong>组成，每个单元表示一个向量到标量的函数。</p>
<h1 id="学习异或xor">学习异或（XOR）</h1>
<p>对于二值函数异或来说，其定义域限制在 <span
class="math inline">\(\mathbb X=\{[0,0]^T,[0,1]^T,[1,0]^T,[1,1]^T
\}\)</span> ，将该问题当成是回归问题，损失函数选择
MSE（虽然他不是一个合适的损失函数），那么整个训练集上表现的 MSE
损失函数为 <span class="math display">\[
J(\theta)=\frac 14 \sum_{x\in \mathbb X} (f^*(x)-f(x;\theta))^2
\]</span> 假设模型选择的是一个线性模型，那么，将其定义为 <span
class="math display">\[
f(x,w,b)=x^Tw + b
\]</span> 这是可以通过正规方程去使得 <span
class="math inline">\(J(\theta)\)</span>
最小的，通过求解正规方程，可以发现 <span
class="math inline">\(w=0,b=\frac 12\)</span> 。</p>
<p><img src="https://szfmsmdx.oss-cn-hangzhou.aliyuncs.com/blog_pic/202405131401989.png" alt="image-20240513140138915" style="zoom:67%;" /></p>
<blockquote>
<p>出现这种情况的原因是，[0,1] 和 [1,0]
两个点的输出都为1（左图的左上和右下），当 <span
class="math inline">\(x_1=0\)</span> 时，模型的输出必须随着 <span
class="math inline">\(x_2\)</span> 的增大而增大，当 <span
class="math inline">\(x_1=0\)</span> 时，模型的输出必须随着 <span
class="math inline">\(x_2\)</span>
的增大而减小，这是线性模型做不到的。不过可以做映射 <span
class="math inline">\(h=f^{(1)}(x;W,c)\)</span>
，让第一层的输出变为输出层的输入，就能比较好的解决这个问题。</p>
<p>而通过线性变换将 [0,1] 和 [1,0] 映射到同一个点 <span
class="math inline">\(h=[1,0]\)</span> 时（右图），就可以描述为 <span
class="math inline">\(h_1\)</span> 增大和 <span
class="math inline">\(h_2\)</span> 减小使得输出增大。</p>
</blockquote>
<p>不过 <span class="math inline">\(f^{(1)}\)</span>
具体应该是那种函数，显然简单的线性不能满足需要了，因为线性的叠加本质上还是线性的，所以需要通过非线性函数来描述特征。在线代神经网络中，默认的推荐是使用由激活函数
<span class="math inline">\(g(z)=\max \{0, z\}\)</span>
定义的<strong>整流线型单元（rectified linear unit）</strong>，或是称为
ReLU</p>
<p>将ReLU应用到上面的异或学习中，我们的整个网络就是 <span
class="math inline">\(f(x;W,c,w,b)=w^T\max \{0,W^Tx+c \}+b\)</span></p>
<h1 id="基于梯度的学习">基于梯度的学习</h1>
<p>由于引入了许多非线性的函数，因此可能最终的代价函数都变得非凸，所以神经网络的训练通常使用迭代的、基于梯度的优化，将代价函数训练到一个很小的值；而不是利用正规方程求解线性的问题。</p>
<p>用于非凸损失函数的随机梯度下降没有这种收敛性保证，并且对参数的初始值很敏感。<strong>对于前馈神经网络，将所有的权重值初始化为小随机数是很重要的。</strong></p>
<h2 id="代价函数">代价函数</h2>
<p>在大多数情况下，我们的参数模型定义了一个分布 <span
class="math inline">\(p(y|x;\theta)\)</span>
并且我们简单地使用最大似然原理。这意味着我们使用训练数据和模型预测间的交叉熵作为代价函数。</p>
<p>有时，我们使用一个更简单的方法，不是预测 y
的完整概率分布，而是仅仅预测在给定 x 的条件下 y
的某种统计量。某些专门的损失函数允许我们来训练这些估计量的预测器。</p>
<h3 id="使用最大似然学习条件分布">使用最大似然学习条件分布</h3>
<p>大多数现代的神经网络使用最大似然来训练。这意味着代价函数就是负的对数似然，它与训练数据和模型分布间的交叉熵等价。这个代价函数表示为
<span class="math display">\[
J(\theta)=-\mathbb E_{x,y\sim \hat{p_{data}}}\log p_{model}(y|x)
\]</span>
上述方程的展开形式通常会有一些项不依赖于模型的参数，我们可以舍去。例如，<span
class="math inline">\(p_{model}(y|x)=\mathcal
N(y;f(x;\theta),I)\)</span> ，那么就能重新得到均方误差代价 <span
class="math inline">\(J(\theta)=\frac 12 \mathbb E_{x,\sim
\hat{p_{data}}}||y-f(x;\theta)||^2+const\)</span> ，至少系数 1/2
和常数项不依赖于 θ。</p>
<blockquote>
<p>使用最大似然来导出代价函数的方法的一个优势是，它减轻了为每个模型设计代价函数的负担。明确一个模型
p(y | x) 则自动地确定了一个代价函数 log p(y | x)。</p>
</blockquote>
<h3 id="学习条件统计量">学习条件统计量</h3>
<blockquote>
<p>有时我们并不是想学习一个完整的概率分布 p(y | x;
θ)，而仅仅是想学习在给定x 时 y 的某个条件统计量。</p>
</blockquote>
<p>例如，我们可能有一个预测器 f(x; θ)，我们想用它来预测 y
的均值。如果我们使用一个足够强大的神经网络，我们可以认为这个神经网络能够表示一大类函数中的任何一个函数
f，这个类仅仅被一些特征所限制，例如连续性和有界，而不是具有特殊的参数形式。</p>
<p>从这个角度来看，我们可以把代价函数看作是一个
<strong>泛函（functional）</strong>而不仅仅是一个函数。泛函是函数到实数的映射。我们因此可以将学习看作是选择一个函数而不仅仅是选择一组参数。</p>
<p>我们可以设计代价泛函在我们想要的某些特殊函数处取得最小值。例如，我们可以设计一个代价泛函，使它的最小值处于一个特殊的函数上，这个函数将
x 映射到给定 x 时 y 的期望值。</p>
<p>我们使用变分法导出的第一个结果是解优化问题</p>
<p><img src="https://szfmsmdx.oss-cn-hangzhou.aliyuncs.com/blog_pic/202405141144738.png" alt="image-20240514114403659" style="zoom:67%;" /></p>
<p>得到</p>
<p><img src="https://szfmsmdx.oss-cn-hangzhou.aliyuncs.com/blog_pic/202405141144042.png" alt="image-20240514114428005" style="zoom:67%;" /></p>
<p>要求这个函数处在我们要优化的类里。换句话说，如果我们能够用无穷多的、来源于真实的数据生成分布的样本进行训练，最小化均方误差代价函数将得到一个函数，它可以用来对每个
x 的值预测出 y 的均值。</p>
<p>不同的代价函数给出不同的统计量。第二个使用变分法得到的结果是</p>
<p><img src="https://szfmsmdx.oss-cn-hangzhou.aliyuncs.com/blog_pic/202405141145424.png" alt="image-20240514114539384" style="zoom:67%;" /></p>
<p>将得到一个函数可以对每个 x 预测 y
取值的中位数，只要这个函数在我们要优化的函数族里。这个代价函数通常被称为
<strong>平均绝对误差（mean absolute error）</strong>。</p>
<p><font color=red>
可惜的是，均方误差和平均绝对误差在使用基于梯度的优化方法时往往成效不佳。一些饱和的输出单元当结合这些代价函数时会产生非常小的梯度。</font>
这就是为什么交叉熵代价函数比均方误差或者平均绝对误差更受欢迎的原因之一了，即使是在没必要估计整个
p(y | x) 分布时。</p>
<h2 id="输出单元">输出单元</h2>
<h3 id="用于高斯输出分布的线性单元">用于高斯输出分布的线性单元</h3>
<p>一种简单的输出单元是基于仿射变换的输出单元，仿射变换不具有非线性。这些单元往往被直接称为线性单元。给定特征
h，线性输出单元层产生一个向量 <span class="math inline">\(\hat y=W^T
h+b\)</span></p>
<p>线性输出层经常被用来产生条件高斯分布的均值： <span
class="math display">\[
p(y|x)=\mathcal N(y;\hat y, I)
\]</span> 最大化其对数似然此时等价于最小化均方误差。</p>
<h3
id="用于bernoulli伯努利输出分布的sigmoid单元">用于Bernoulli(伯努利)输出分布的sigmoid单元</h3>
<p>许多任务需要预测二值型变量 y
的值。具有两个类的分类问题可以归结为这种形式。</p>
<p>此时最大似然的方法是定义 y 在 x 条件下的 Bernoulli 分布。</p>
<p>Bernoulli 分布仅需单个参数来定义。神经网络只需要预测 P(y = 1 | x)
即可。为了使这个数是有效的概率，它必须处在区间 [0, 1] 中。</p>
<p>sigmoid输出单元定义为 <span class="math inline">\(\hat y=\sigma(w^T h
+ b)\)</span></p>
<p>我们可以认为 sigmoid
输出单元具有两个部分。首先，它使用一个线性层来计算 <span
class="math inline">\(z=w^T h + b\)</span>​ 。接着，它使用 sigmoid
激活函数将 z 转化成概率。</p>
<p>如果我们假定非归一化的对数概率对 y 和 z
是线性的，可以对它取指数来得到非归一化的概率。我们然后对它归一化，可以发现这服从
Bernoulli 分布，该分布受 z 的 sigmoid 变换控制：</p>
<p><img src="https://szfmsmdx.oss-cn-hangzhou.aliyuncs.com/blog_pic/202405141214607.png" alt="image-20240514121415561" style="zoom:67%;" /></p>
<blockquote>
<p>上式之所以写成 2y-1，是因为带入 <span
class="math inline">\(y&#39;=0,1\)</span>
时发现，2y-1能够得到相应的结果</p>
</blockquote>
<p>sigmoid 激活函数在 z 取非常小的负值时会饱和到 0，当 z
取非常大的正值时会饱和到
1。这种情况一旦发生，梯度会变得非常小以至于不能用来学习，无论此时模型给出的是正确还是错误的答案。因此，最大似然几乎总是训练
sigmoid 输出单元的优选方法。</p>
<h3 id="用于-multinoulli-输出分布的-softmax-单元">用于 Multinoulli
输出分布的 softmax 单元</h3>
<p>任何时候当我们想要表示一个具有 n
个可能取值的离散型随机变量的分布时，我们都可以使用 softmax
函数。它可以看作是 sigmoid 函数的扩展，其中 sigmoid
函数用来表示二值型变量的分布。</p>
<p>首先，线性层预测了未归一化的对数概率： <span class="math display">\[
z=W^T h + b
\]</span> 其中 <span class="math inline">\(z_i = \log \hat
P(y=i|x)\)</span> ，softmax 函数然后可以对 z 指数化和归一化来获得需要的
<span class="math inline">\(\hat y\)</span> 。最终，softmax 函数的形式为
<span class="math display">\[
\mathrm{softmax}(z)_i=\frac{\exp(z_i)}{\sum_j \exp(z_j)}
\]</span> 和 logistic sigmoid一样，当使用最大化对数似然训练 softmax
来输出目标值 y 时，使用指数函数工作地非常好。这种情况下，我们想要最大化
<span class="math inline">\(\log P(y = i; z) = \log \mathrm
{softmax}(z)_i\)</span>。将 softmax
定义成指数的形式是很自然的因为对数似然中的 log 可以抵消 softmax 中的
exp： <span class="math display">\[
\log \mathrm {softmax} (z)_i=z_i - \log \sum_j \exp(z)_j
\]</span> 通过观察其表达式能够发现，第一项 <span
class="math inline">\(z_i\)</span>
是不会饱和的，因此学习总是可以进行；而第二项 <span
class="math inline">\(\log \sum_j \exp (z_j)\)</span>
有一个直观的理解是，这一项可以被近似为 <span
class="math inline">\(\max_j z_j\)</span>
，所以，<font color=#00ffff> 负对数似然函数总是强烈地惩罚最活跃的不正确预测 </font>
，如果正确答案已经具有了 softmax 的最大输入，那么 <span
class="math inline">\(-z_i\)</span> 和 <span class="math inline">\(\log
\sum_j \exp(z_j)\approx \max_j z_j=z_i\)</span>
项将大致抵消。这个样本对于整体训练代价贡献很小，这个代价主要由其他未被正确分类的样本产生。</p>
<p>对于 softmax
函数，当输入值之间的差异变得极端时，这些输出值可能会饱和，当 softmax
饱和时，基于 softmax
的许多代价函数也饱和，除非他们能够转化饱和的激活函数。</p>
<p>由于对所有输入都加上一个相同常数时，softmax的输出不变： <span
class="math display">\[
\mathrm{softmax}(z)=\mathrm{softmax}(z+c) \\
=&gt;\mathrm{softmax}(z)=\mathrm{sorfmax}(z-\max_i z_i)
\]</span> 变换后的形式允许我们在对 softmax
函数求值时只有很小的数值误差，即使是当 z 包含极正或者极负的数时。观察
softmax 数值稳定的变体，可以看到 softmax 函数由它的变量偏离 maxi zi
的量来驱动。</p>
<p>当其中一个输入是最大 <span class="math inline">\((z_i=\max_i
z_i)\)</span> 并且 <span class="math inline">\(z_i\)</span>
远大于其他的输入时，相应的输出会饱和到 1。当 z_i
不是最大值并且最大值非常大时，相应的输出也会饱和到 0。</p>
<h2 id="隐藏单元">隐藏单元</h2>
<p>除非另有说明，大多数的隐藏单元都可以描述为接受输入向量 x，计算仿射变
换 z = W⊤ x + b，然后使用一个逐元素的非线性函数
g(z)。大多数隐藏单元的区别 仅仅在于激活函数 g(z) 的形式。</p>
<h3 id="relu及其扩展">ReLU及其扩展</h3>
<p>整流线性单元通常作用于仿射变换之上： <span class="math display">\[
h=g(W^T x + b)
\]</span> 当初始化仿射变换的参数时，可以将 b
的所有元素设置成一个小的正值，例如
0.1。这使得整流线性单元很可能初始时就对训练集中的大多数输入呈现激活状态，并且允许导数通过。</p>
<p>ReLU的一个缺陷是它们不能够通过基于梯度的方法学习哪些使它们激活为0的样本，所以，拓展保证了他们能够在各个位置都有梯度。</p>
<p>整流线性单元的三个扩展基于当 <span class="math inline">\(z_i &lt;
0\)</span> 时使用一个非零的斜率 <span
class="math inline">\(\alpha_i\)</span>：<span
class="math inline">\(h_i=g(z,\alpha)_i=\max(0,z_i)+\alpha_i
\min(0,z_i)\)</span></p>
<ul>
<li><strong>绝对值整流（absolute value rectification）</strong>：固定
<span class="math inline">\(\alpha=-1\)</span></li>
<li><strong>渗漏整流线性单元（Leaky ReLU）</strong>：固定 <span
class="math inline">\(\alpha\)</span> 为一个非常小的值</li>
<li><strong>参数化整流线性单元（parametric ReLU）或PReLU</strong>
：<span class="math inline">\(\alpha\)</span> 作为学习的参数</li>
</ul>
<p>maxout 单元可以学习具有多达 k 段的分段线性的凸函数。maxout
单元因此可以视为学习激活函数本身而不仅仅是单元之间的关系。使用足够大的
k，maxout 单元可以以任意的精确度来近似任何凸函数。</p>
<p>每个 maxout 单元现在由 k 个权重向量来参数化，而不仅仅是一个，所以
maxout单元通常比整流线性单元需要更多的正则化。如果训练集很大并且每个单元的块数保持很低的话，它们可以在没有正则化的情况下工作得不错</p>
<h2 id="logistic-sigmoid与双曲正切函数">logistic
sigmoid与双曲正切函数</h2>
<p>在ReLU之前，很多神经网络使用 logistic sigmoid激活函数 <span
class="math display">\[
g(z)=\sigma(z)
\]</span> 或者是双曲正切激活函数 <span class="math display">\[
g(z)=\tanh (z)=\frac{e^z-e^{-z}}{e^z+e^{-z}}
\]</span> 他们本质上是相同的，因为 <span
class="math inline">\(\tanh(z)=2\sigma(2z) - 1\)</span></p>
<h2 id="其他隐藏单元">其他隐藏单元</h2>
<p>也存在许多其他种类的隐藏单元，但它们并不常用。</p>
<ul>
<li><strong>径向基函数（radial basis function，RBF）</strong>：<span
class="math inline">\(h_i=\exp (-\frac
1{\sigma^2}||W_{:,i}-x||^2)\)</span> ，这个函数在 x 接近模板 W
时更加活跃，因为它对大部分 x 都饱和到0</li>
<li><strong>softplus函数</strong>： <span
class="math inline">\(g(a)=\zeta(a)=\log (1+e^a)\)</span>
，这是ReLU的平滑版本。softplus
表明隐藏单元类型的性能可能是非常反直觉的——因为它处处可导或者因为它不完全饱和，人们可能希望它具有优于整流线性单元的点，但根据经验来看，它并没有。</li>
<li><strong>硬双曲正切函数（hard tanh）</strong>： <span
class="math inline">\(g(a)=\max (-1, \min(1, a))\)</span></li>
</ul>
<h1 id="架构设计">架构设计</h1>
<p>大多数神经网络被组织成称为层的单元组，例如，第一层为：</p>
<p><img src="https://szfmsmdx.oss-cn-hangzhou.aliyuncs.com/blog_pic/202405161548317.png" alt="image-20240516154840245" style="zoom:50%;" /></p>
<p>第二层为：</p>
<p><img src="https://szfmsmdx.oss-cn-hangzhou.aliyuncs.com/blog_pic/202405161548922.png" alt="image-20240516154853853" style="zoom:50%;" /></p>
<p>以此类推。</p>
<p>在这些链式架构中，主要考虑的是<strong>网络的深度</strong>和<strong>每层的宽度</strong></p>
<h2 id="万能近似性质和深度">万能近似性质和深度</h2>
<blockquote>
<p><strong>万能近似定理（universal approximation
theorem）</strong>表明，一个前馈神经网络如果具有线性输出层和至少一层具有任何一种
‘‘挤压’’ 性质的激活函数（例如logistic
sigmoid激活函数）的隐藏层，只要给予网络足够数量的隐藏单元，它可以以任意的精度来近似任何从一个有限维空间到另一个有限维空间的
Borel 可测函数。</p>
</blockquote>
<p>神经网络也可以近似从任何有限维离散空间映射到另一个的任意函数。虽然原始定理最初以具有特殊激活函数的单元的形式来描述，这个激活函数当变量取绝对值非常大的正值和负值时都会饱和，万能近似定理也已经被证明对于更广泛类别的激活函数也是适用的，其中就包括现在常用的ReLU</p>
<p>万能近似定理意味着无论我们试图学习什么函数，我们知道一个大的 MLP
一定能够表示这个函数。然而，我们不能保证训练算法能够学得这个函数。</p>
<ul>
<li>用于训练的优化算法可能找不到用于期望函数的参数值</li>
<li>训练算法可能由于过拟合而选择了错误的函数</li>
</ul>
<p>具有单层的前馈网络足以表示任何函数，但是网络层可能大得不可实现，并且可能无法正确地学习和泛化。在很多情况下，使用更深的模型能够减少表示期望函数所需的单元的数量，并且可以减少泛化误差。</p>
<p>存在一些函数族能够在网络的深度大于某个值 d
时被高效地近似，而当深度被限制到小于或等于 d
时需要一个远远大于之前的模型。在很多情况下，浅层模型所需的隐藏单元的数量是
n 的指数级。</p>
<p>Montufar的主要定理指出，具有 d 个输入、深度为 l、每个隐藏层具有 n
个单元的深度整流网络可以描述的线性区域的数量是</p>
<p><img src="https://szfmsmdx.oss-cn-hangzhou.aliyuncs.com/blog_pic/202405161617141.png" alt="image-20240516161710071" style="zoom:67%;" /></p>
<p>意味着，这是深度 l 的指数级。在每个单元具有 k 个过滤器的 maxout
网络中，线性区域的数量是</p>
<p><img src="https://szfmsmdx.oss-cn-hangzhou.aliyuncs.com/blog_pic/202405161617068.png" alt="image-20240516161729003" style="zoom:67%;" /></p>
<p><img src="https://szfmsmdx.oss-cn-hangzhou.aliyuncs.com/blog_pic/202405161619644.png" alt="image-20240516161943564" style="zoom:67%;" /></p>
<blockquote>
<p>参数数量的影响。更深的模型往往表现更好。这不仅仅是因为模型更大。这项实验表明，增加卷积网络层中参数的数量，但是不增加它们的深度，在提升测试集性能方面几乎没有效果，如此图所示。</p>
</blockquote>
<h2 id="其他架构上的考虑">其他架构上的考虑</h2>
<p>一般的，层不需要连接在链中，尽管这是最常见的做法。许多架构构建了一个主链，但随后又添加了额外的架构特性，例如从层
i 到层 i + 2
或者更高层的跳跃连接。这些跳跃连接使得梯度更容易从输出层流向更接近输入的层。</p>
<p>架构设计考虑的另外一个关键点是如何将<strong>层与层之间连接起来</strong>。默认的神经网络层采用矩阵
W 描述的线性变换，每个输入单元连接到每个输出单元。</p>
<p>用于减少连接数量的策略减少了参数的数量以及用于评估网络的计算量，但通常高度依赖于问题。</p>
<h1 id="反向传播和其他的微分算法">反向传播和其他的微分算法</h1>
<p>使用前馈神经网络接收输入 x 并产生输出 <span
class="math inline">\(\hat y\)</span> 时，信息通过网络向前流动</p>
<ul>
<li>从输入 x 提供原始信息，到最终产生输出 <span
class="math inline">\(\hat y\)</span> ，这称之为
<strong>前向传播（forward propagation）</strong></li>
<li>从来自代价函数的信息通过网络向后流动，以便计算梯度称之为
<strong>反向传播（back propagation）</strong></li>
</ul>
<p>反向传播这个术语经常被误解为用于多层神经网络的整个学习算法。实际上，反向传播仅指用于计算梯度的方法，而另一种算法，例如随机梯度下降，使用该梯度来进行学习。</p>
<h2 id="计算图">计算图</h2>
<p>为了更精确地描述反向传播算法，需要引入 <strong>计算图（computational
graph）</strong></p>
<p>使用图中的每一个节点来表示一个变量。变量可以是标量、向量、矩阵、张量、或者甚至是另一类型的变量。</p>
<p>为了形式化我们的图形，我们还需引入
<strong>操作（operation）</strong>这一概念：是指一个或多个变量的简单函数。</p>
<p><img src="https://szfmsmdx.oss-cn-hangzhou.aliyuncs.com/blog_pic/202405161644450.png" alt="image-20240516164445348" style="zoom:80%;" /></p>
<blockquote>
<p>一些计算图示例</p>
</blockquote>
<h2 id="微积分中的链式法则">微积分中的链式法则</h2>
<p>反向传播是一种计算链式法则的算法，使用高效的特定运算顺序。</p>
<p>设 x 是实数，f 和 g 是从实数映射到实数的函数。假设 <span
class="math inline">\(y=g(x)\)</span> 且 <span
class="math inline">\(z=f(g(x))=f(y)\)</span> ，那么链式法则的意思是
<span class="math display">\[
\frac {dz}{dx}=\frac{dz}{dy} \frac{dy}{dx}
\]</span> 我们可以将这种标量情况进行扩展。假设 <span
class="math inline">\(x \in \mathbb R^m\)</span> , <span
class="math inline">\(y \in \mathbb R^n\)</span> ，g是从 <span
class="math inline">\(\mathbb R^m\)</span> 到 <span
class="math inline">\(\mathbb R^n\)</span> 的映射，f 是从 <span
class="math inline">\(\mathbb R^n\)</span> 到 <span
class="math inline">\(\mathbb R\)</span> 的映射，若有 <span
class="math inline">\(y=g(x)\)</span> 且 <span
class="math inline">\(z=f(y)\)</span> ，那么 <span
class="math display">\[
\frac {\partial z}{\partial x_i}=\sum_j \frac{\partial z}{\partial y_j}
\frac {\partial y_j}{\partial x_i}
\]</span> 使用向量记法，可以等价地写成 <span
class="math inline">\(\nabla_x z=(\frac {\partial y}{\partial x})^T
\nabla_y z\)</span> ，其中 <span class="math inline">\(\frac {\partial
y}{\partial x}\)</span> 就是 g 的 n x m的 Jacobian 矩阵</p>
<blockquote>
<p>通常我们将反向传播算法应用于任意维度的张量，而不仅仅用于向量。在我们运行反向传播之前，将每个张量变平为一个向量，计算一个向量值梯度，然后将该梯度重新构造成一个张量。</p>
<p>从这种重新排列的观点上看，反向传播仍然只是将 Jacobian 乘以梯度。</p>
</blockquote>
<p>为了表示值 z 关于张量 X 的梯度，我们记为 <span
class="math inline">\(\nabla_X z\)</span> ，就像 X 是向量一样。X
的索引现在有多个坐标——比如张量的维度为3，那么坐标就需要三个索引来表示。不过可以使用单个变量
i 来表示完整的<strong>索引元组</strong>，从而完全抽象出来。同样的，如果
<span class="math inline">\(Y=g(X)\)</span> 且 <span
class="math inline">\(z=f(Y)\)</span> ，那么 <span
class="math display">\[
\nabla_X z=\sum_j (\nabla_X Y_j) \frac {\partial z}{\partial Y_j}
\]</span></p>
<h2
id="递归地使用链式法则实现反向传播">递归地使用链式法则实现反向传播</h2>
<p><img src="https://szfmsmdx.oss-cn-hangzhou.aliyuncs.com/blog_pic/202405161745844.png" alt="image-20240516174554739" style="zoom:80%;" /></p>
<p>反向传播算法被设计为减少公共子表达式的数量而不考虑存储的开销。具体来说，它大约对图中的每个节点执行一个
Jacobian 乘积。</p>
<p><img src="https://szfmsmdx.oss-cn-hangzhou.aliyuncs.com/blog_pic/202405161747716.png" alt="image-20240516174745628" style="zoom:67%;" /></p>
<blockquote>
<p>图 6.9: 计算梯度时导致重复子表达式的计算图。令 w ∈ R
为图的输入。我们对链中的每一步使用相同的操作函数f : R → R，这样 x =
f(w), y = f(x), z = f(y)。为了计算 ∂z / ∂w，我们应用链式法则得到：</p>
<p><img src="https://szfmsmdx.oss-cn-hangzhou.aliyuncs.com/blog_pic/202405161748140.png" alt="image-20240516174839055" style="zoom:50%;" /></p>
<p>式 (6.52) 建议我们采用的实现方式是，仅计算 f(w)
的值一次并将它存储在变量 x 中。这是反向传播算法所采用的方法。式 (6.53)
提出了一种替代方法，其中子表达式 f(w)
出现了不止一次。在替代方法中，每次只在需要时重新计算
f(w)。当存储这些表达式的值所需的存储较少时，式 (6.52)
的反向传播方法显然是较优的，因为它减少了运行时间。然而，式 (6.53)
也是链式法则的有效实现，并且当存储受限时它是有用的。</p>
</blockquote>
<h2 id="全连接-mlp-中的反向传播">全连接 MLP 中的反向传播</h2>
<p><img src="https://szfmsmdx.oss-cn-hangzhou.aliyuncs.com/blog_pic/202405161805492.png" alt="image-20240516180512371" style="zoom:67%;" /></p>
<h2 id="符号到符号的导数">符号到符号的导数</h2>
<p>代数表达式和计算图都对
<strong>符号（symbol）</strong>或不具有特定值的变量进行操作。这些代数或者基于图的表达式被称为
<strong>符号表示（symbolic representation）</strong>。</p>
<p>当我们实际使用或者训练神经网络时，我们必须给这些符号赋特定的值。我们用一个特定的
<strong>数值（numeric value）</strong>来替代网络的符号输入 x</p>
<p>一些反向传播的方法采用
<strong><em>计算图和一组用于图的输入的数值，然后返回在这些输入值处梯度的一组数值</em></strong>
。我们将这种方法称为 <strong>符号到数值的微分</strong>。这种方法用在诸如
Torch和 Caffe之类的库中。</p>
<p>另一种方法是<strong><em>采用计算图以及添加一些额外的节点到计算图中</em></strong>，这些额外的节点提供了我们所需导数的符号描述。这是
Theano和 TensorFlow 所采用的方法。</p>
<blockquote>
<p>个人理解是动态和静态的区别？</p>
<p>因为符号到微分的方法，需要前面的节点进行建图，这是动态的</p>
<p>而添加额外节点，比如对输入构建一层额外的节点，<span
class="math inline">\(a^{(0)}\)</span> 是输入，<span
class="math inline">\(h^{(1)}=a^{(0)}\)</span>
，多构建一层，然后基于h，建立后续的图关系</p>
</blockquote>
<p><img src="https://szfmsmdx.oss-cn-hangzhou.aliyuncs.com/blog_pic/202405161810183.png" alt="image-20240516181010069" style="zoom:80%;" /></p>
<p><img src="https://szfmsmdx.oss-cn-hangzhou.aliyuncs.com/blog_pic/202405161812666.png" alt="image-20240516181200546" style="zoom:80%;" /></p>
<h2 id="一般化的反向传播">一般化的反向传播</h2>
<p>为了计算某个标量 z 关于图中它的一个祖先 x
的梯度，我们首先观察到它关于 z 的梯度由 dz dz = 1
给出。然后，我们可以计算对图中 z
的每个父节点的梯度，通过现有的梯度乘以产生 z 的操作的
Jacobian。我们继续乘以 Jacobian，以这种方式向后穿过图，直到我们到达
x。对于从 z
出发可以经过两个或更多路径向后行进而到达的任意节点，我们简单地对该节点来自不同路径上的梯度进行求和。</p>
<p>更正式地，图 G
中的每个节点对应着一个变量。为了实现最大的一般化，我们将这个变量描述为一个张量
V。张量通常可以具有任意维度，并且包含标量、向量和矩阵。</p>
<p>我们假设每个变量 V 与下列子程序相关联：</p>
<ul>
<li>get_operation：它返回用于计算 V 的操作，代表了在计算图中流入 V
的边。</li>
<li>get_consumers(V, G)：它返回一组变量，是计算图 G 中 V 的子节点。</li>
<li>get_inputs(V, G)：它返回一组变量，是计算图 G 中 V 的父节点。</li>
</ul>
<p><img src="https://szfmsmdx.oss-cn-hangzhou.aliyuncs.com/blog_pic/202405161825716.png" alt="image-20240516182538605" style="zoom:80%;" /></p>
<p><img src="https://szfmsmdx.oss-cn-hangzhou.aliyuncs.com/blog_pic/202405161825012.png" alt="image-20240516182558894" style="zoom:80%;" /></p>
<h2 id="以mlp为例的反向传播">以MLP为例的反向传播</h2>
<p>网络的计算隐藏特征层为 <span class="math inline">\(H=\max
{0,XW^{(1)}}\)</span> ，输出概率预测为 <span
class="math inline">\(HW^{(2)}\)</span> 给出，假设最后的损失函数为
cross_entropy 操作，并且给上一个正则项，那么总的代价函数为</p>
<p><img src="https://szfmsmdx.oss-cn-hangzhou.aliyuncs.com/blog_pic/202405161834171.png" alt="image-20240516183404085" style="zoom:67%;" /></p>
<p>那么计算图如下所示：</p>
<p><img src="https://szfmsmdx.oss-cn-hangzhou.aliyuncs.com/blog_pic/202405161834812.png" alt="image-20240516183421699" style="zoom:80%;" /></p>
<p>对于参数 <span class="math inline">\(\nabla_{W^{(1)}}J\)</span> 和
<span class="math inline">\(\nabla_{W^{(2)}}J\)</span>
，有两种不同的路径从 J 后退回到权重：</p>
<ul>
<li>一条通过交叉熵代价</li>
<li>一条通过权重衰减代价</li>
</ul>
<p>通过权重衰减的代价总是贡献 <span class="math inline">\(2\lambda
W^{(i)}\)</span></p>
<p>而通过交叉熵代价令 G 是由 cross_entropy 操作提供的对未归一化对数概率
<span class="math inline">\(U^{(2)}\)</span>
的梯度。反向传播算法现在需要探索两个不同的分支。在较短的分支上，它使用对矩阵乘法的第二个变量的反向传播规则，将
<span class="math inline">\(H^T G\)</span> 加到 <span
class="math inline">\(W^{(2)}\)</span>
的梯度上。另一条更长些的路径沿着网络逐步下降。首先，反向传播算法使用对矩阵乘法的第一个变量的反向传播规则，计算
<span class="math inline">\(\nabla_H J=GW^{(2)T}\)</span> 。接下来，relu
操作使用其反向传播规则对先前梯度的部分位置清零，这些位置对应着 <span
class="math inline">\(U^{(1)}\)</span> 中所有小于 0 的元素。记上述结果为
G ′。反向传播算法的最后一步是使用对 matmul
操作的第二个变量的反向传播规则，将 <span
class="math inline">\(X^TG&#39;\)</span> <strong>加到</strong> <span
class="math inline">\(W^{(1)}\)</span> 的梯度上。</p>
<blockquote>
<p>对于
MLP，计算成本主要来源于矩阵乘法。在前向传播阶段，我们乘以每个权重矩阵，得到了
O(w) 数量的乘-加，其中 w
是权重的数量。在反向传播阶段，我们乘以每个权重矩阵的转置，这具有相同的计算成本。</p>
<p>算法主要的存储成本是我们需要将输入存储到隐藏层的非线性中去。这些值从被计算时开始存储，直到反向过程回到了同一点。因此存储成本是
<span class="math inline">\(O(mn_h)\)</span>，其中 m
是小批量中样本的数目，<span class="math inline">\(n_h\)</span>
是隐藏单元的数量。</p>
</blockquote>
<h2 id="其他问题">其他问题</h2>
<ul>
<li>大多数软件实现需要支持可以返回多个张量的操作。</li>
<li>反向传播经常涉及将许多张量加在一起。在朴素方法中，将分别计算这些张量中的每一个，然后在第二步中对所有这些张量求和。朴素方法具有过高的存储瓶颈
<ul>
<li>可以通过保持一个缓冲器，并且在计算时将每个值加到该缓冲器中来避免该瓶颈。</li>
</ul></li>
<li>反向传播的现实实现还需要处理各种数据类型，例如 32 位浮点数、64
位浮点数和整型。处理这些类型的策略需要特别的设计考虑。</li>
<li>。。。</li>
</ul>
<h2 id="深度学习界以外的微分">深度学习界以外的微分</h2>
<p><strong>自动微分（automatic
differentiation）</strong>领域关心如何以算法方式计算导数。这里描述的反向传播算法只是自动微分的一种方法。它是一种称为
<strong>反向模式累加（reverse mode
accumulation）</strong>的更广泛类型的技术的特殊情况。</p>
<p>找到计算梯度的最优操作序列是 NP
完全问题，在这种意义上，它可能需要将代数表达式简化为它们最廉价的形式。</p>
<p>当图的输出数目大于输入的数目时，有时更偏向于使用另外一种形式的自动微分，称为
<strong>前向模式累加（forward mode accumulation）</strong>
。前向模式和后向模式的关系类似于左乘和右乘一系列矩阵之间的关系，例如
<span class="math display">\[
ABCD
\]</span> 其中的矩阵可以认为是 Jacobian 矩阵。例如，如果 D 是列向量，而
A
有很多行，那么这对应于一幅具有单个输出和多个输入的图，并且从最后开始乘，反向进行，只需要矩阵-向量的乘积。这对应着反向模式。相反，从左边开始乘将涉及一系列的矩阵-矩阵乘积，这使得总的计算变得更加昂贵。然而，如果
A 的行数小于 D 的列数，则从左到右乘更为便宜，这对应着前向模式。</p>
<h2 id="高阶微分">高阶微分</h2>
<p>在深度学习的相关领域，很少会计算标量函数的单个二阶导数。相反，我们通常对
Hessian 矩阵的性质比较感兴趣。如果我们有函数 <span
class="math inline">\(f:\mathbb R^n \to \mathbb R^n\)</span> ，那么
Hessian 矩阵的大小是 n × n。在典型的深度学习应用中，n
将是模型的参数数量，可能很容易达到数十亿。因此，完整的 Hessian
矩阵甚至不能表示。</p>
<p>典型的深度学习方法是使用 <strong>Krylov 方法（Krylov
method）</strong>，而不是显式地计算 Hessian 矩阵。Krylov
方法是用于执行各种操作的一组迭代技术，这些操作包括像近似求解矩阵的逆、或者近似矩阵的特征值或特征向量等，而不使用矩阵-向量乘法以外的任何操作。</p>
<p>为了在 Hesssian 矩阵上使用 Krylov 方法，我们只需要能够计算 Hessian
矩阵H 和一个任意向量 v 间的乘积即可。实现这一目标的一种直观方法是 <span
class="math display">\[
Hv=\nabla_x [(\nabla_x f(x))^T v]
\]</span></p>

    </div>

    
    
    

    <div>
      
      <div>
    
        <div style="text-align:center;color: #ccc;font-size:14px;">-------------本文结束<i class="fa fa-paw"></i>感谢您的阅读-------------</div>
    
</div>
      
    </div>

    <footer class="post-footer">
          

<div class="post-copyright">
<ul>
  <li class="post-copyright-author">
      <strong>本文作者： </strong>Szfmsmdx
  </li>
  <li class="post-copyright-link">
      <strong>本文链接：</strong>
      <a href="http://szf.cool/2024/05/10/%E8%8A%B1%E4%B9%A6%E2%80%94%E2%80%94%E6%B7%B1%E5%BA%A6%E5%89%8D%E9%A6%88%E7%BD%91%E7%BB%9C/" title="花书——深度前馈网络">http://szf.cool/2024/05/10/花书——深度前馈网络/</a>
  </li>
  <li class="post-copyright-license">
      <strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/zh-CN" rel="noopener" target="_blank"><i class="fab fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！
  </li>
</ul>
</div>

          <div class="post-tags">
              <a href="/tags/Deep-Learning/" rel="tag"><i class="fa fa-tag"></i> Deep Learning</a>
              <a href="/tags/%E8%8A%B1%E4%B9%A6/" rel="tag"><i class="fa fa-tag"></i> 花书</a>
              <a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" rel="tag"><i class="fa fa-tag"></i> 机器学习</a>
              <a href="/tags/%E5%89%8D%E9%A6%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" rel="tag"><i class="fa fa-tag"></i> 前馈神经网络</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2024/04/29/129%E5%9C%BA%E5%8F%8C%E5%91%A8%E8%B5%9B-395%E5%9C%BA%E5%91%A8%E8%B5%9B/" rel="prev" title="129场双周赛&&395场周赛">
                  <i class="fa fa-angle-left"></i> 129场双周赛&&395场周赛
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/2024/05/16/%E8%8A%B1%E4%B9%A6%E2%80%94%E2%80%94%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%AD%E7%9A%84%E6%AD%A3%E5%88%99%E5%8C%96/" rel="next" title="花书——深度学习中的正则化">
                  花书——深度学习中的正则化 <i class="fa fa-angle-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>





    <div class="comments" id="waline"></div>
</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">

  <div class="beian"><a href="https://beian.miit.gov.cn/" rel="noopener" target="_blank">皖ICP备2023007923号-2 </a>
      <img src="https://szfmsmdx.oss-cn-hangzhou.aliyuncs.com/blog_pic/202405231028849.png" alt=""><a href="https://beian.mps.gov.cn/#/query/webSearch?code=34018102340752" rel="noopener" target="_blank">皖公网安备34018102340752号 </a>
  </div>
  <div class="copyright">
    &copy; 
    <span itemprop="copyrightYear">2024</span>
    <span class="with-love">
      <i class="fa fa-heart"></i>
    </span>
    <span class="author" itemprop="copyrightHolder">Szfmsmdx</span>
  </div>
<div class="wordcount">
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-line"></i>
    </span>
    <span title="站点总字数">84k</span>
  </span>
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
    <span title="站点阅读时长">2:32</span>
  </span>
</div>

<!--
-->

    </div>
  </footer>

  
  <div class="back-to-top" role="button" aria-label="返回顶部">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/next-theme-pjax/0.6.0/pjax.min.js" integrity="sha256-vxLn1tSKWD4dqbMRyv940UYw4sXgMtYcK6reefzZrao=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/next-boot.js"></script><script src="/js/pjax.js"></script>

  <script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-generator-searchdb/1.4.1/search.js" integrity="sha256-1kfA5uHPf65M5cphT2dvymhkuyHPQp5A53EGZOnOLmc=" crossorigin="anonymous"></script>
<script src="/js/third-party/search/local-search.js"></script>







  




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"ams","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>


<script class="next-config" data-name="waline" type="application/json">{"lang":"zh-cn","enable":true,"serverURL":"comment.szf.cool","cssUrl":"https://unpkg.com/@waline/client@v2/dist/waline.css","commentCount":true,"pageview":false,"locale":null,"placeholder":"友好交流，善意评论 ^ ^","avatar":"mm","meta":["nick","mail","link"],"pageSize":10,"visitor":false,"comment_count":true,"requiredFields":[],"libUrl":"//unpkg.com/@waline/client@v2/dist/waline.js","dark":"body.darkmode--activated","el":"#waline","comment":true,"path":"/2024/05/10/%E8%8A%B1%E4%B9%A6%E2%80%94%E2%80%94%E6%B7%B1%E5%BA%A6%E5%89%8D%E9%A6%88%E7%BD%91%E7%BB%9C/"}</script>
<link rel="stylesheet" href="https://unpkg.com/@waline/client@v2/dist/waline.css">
<script>
document.addEventListener('page:loaded', () => {
  NexT.utils.loadComments(CONFIG.waline.el).then(() =>
    NexT.utils.getScript(CONFIG.waline.libUrl, { condition: window.Waline })
  ).then(() => 
    Waline.init(Object.assign({}, CONFIG.waline,{ el: document.querySelector(CONFIG.waline.el) }))
  );
});
</script>
<script src="https://unpkg.com/darkmode-js@1.5.7/lib/darkmode-js.min.js"></script>

<script>
var options = {
  bottom: '64px',
  right: '64px',
  left: 'unset',
  time: '0.3s',
  mixColor: 'transparent',
  backgroundColor: 'transparent',
  buttonColorDark: '#100f2c',
  buttonColorLight: '#fff',
  saveInCookies: false,
  label: '🌓',
  autoMatchOsTheme: false
}
const darkmode = new Darkmode(options);
window.darkmode = darkmode;
darkmode.showWidget();
if (window.darkmode && !window.darkmode.isActivated()) {
  window.darkmode.toggle();
  var toggleButtons = document.getElementsByClassName("darkmode-toggle");
  if (toggleButtons && toggleButtons.length > 0) {
    for (i = 0; i < toggleButtons.length; i++) {
      toggleButtons[i].classList.add("darkmode-toggle--white");
    }
  }
}
</script>

</body>
</html>
