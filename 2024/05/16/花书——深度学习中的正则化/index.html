<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 7.1.1">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">

<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext">

<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.1/css/all.min.css" integrity="sha256-wiz7ZSCn/btzhjKDQBms9Hx4sSeUYsDrTLg7roPstac=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"szf.cool","root":"/","images":"/images","scheme":"Gemini","darkmode":false,"version":"8.19.2","exturl":false,"sidebar":{"position":"left","display":"hide","padding":12,"offset":12},"copycode":{"enable":false,"style":null},"fold":{"enable":false,"height":500},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":true,"nav":null,"count":true,"text":true},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"},"path":"/search.xml","localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false}}</script><script src="/js/config.js"></script>

    <meta name="description" content="研读花书，皆为拙记，如有错误还望各位不惜笔墨，不啬赐教。本节主要包括正则化部分内容">
<meta property="og:type" content="article">
<meta property="og:title" content="花书——深度学习中的正则化">
<meta property="og:url" content="http://szf.cool/2024/05/16/%E8%8A%B1%E4%B9%A6%E2%80%94%E2%80%94%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%AD%E7%9A%84%E6%AD%A3%E5%88%99%E5%8C%96/">
<meta property="og:site_name" content="Szf&#39;s blog ^ ^">
<meta property="og:description" content="研读花书，皆为拙记，如有错误还望各位不惜笔墨，不啬赐教。本节主要包括正则化部分内容">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://szfmsmdx.oss-cn-hangzhou.aliyuncs.com/blog_pic/202405171131908.png">
<meta property="og:image" content="https://szfmsmdx.oss-cn-hangzhou.aliyuncs.com/blog_pic/202405171620309.png">
<meta property="og:image" content="https://szfmsmdx.oss-cn-hangzhou.aliyuncs.com/blog_pic/202405171628055.png">
<meta property="og:image" content="https://szfmsmdx.oss-cn-hangzhou.aliyuncs.com/blog_pic/202405171628143.png">
<meta property="og:image" content="https://szfmsmdx.oss-cn-hangzhou.aliyuncs.com/blog_pic/202405181104497.png">
<meta property="og:image" content="https://szfmsmdx.oss-cn-hangzhou.aliyuncs.com/blog_pic/202405181200927.png">
<meta property="og:image" content="https://szfmsmdx.oss-cn-hangzhou.aliyuncs.com/blog_pic/202405181308909.png">
<meta property="og:image" content="https://szfmsmdx.oss-cn-hangzhou.aliyuncs.com/blog_pic/202405181320130.png">
<meta property="article:published_time" content="2024-05-16T10:56:12.000Z">
<meta property="article:modified_time" content="2024-05-18T05:32:44.964Z">
<meta property="article:author" content="Szfmsmdx">
<meta property="article:tag" content="Deep Learning">
<meta property="article:tag" content="花书">
<meta property="article:tag" content="机器学习">
<meta property="article:tag" content="正则化">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://szfmsmdx.oss-cn-hangzhou.aliyuncs.com/blog_pic/202405171131908.png">


<link rel="canonical" href="http://szf.cool/2024/05/16/%E8%8A%B1%E4%B9%A6%E2%80%94%E2%80%94%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%AD%E7%9A%84%E6%AD%A3%E5%88%99%E5%8C%96/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"zh-CN","comments":true,"permalink":"http://szf.cool/2024/05/16/%E8%8A%B1%E4%B9%A6%E2%80%94%E2%80%94%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%AD%E7%9A%84%E6%AD%A3%E5%88%99%E5%8C%96/","path":"2024/05/16/花书——深度学习中的正则化/","title":"花书——深度学习中的正则化"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>花书——深度学习中的正则化 | Szf's blog ^ ^</title>
  







<link rel="dns-prefetch" href="comment.szf.cool">
  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
<style>.darkmode--activated{--body-bg-color:#282828;--content-bg-color:#333;--card-bg-color:#555;--text-color:#ccc;--blockquote-color:#bbb;--link-color:#ccc;--link-hover-color:#eee;--brand-color:#ddd;--brand-hover-color:#ddd;--table-row-odd-bg-color:#282828;--table-row-hover-bg-color:#363636;--menu-item-bg-color:#555;--btn-default-bg:#222;--btn-default-color:#ccc;--btn-default-border-color:#555;--btn-default-hover-bg:#666;--btn-default-hover-color:#ccc;--btn-default-hover-border-color:#666;--highlight-background:#282b2e;--highlight-foreground:#a9b7c6;--highlight-gutter-background:#34393d;--highlight-gutter-foreground:#9ca9b6}.darkmode--activated img{opacity:.75}.darkmode--activated img:hover{opacity:.9}.darkmode--activated code{color:#69dbdc;background:0 0}button.darkmode-toggle{z-index:9999}.darkmode-ignore,img{display:flex!important}.beian img{display:inline-block!important}</style></head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">Szf's blog ^ ^</p>
      <i class="logo-line"></i>
    </a>
      <p class="site-subtitle" itemprop="description">Yesterday you said tomorrow...</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="搜索" role="button">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup"><div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off" maxlength="80"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close" role="button">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div class="search-result-container no-result">
  <div class="search-result-icon">
    <i class="fa fa-spinner fa-pulse fa-5x"></i>
  </div>
</div>

    </div>
  </div>

</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%8F%82%E6%95%B0%E8%8C%83%E6%95%B0%E6%83%A9%E7%BD%9A"><span class="nav-number">1.</span> <span class="nav-text">参数范数惩罚</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#l2-%E5%8F%82%E6%95%B0%E6%AD%A3%E5%88%99%E5%8C%96"><span class="nav-number">1.1.</span> <span class="nav-text">\(L^2\)
参数正则化</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#l1-%E5%8F%82%E6%95%B0%E6%AD%A3%E5%88%99%E5%8C%96"><span class="nav-number">1.2.</span> <span class="nav-text">\(L^1\)
参数正则化</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E4%BD%9C%E4%B8%BA%E7%BA%A6%E6%9D%9F%E7%9A%84%E8%8C%83%E6%95%B0%E6%83%A9%E7%BD%9A"><span class="nav-number">2.</span> <span class="nav-text">作为约束的范数惩罚</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E6%AD%A3%E5%88%99%E5%8C%96%E5%92%8C%E6%AC%A0%E7%BA%A6%E6%9D%9F%E9%97%AE%E9%A2%98"><span class="nav-number">3.</span> <span class="nav-text">正则化和欠约束问题</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E6%95%B0%E6%8D%AE%E9%9B%86%E5%A2%9E%E5%BC%BA"><span class="nav-number">4.</span> <span class="nav-text">数据集增强</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%99%AA%E5%A3%B0%E9%B2%81%E6%A3%92%E6%80%A7"><span class="nav-number">5.</span> <span class="nav-text">噪声鲁棒性</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%8D%8A%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0"><span class="nav-number">6.</span> <span class="nav-text">半监督学习</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%A4%9A%E4%BB%BB%E5%8A%A1%E5%AD%A6%E4%B9%A0"><span class="nav-number">7.</span> <span class="nav-text">多任务学习</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E6%8F%90%E5%89%8D%E7%BB%88%E6%AD%A2"><span class="nav-number">8.</span> <span class="nav-text">提前终止</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%8F%82%E6%95%B0%E7%BB%91%E5%AE%9A%E5%92%8C%E5%8F%82%E6%95%B0%E5%85%B1%E4%BA%AB"><span class="nav-number">9.</span> <span class="nav-text">参数绑定和参数共享</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E7%A8%80%E7%96%8F%E8%A1%A8%E7%A4%BA"><span class="nav-number">10.</span> <span class="nav-text">稀疏表示</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#bagging-%E5%92%8C%E5%85%B6%E4%BB%96%E9%9B%86%E6%88%90%E6%96%B9%E6%B3%95"><span class="nav-number">11.</span> <span class="nav-text">Bagging 和其他集成方法</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#dropout"><span class="nav-number">12.</span> <span class="nav-text">Dropout</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%AF%B9%E6%8A%97%E8%AE%AD%E7%BB%83"><span class="nav-number">13.</span> <span class="nav-text">对抗训练</span></a></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Szfmsmdx"
      src="/images/avatar.jpg">
  <p class="site-author-name" itemprop="name">Szfmsmdx</p>
  <div class="site-description" itemprop="description">To be a pure man</div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">17</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">5</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">29</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author animated">
      <span class="links-of-author-item">
        <a href="https://github.com/szfmsmdx" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;szfmsmdx" rel="noopener me" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:szfmsmdx@163.com" title="E-Mail → mailto:szfmsmdx@163.com" rel="noopener me" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>
  <div class="cc-license animated" itemprop="license">
    <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/zh-CN" class="cc-opacity" rel="noopener" target="_blank"><img src="https://cdnjs.cloudflare.com/ajax/libs/creativecommons-vocabulary/2020.11.3/assets/license_badges/small/by_nc_sa.svg" alt="Creative Commons"></a>
  </div>

        </div>
      </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://szf.cool/2024/05/16/%E8%8A%B1%E4%B9%A6%E2%80%94%E2%80%94%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%AD%E7%9A%84%E6%AD%A3%E5%88%99%E5%8C%96/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpg">
      <meta itemprop="name" content="Szfmsmdx">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Szf's blog ^ ^">
      <meta itemprop="description" content="To be a pure man">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="花书——深度学习中的正则化 | Szf's blog ^ ^">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          花书——深度学习中的正则化
        </h1>

        

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2024-05-16 18:56:12" itemprop="dateCreated datePublished" datetime="2024-05-16T18:56:12+08:00">2024-05-16</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2024-05-18 13:32:44" itemprop="dateModified" datetime="2024-05-18T13:32:44+08:00">2024-05-18</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Deep-Learning/" itemprop="url" rel="index"><span itemprop="name">Deep Learning</span></a>
        </span>
    </span>

  
  
  <span class="post-meta-item">
    
    <span class="post-meta-item-icon">
      <i class="far fa-comment"></i>
    </span>
    <span class="post-meta-item-text">Waline：</span>
  
    <a title="waline" href="/2024/05/16/%E8%8A%B1%E4%B9%A6%E2%80%94%E2%80%94%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%AD%E7%9A%84%E6%AD%A3%E5%88%99%E5%8C%96/#waline" itemprop="discussionUrl">
      <span class="post-comments-count waline-comment-count" data-path="/2024/05/16/%E8%8A%B1%E4%B9%A6%E2%80%94%E2%80%94%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%AD%E7%9A%84%E6%AD%A3%E5%88%99%E5%8C%96/" itemprop="commentCount"></span>
    </a>
  </span>
  
  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>7.7k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>14 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    


    <div class="post-body" itemprop="articleBody"><blockquote class="blockquote-center">
<p>研读花书，皆为拙记，如有错误还望各位不惜笔墨，不啬赐教。本节主要包括<strong>正则化</strong>部分内容</p>

</blockquote>
<span id="more"></span>
<blockquote>
<p>机器学习中的一个核心问题是设计不仅在训练数据上表现好，并且能在新输入上泛化好的算法。</p>
<p>在机器学习中，许多策略显式地被设计来减少测试误差（可能会以增大训练误差为代价）。这些策略被统称为<strong>正则化</strong>。</p>
</blockquote>
<h1 id="参数范数惩罚">参数范数惩罚</h1>
<p>许多正则化方法通过对目标函数 J 添加一个参数范数惩罚
Ω(θ)，限制模型（如神经网络、线性回归或逻辑回归）的学习能力。 <span
class="math display">\[
\tilde J(\theta ;X, y)=J(\theta;X,y)+\alpha \Omega(\theta)
\]</span> 其中 <span class="math inline">\(\alpha \in [0,
+\infty]\)</span> 是权衡范数惩罚项 Ω 和标准目标函数 <span
class="math inline">\(J(X;\theta)\)</span> 相对贡献的超参数将 α 设为 0
表示没有正则化。α 越大，对应正则化惩罚越大。</p>
<blockquote>
<p>在神经网络中，参数包括每一层仿射变换的权重和偏置，我们通常只对<strong>权重</strong>做惩罚而不对偏置做正则惩罚。</p>
<ul>
<li>每个权重会指定两个变量如何相互作用。我们需要在各种条件下观察这两个变量才能良好地拟合权重。</li>
<li>而每个偏置仅控制一个单变量。这意味着，我们不对其进行正则化也不会导致太大的方差。</li>
</ul>
<p>另外，正则化偏置参数可能会导致明显的欠拟合。因此，我们使用向量 w
表示所有应受范数惩罚影响的权重，而向量 θ 表示所有参数 (包括 w
和无需正则化的参数)。</p>
</blockquote>
<p>在神经网络的情况下，有时希望对网络的每一层使用单独的惩罚，并分配不同的
α
系数。寻找合适的多个超参数的代价很大，因此为了减少搜索空间，我们会在所有层使用相同的权重衰减。</p>
<h2 id="l2-参数正则化"><span class="math inline">\(L^2\)</span>
参数正则化</h2>
<p><strong>权重衰减（weight decay）</strong> 的 <span
class="math inline">\(L^2\)</span>
参数范数惩罚，这个正则化策略通过向目标函数添加一个正则项 <span
class="math inline">\(\Omega(\theta)=\frac 12 ||w||^2\)</span>
使得权重更加接近原点。同时 <span class="math inline">\(L^2\)</span>
也被称为岭回归或 Tikhonov 正则</p>
<p>如果假定模型没有偏置参数，那么 <span
class="math inline">\(\theta\)</span> 其实就是 <span
class="math inline">\(w\)</span> ，所以目标函数应该是 <span
class="math display">\[
\tilde J(w;X,y)=\frac \alpha 2 w^T w + J(w; X, y)
\]</span> 那么他的梯度应该是 <span class="math display">\[
\nabla_w \tilde J(w;X,y)=\alpha w + \nabla_w J(w; X, y)
\]</span> 使用单步梯度下降更新权重，即执行以下更新： <span
class="math display">\[
w \gets w - \epsilon (\alpha w + \nabla_w J(w; X ,
y))=(1-\epsilon\alpha)w-\epsilon \nabla_w J(w;X,y)
\]</span> 进一步地，令 <span class="math inline">\(w^*\)</span>
是不带正则化的最小训练误差权重向量，那么 <span
class="math inline">\(\hat J(\theta)\)</span> 在 <span
class="math inline">\(w^*\)</span>
处的一阶梯度应该是为0的，我们看他的二阶近似，有 <span
class="math display">\[
\hat J(\theta)=J(w^*) + \frac 12 (w-w^*)^T H (w-w^*)
\]</span> 其中 H 是 J 在 <span class="math inline">\(w^*\)</span>
处的黑塞矩阵，当 <span class="math inline">\(\hat J\)</span>
取最小时，应该有 <span class="math inline">\(\hat J\)</span>
的梯度最小，那么 <span class="math display">\[
\nabla_w \hat J(w)=H(w-w^*)
\]</span> 为 0</p>
<p>所以，此时添加正则项，由于加法法则，所以 <span
class="math display">\[
\alpha \tilde w + H(\tilde w - w^*)=0 \\
(H+\alpha I)\tilde w=H w^* \\
\tilde w = (H + \alpha I)^{-1} H w^*
\]</span> 当 α 趋向于 0 时，正则化的解 <span
class="math inline">\(\tilde w\)</span> 会趋向于 <span
class="math inline">\(w^*\)</span>​ ，下面再研究 <span
class="math inline">\(\alpha\)</span> 增加时的特征，由于 H
是实对称的，于是可以对其进行对角化分解，于是有 <span
class="math display">\[
\begin{aligned}
\tilde w &amp;= (Q \Lambda Q^T + \alpha I)^{-1}Q \Lambda Q^T w^* \\
&amp;=[Q(\Lambda + \alpha I)Q^T]^{-1}Q\Lambda Q^T w^* \\
&amp;=Q(\Lambda + \alpha I)^{-1}\Lambda Q^T w^*
\end{aligned}
\]</span> 我们可以看到权重衰减的效果是沿着由 H 的特征向量所定义的轴缩放
<span class="math inline">\(w^*\)</span>​</p>
<blockquote>
<p>具体来说，我们会根据 <span class="math inline">\(\frac
{\lambda_i}{\lambda_i + \alpha}\)</span> 因子缩放与 H 第 i
个特征向量对其的 <span class="math inline">\(w^*\)</span>​ 的分量</p>
<p>沿着 H 特征值较大的方向 (如 λi ≫ α)正则化的影响较小。而 λi ≪ α
的分量将会收缩到几乎为零。</p>
</blockquote>
<p><img src="https://szfmsmdx.oss-cn-hangzhou.aliyuncs.com/blog_pic/202405171131908.png" alt="image-20240517113113857" style="zoom:67%;" /></p>
<p>实线椭圆表示没有正则化目标的等值线。虚线圆圈表示 <span
class="math inline">\(L^2\)</span> 正则化项的等值线。在 <span
class="math inline">\(\tilde w\)</span>
点，这两个竞争目标达到平衡。目标函数 J 的 Hessian
的第一维特征值很小。当从 <span class="math inline">\(w^*\)</span>​
水平移动时，目标函数不会增加得太多。</p>
<p>简单来说，曲率大的地方说明黑塞阵的特征值是大的，所以在该区域移动会导致较大的变化，而如图中的等高线圆，在水平方向的特征值并不大，所以能够在水平移动较大距离</p>
<h2 id="l1-参数正则化"><span class="math inline">\(L^1\)</span>
参数正则化</h2>
<p>形式地，对模型参数 w 的 <span class="math inline">\(L^1\)</span>
正则化被定义为： <span class="math display">\[
\Omega(\theta)=||w||_1=\sum_i |w_i|
\]</span> 即各个参数的绝对值之和</p>
<p>其正则化的目标函数 <span class="math inline">\(\tilde
J(w;X,y)\)</span> 如下所示： <span class="math display">\[
\tilde J(w; X, y) = \alpha ||w||_1 + J(w; X , y)
\]</span> 对应的梯度为 <span class="math display">\[
\nabla_w \tilde J(w;X,y)=\alpha \mathrm{sign}(w) + \nabla_w J(w;X,y)
\]</span> 可以看到正则化对梯度的影响不再是线性地缩放每个
wi；而是添加了一项与sign(wi)
同号的常数。使用这种形式的梯度之后，我们不一定能得到 <span
class="math inline">\(J(X,y;w)\)</span> 二次近似的直接算术解</p>
<p>同理于 <span class="math inline">\(L^2\)</span> 正则，有 <span
class="math display">\[
\nabla_w \hat J(w) = H(w - w*)
\]</span> 同样，H 是 J 在 <span class="math inline">\(w^*\)</span> 处的
Hessian 矩阵（关于 <span class="math inline">\(w\)</span> )，同样的</p>
<p>由于 <span class="math inline">\(L^1\)</span>
惩罚项在完全一般化的黑塞阵情况下，无法得到清晰的代数表达式，所以假设其是对角的<strong>（对数据进行处理，去除相关性）</strong>，即
<span class="math inline">\(H = \mathrm{diag}([H_{1,1}, \cdots,
H_{n,n}])\)</span> ，其中 <span class="math inline">\(H_{i,i} &gt;
0\)</span> ，那么可以将 <span class="math inline">\(L^1\)</span>
正则化目标函数的二次近似分解成关于参数的求和： <span
class="math display">\[
\hat J(w; X, y) = J(w^*;X, y) + \sum_i[\frac12 H_{i,i}(w_i - w_i^*)^2 +
\alpha|w_i|]
\]</span> 所以有解析解能够使得最小化这个近似代价函数 <span
class="math display">\[
w_i = \mathrm {sign}(w_i^*)\max \{|w_i^*| - \frac\alpha {H_{i,i}},0\}
\]</span> 对每个 i, 考虑 <span class="math inline">\(w_i^*&gt;0\)</span>
的情形，会有两种可能结果：</p>
<ol type="1">
<li><span class="math inline">\(w_i^* \le \frac \alpha
{H_{i,i}}\)</span> ：最优解是 <span class="math inline">\(w_i =
0\)</span></li>
<li><span class="math inline">\(w_i^* &gt; \frac \alpha
{H_{i,i}}\)</span>：这种情况下，正则化不会将 <span
class="math inline">\(w_i\)</span> 的最优值推至0，而仅仅在那个方向上移动
<span class="math inline">\(\frac \alpha {H_{i,i}}\)</span> 的距离</li>
</ol>
<p>由 L 1正则化导出的稀疏性质已经被广泛地用于 <strong>特征选择（feature
selection）</strong>机制。特征选择从可用的特征子集选择出有意义的特征，化简机器学习问题。著名的LASSO
模型将 L 1 惩罚和线性模型结合，并使用最小二乘代价函数。L 1
惩罚使部分子集的权重为零，表明相应的特征可以被安全地忽略。</p>
<h1 id="作为约束的范数惩罚">作为约束的范数惩罚</h1>
<p>其实就是引入了最优化方法里的一些技巧（KKT条件对之类）</p>
<p>首先考虑对惩罚项施加约束条件 <span class="math display">\[
\Omega(\theta) &lt; k
\]</span> 于是构建广义拉格朗日函数 <span class="math display">\[
\mathcal L(\theta,\alpha;X,y)=J(\theta;X,y) + \alpha(\Omega(\theta) - k)
\]</span> 则最优的参数为： <span class="math display">\[
\theta^* = \arg \min_\theta \max_{\alpha, \alpha\ge 0}\mathcal
L(\theta,\alpha)
\]</span> 如果 Ω 是 L 2 范数，那么权重就是被约束在一个 L 2 球中。</p>
<p>如果 Ω 是 L 1 范数，那么权重就是被约束在一个 L 1
范数限制的区域中。</p>
<p>有时候，我们希望使用显式的限制，而不是惩罚。</p>
<p>显式约束的好处：</p>
<ol type="1">
<li>可以先计算J的梯度，再去考虑满足约束的最近的θ。</li>
<li>惩罚项可能导致目标函数非凸，非凸就会导致陷入局部最优的可能性。</li>
<li>增加一定的稳定性。</li>
</ol>
<h1 id="正则化和欠约束问题">正则化和欠约束问题</h1>
<p>机器学习中的大多数模型，都依赖于对矩阵 <span
class="math inline">\(X^T X\)</span>
求逆，只要其奇异，这些方法、模型就会失效</p>
<p>但是正则化的许多形式对应求逆 <span class="math inline">\(X^T X +
\alpha I\)</span> ，这个正则化矩阵是可以保证可逆的</p>
<h1 id="数据集增强">数据集增强</h1>
<p>让机器学习模型泛化得更好的最好办法是使用更多的数据进行训练。解决这个问题的一种方法是创建假数据并添加到训练集中。</p>
<ul>
<li><p>对于图像，可以进行图像的<strong>平移/旋转/缩放/翻转/改变光照</strong>等变化。但是不能改变标记。（6不能旋转成9，b不能镜像到d）</p></li>
<li><p>语音识别任务也可以进行数据集增强。</p></li>
<li><p>数据增强的另一种方式是在<strong>输入层注入噪声</strong>(比如去噪自编码器)。也可以在隐藏单元注入噪声（抽象层的数据集增强）</p></li>
</ul>
<h1 id="噪声鲁棒性">噪声鲁棒性</h1>
<ul>
<li>在输入层（隐层）注入噪声 = 数据增强</li>
<li>在隐层注入噪声 → dropout</li>
<li>在权重（参数）中注入噪声，应用在循环神经网络，表示权重的不确定性 =
关于权重的贝叶斯推断的随机实现</li>
<li>在输出中注入噪声，对标签中的错误（噪声）显式建模，<strong>标签平滑</strong>。</li>
</ul>
<h1 id="半监督学习">半监督学习</h1>
<p>在半监督学习的框架下，P(x) 产生的未标记样本和 P(x, y)
中的标记样本都用于估计 P(y | x) 或者根据 x 预测 y</p>
<p>在深度学习的背景下，半监督学习通常指的是学习一个表示 h =
f(x)。学习表示的目的是使相同类中的样本有类似的表示。</p>
<p>我们可以构建这样一个模型，其中生成模型 P(x) 或 P(x, y) 与判别模型 P(y
| x) 共享参数，而不用分离无监督和监督部分。</p>
<h1 id="多任务学习">多任务学习</h1>
<p>将多个<strong>相关</strong>的任务放到一起学习，合并多个任务中的样例。可以视为对参数施加了软约束（因为共享给其他任务），同时合并多任务的样例从而增加了数据量（样本数量与样本间的相关信息）。</p>
<p>该模型通常可以分为两类相关的参数：</p>
<ol type="1">
<li>具体任务的参数（只能从各自任务的样本中实现良好的泛化）。如图上层</li>
<li>所有任务共享的通用参数（从所有任务的汇集数据中获益）。如图下层</li>
</ol>
<p><img src="https://szfmsmdx.oss-cn-hangzhou.aliyuncs.com/blog_pic/202405171620309.png" alt="image-20240517162029233" style="zoom:67%;" /></p>
<h1 id="提前终止">提前终止</h1>
<p>由于过拟合会在某一参数节点后，损失不断上升，所以我们为了避免这种情况，当每次<strong>验证集误差有所改善时</strong>我们存储模型参数的副本，当训练算法终止时，返回这些参数而不是最新的参数。</p>
<p>当验证集上的误差在事先指定的循环次数内没有进一步改善时，算法就会终止。</p>
<figure>
<img
src="https://szfmsmdx.oss-cn-hangzhou.aliyuncs.com/blog_pic/202405171624943.png"
alt="image-20240517162435854" />
<figcaption aria-hidden="true">image-20240517162435854</figcaption>
</figure>
<p>提前终止是一种非常不显眼的正则化形式，它几乎不需要改变基本训练过程、目标函数或一组允许的参数值。这意味着，无需破坏学习动态就能很容易地使用提前终止。</p>
<ul>
<li>提前终止可单独使用或与其他的正则化策略结合使用。</li>
<li>提前终止需要验证集。为了更好地利用这一额外的数据，我们可以在完成提前终止的首次训练之后，进行额外的训练。有两个基本的策略都可以用于第二轮训练过程。
<ul>
<li>再次初始化模型，然后使用所有数据再次训练：我们使用第一轮提前终止训练确定的最佳步数。</li>
<li>保持从第一轮训练获得的参数，然后使用全部的数据<strong>继续</strong>训练。这里因为验证集被拿来训练，所以观察平均损失函数，继续训练到低于提前终止时的目标值</li>
</ul></li>
</ul>
<p><img src="https://szfmsmdx.oss-cn-hangzhou.aliyuncs.com/blog_pic/202405171628055.png" alt="image-20240517162817980" style="zoom:80%;" /></p>
<p><img src="https://szfmsmdx.oss-cn-hangzhou.aliyuncs.com/blog_pic/202405171628143.png" alt="image-20240517162829065" style="zoom:80%;" /></p>
<h1 id="参数绑定和参数共享">参数绑定和参数共享</h1>
<p>我们经常想要表达的一种常见依赖是某些参数应当彼此接近。考虑以下情形：我们有两个模型执行相同的分类任务（具有相同类别），但输入分布稍有不同。形式地，我们有参数为
<span class="math inline">\(w^{(A)}\)</span> 的 A 模型和参数为 <span
class="math inline">\(w^{(B)}\)</span> 的 B
模型，将这两种模型输入映射到两个不同但相关的输出：<span
class="math inline">\(\hat y^{(A)}=f(w^{(A)},x)\)</span> 和 <span
class="math inline">\(\hat y^{(B)}=f(w^{(B)},x)\)</span></p>
<p>这些任务会足够相似（或许具有相似的输入和输出分布），因此我们认为模型参数应彼此靠近。我们可以通过正则化利用此信息。使用如下形式的参数范数惩罚
<span
class="math inline">\(\Omega(w^{(A)},w^{(B)})=||w^{(A)}-w^{(A)}||_2^2\)</span>​
(也可以使用其他范数或者惩罚项)</p>
<p>参数范数惩罚是正则化参数使其彼此接近的一种方式，而更流行的方法是使用约束：强迫某些参数相等。</p>
<p>由于我们将各种模型或模型组件解释为共享唯一的一组参数，这种正则化方法通常被称为
<strong>参数共享（parameter
sharing）</strong>。好处是可以减少内存，减少训练开支。主要用于卷积神经网络。</p>
<h1 id="稀疏表示">稀疏表示</h1>
<p>之前说的加上惩罚项是直接惩罚模型的参数，另一种办法是惩罚神经网络中的激活函数单元，稀疏化激活单元。</p>
<p><img src="https://szfmsmdx.oss-cn-hangzhou.aliyuncs.com/blog_pic/202405181104497.png" alt="image-20240518110409352" style="zoom: 67%;" /></p>
<p>第一个表达式是参数稀疏的线性回归模型的例子。第二个表达式是数据 x
具有稀疏表示 h 的线性回归。也就是说，h 是 x
的一个函数，在某种意义上表示存在于 x
中的信息，但只是用一个稀疏向量表示。</p>
<p>表示的正则化可以使用参数正则化中同种类型的机制实现。 <span
class="math display">\[
\tilde J(\theta;X,y)=J(\theta;X,y) + \alpha\Omega(h)
\]</span>
此外，还有一些其他方法通过激活值的硬性约束来获得表示稀疏。例如，<strong>正交匹配追踪（orthogonal
matching pursuit）</strong> 通过解决以下约束化问题将输入值 x 编码成表示
h <span class="math display">\[
\arg \min_{h,||h_0|| &lt; k} ||x - Wh||^2
\]</span> 其中 <span class="math inline">\(||h_0||\)</span> 表示 h
中非零项的个数。当 W 被约束为正交时，我们可以高效地解决这个问题。</p>
<h1 id="bagging-和其他集成方法">Bagging 和其他集成方法</h1>
<p><strong>Bagging（bootstrap
aggregating）</strong>是通过结合几个模型降低泛化误差的技术。主要想法是分别训练几个不同的模型，然后让所有模型表决测试样例的输出。这是机器学习中常规策略的一个例子，被称为
<strong>模型平均（model
averaging）</strong>。采用这种策略的技术被称为集成方法。</p>
<p><strong>模型平均（model
averaging）</strong>奏效的原因是不同的模型通常不会在测试集上产生完全相同的误差。</p>
<p>假设我们有 k 个回归模型。假设每个模型在每个例子上的误差是
ϵi，这个误差服从零均值方差为 <span class="math inline">\(\mathbb
E[\epsilon_i^2]=v\)</span> 且协方差为 <span
class="math inline">\(\mathbb E[\epsilon_i \epsilon_j]=c\)</span>
的多维正态分布，通过所有集成模型的平均预测所得误差是 <span
class="math inline">\(\frac 1k \sum_i \epsilon_i\)</span>
，所以集成预测器平方误差的期望是 <span class="math display">\[
\mathbb E[(\frac 1k \sum_i \epsilon_i)^2] = \frac 1{k^2}\mathbb
E[\sum_i(\epsilon_i^2 + \sum_{j\neq i}\epsilon_i \epsilon_j)]=\frac 1k v
+ \frac {k-1}k c
\]</span> 在误差完全相关即 c = v 的情况下，均方误差减少到
v，所以模型平均没有任何帮助。在错误完全不相关即 c = 0
的情况下，该集成平方误差的期望仅为 <span class="math inline">\(\frac 1k
v\)</span>。这意味着集成平方误差的期望会随着集成规模增大而线性减小。换言之，平均上，集成至少与它的任何成员表现得一样好，并且如果成员的误差是独立的，集成将显著地比其成员表现得更好。</p>
<p>ensemble方法可以是将完全不同的模型结合起来，而Bagging方法则是同一种模型和目标函数，但是产生k种不同的训练集，每个训练集与原训练集所含数据量相同，但会以某一概率去掉某些样本并以其他的重复样本代替，第i个模型就在第i个训练集上进行训练。例如下图中所示，我们想鉴别某个数字是否是8，原数据包含9，6和8，第一个重新取样的数据集中仅包含6和8，9倍替换掉，则模型1会学习到需要数字有上半圈才能为8，而第二个重新取样的数据集中去掉6，仅包含9和8，则模型2会学习到数字需有下半圈才能为8，将他们集合起来就可以比较准确的推测需要同时有上下半圈才鉴定数字为8。</p>
<p><img src="https://szfmsmdx.oss-cn-hangzhou.aliyuncs.com/blog_pic/202405181200927.png" alt="image-20240518120040837" style="zoom:67%;" /></p>
<h1 id="dropout">Dropout</h1>
<p>可以看作是一个深度神经网络<strong>内部</strong>的<strong>Bagging集成近似</strong>。</p>
<p>Dropout可以看作在深度神经网络内部利用<strong>子网</strong>模拟集成。子网络是通过<strong>删除</strong>非输出单元实现的，删除可以通过乘0
<strong>系数</strong>简单实现。然后再将自网络集成起来，子网络都是来源于父网络的，权重共享。</p>
<p>Dropout可以理解做是将ensemble应用到大型神经网络的一种更为实际有效的方法。由于ensemble需要训练多个模型，对于大型神经网络，其训练和评估所需时间和存储资源较大，这种方法常常不太实际，Dropout就提供了一个更便宜的解决方案：即通过随机去掉一些节点的方法训练多个子网络，并最终将这些子网络ensemble起来，如下图所示：</p>
<p><img src="https://szfmsmdx.oss-cn-hangzhou.aliyuncs.com/blog_pic/202405181308909.png" alt="image-20240518130859819" style="zoom:67%;" /></p>
<blockquote>
<p>Dropout训练由所有子网络组成的集成，<strong>其中子网络通过从基本网络中删除非输出单元构建</strong>。我们从具有两个可见单元和两个隐藏单元的基本网络开始。这四个单元有十六个可能的子集。
右图展示了从原始网络中丢弃不同的单元子集而形成的所有十六个子网络。在这个小例子中，所得到的大部分网络没有输入单元或没有从输入连接到输出的路径。当层较宽时，丢弃所有从输入
到输出的可能路径的概率变小，所以这个问题不太可能在出现层较宽的网络中。</p>
</blockquote>
<p>其具体方法是当我们利用minibatch的算法如随机梯度下降算法来学习时，我们可以随机的选取一个binary
mask(0表示节点输出为零，1表示正常输出该节点）决定哪些输入和隐藏层节点保留，每次的mask的选择是独立的。而mask为1的概率是我们可以调控的超参数。</p>
<p><font color=#00ffff>和bagging方法相比，bagging中每个模型是完全独立的，而dropout中，模型间由于继承了父网络中的参数的子集会共享一些参数，这使得在有限的存储空间中我们可以表示多个模型。</font></p>
<p>以上是训练过程，而在做inference预测时，我们需要取所有模型的预测的均值，但是这往往计算量过多，Hinton提出inference时我们实际可以只用一个模型但其中每个节点的权重需要乘以包含这个节点的概率，这种方法称作
<strong>权重比例推断规则（weight scaling inference
rule）</strong>。实际中，我们常常把weight
scaling过程放在训练过程中，即训练中每个节点输出就乘以包含该节点的概率的倒数，则inference时只需要正常的通过一遍前馈过程即可，不需要在进行weight
scaling。</p>
<h1 id="对抗训练">对抗训练</h1>
<p>Adversarial
Training对抗训练很有意思，它让人们深入思考机器学习究竟学到了什么有效信息。这方面的工作主要是由谷歌的Szegedy和本书作者Ian
Goodfellow进行的。他们可以制造一些对抗样本迷惑神经网络，如下图中所示，他们对于熊猫图片加了一些人眼不可见的干扰，形成新样本，而新的人眼仍可鉴定为熊猫的图片却会被机器以较大置信率鉴定为长臂猿。</p>
<p><img src="https://szfmsmdx.oss-cn-hangzhou.aliyuncs.com/blog_pic/202405181320130.png" alt="image-20240518132006038" style="zoom:67%;" /></p>
<blockquote>
<p>在 ImageNet 上应用 GoogLeNet的对抗样本生成的演示。</p>
<p>通过添加一个不可察觉的小向量（其中元素等于代价函数相对于输入的梯度元素的符号），我们可以改变GoogLeNet
对此图像的分类结果。</p>
</blockquote>

    </div>

    
    
    

    <div>
      
      <div>
    
        <div style="text-align:center;color: #ccc;font-size:14px;">-------------本文结束<i class="fa fa-paw"></i>感谢您的阅读-------------</div>
    
</div>
      
    </div>

    <footer class="post-footer">
          

<div class="post-copyright">
<ul>
  <li class="post-copyright-author">
      <strong>本文作者： </strong>Szfmsmdx
  </li>
  <li class="post-copyright-link">
      <strong>本文链接：</strong>
      <a href="http://szf.cool/2024/05/16/%E8%8A%B1%E4%B9%A6%E2%80%94%E2%80%94%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%AD%E7%9A%84%E6%AD%A3%E5%88%99%E5%8C%96/" title="花书——深度学习中的正则化">http://szf.cool/2024/05/16/花书——深度学习中的正则化/</a>
  </li>
  <li class="post-copyright-license">
      <strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/zh-CN" rel="noopener" target="_blank"><i class="fab fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！
  </li>
</ul>
</div>

          <div class="post-tags">
              <a href="/tags/Deep-Learning/" rel="tag"><i class="fa fa-tag"></i> Deep Learning</a>
              <a href="/tags/%E8%8A%B1%E4%B9%A6/" rel="tag"><i class="fa fa-tag"></i> 花书</a>
              <a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" rel="tag"><i class="fa fa-tag"></i> 机器学习</a>
              <a href="/tags/%E6%AD%A3%E5%88%99%E5%8C%96/" rel="tag"><i class="fa fa-tag"></i> 正则化</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2024/05/10/%E8%8A%B1%E4%B9%A6%E2%80%94%E2%80%94%E6%B7%B1%E5%BA%A6%E5%89%8D%E9%A6%88%E7%BD%91%E7%BB%9C/" rel="prev" title="花书——深度前馈网络">
                  <i class="fa fa-angle-left"></i> 花书——深度前馈网络
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/2024/05/18/%E8%8A%B1%E4%B9%A6%E2%80%94%E2%80%94%E6%B7%B1%E5%BA%A6%E6%A8%A1%E5%9E%8B%E4%B8%AD%E7%9A%84%E4%BC%98%E5%8C%96/" rel="next" title="花书——深度模型中的优化">
                  花书——深度模型中的优化 <i class="fa fa-angle-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>





    <div class="comments" id="waline"></div>
</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">

  <div class="beian"><a href="https://beian.miit.gov.cn/" rel="noopener" target="_blank">皖ICP备2023007923号-2 </a>
      <img src="https://szfmsmdx.oss-cn-hangzhou.aliyuncs.com/blog_pic/202405231028849.png" alt=""><a href="https://beian.mps.gov.cn/#/query/webSearch?code=34018102340752" rel="noopener" target="_blank">皖公网安备34018102340752号 </a>
  </div>
  <div class="copyright">
    &copy; 
    <span itemprop="copyrightYear">2024</span>
    <span class="with-love">
      <i class="fa fa-heart"></i>
    </span>
    <span class="author" itemprop="copyrightHolder">Szfmsmdx</span>
  </div>
<div class="wordcount">
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-line"></i>
    </span>
    <span title="站点总字数">84k</span>
  </span>
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
    <span title="站点阅读时长">2:32</span>
  </span>
</div>

<!--
-->

    </div>
  </footer>

  
  <div class="back-to-top" role="button" aria-label="返回顶部">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/next-theme-pjax/0.6.0/pjax.min.js" integrity="sha256-vxLn1tSKWD4dqbMRyv940UYw4sXgMtYcK6reefzZrao=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/next-boot.js"></script><script src="/js/pjax.js"></script>

  <script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-generator-searchdb/1.4.1/search.js" integrity="sha256-1kfA5uHPf65M5cphT2dvymhkuyHPQp5A53EGZOnOLmc=" crossorigin="anonymous"></script>
<script src="/js/third-party/search/local-search.js"></script>







  




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"ams","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>


<script class="next-config" data-name="waline" type="application/json">{"lang":"zh-cn","enable":true,"serverURL":"comment.szf.cool","cssUrl":"https://unpkg.com/@waline/client@v2/dist/waline.css","commentCount":true,"pageview":false,"locale":null,"placeholder":"友好交流，善意评论 ^ ^","avatar":"mm","meta":["nick","mail","link"],"pageSize":10,"visitor":false,"comment_count":true,"requiredFields":[],"libUrl":"//unpkg.com/@waline/client@v2/dist/waline.js","dark":"body.darkmode--activated","el":"#waline","comment":true,"path":"/2024/05/16/%E8%8A%B1%E4%B9%A6%E2%80%94%E2%80%94%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%AD%E7%9A%84%E6%AD%A3%E5%88%99%E5%8C%96/"}</script>
<link rel="stylesheet" href="https://unpkg.com/@waline/client@v2/dist/waline.css">
<script>
document.addEventListener('page:loaded', () => {
  NexT.utils.loadComments(CONFIG.waline.el).then(() =>
    NexT.utils.getScript(CONFIG.waline.libUrl, { condition: window.Waline })
  ).then(() => 
    Waline.init(Object.assign({}, CONFIG.waline,{ el: document.querySelector(CONFIG.waline.el) }))
  );
});
</script>
<script src="https://unpkg.com/darkmode-js@1.5.7/lib/darkmode-js.min.js"></script>

<script>
var options = {
  bottom: '64px',
  right: '64px',
  left: 'unset',
  time: '0.3s',
  mixColor: 'transparent',
  backgroundColor: 'transparent',
  buttonColorDark: '#100f2c',
  buttonColorLight: '#fff',
  saveInCookies: false,
  label: '🌓',
  autoMatchOsTheme: false
}
const darkmode = new Darkmode(options);
window.darkmode = darkmode;
darkmode.showWidget();
if (window.darkmode && !window.darkmode.isActivated()) {
  window.darkmode.toggle();
  var toggleButtons = document.getElementsByClassName("darkmode-toggle");
  if (toggleButtons && toggleButtons.length > 0) {
    for (i = 0; i < toggleButtons.length; i++) {
      toggleButtons[i].classList.add("darkmode-toggle--white");
    }
  }
}
</script>

</body>
</html>
