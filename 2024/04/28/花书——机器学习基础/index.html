<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 7.1.1">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">

<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext">

<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.1/css/all.min.css" integrity="sha256-wiz7ZSCn/btzhjKDQBms9Hx4sSeUYsDrTLg7roPstac=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"szf.cool","root":"/","images":"/images","scheme":"Gemini","darkmode":false,"version":"8.19.2","exturl":false,"sidebar":{"position":"left","display":"hide","padding":12,"offset":12},"copycode":{"enable":false,"style":null},"fold":{"enable":false,"height":500},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":true,"nav":null,"count":true,"text":true},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"},"path":"/search.xml","localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false}}</script><script src="/js/config.js"></script>

    <meta name="description" content="研读花书，皆为拙记，如有错误还望各位不惜笔墨，不啬赐教。本节主要包括机器学习基础部分内容">
<meta property="og:type" content="article">
<meta property="og:title" content="花书——机器学习基础">
<meta property="og:url" content="http://szf.cool/2024/04/28/%E8%8A%B1%E4%B9%A6%E2%80%94%E2%80%94%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/">
<meta property="og:site_name" content="Szf&#39;s blog ^ ^">
<meta property="og:description" content="研读花书，皆为拙记，如有错误还望各位不惜笔墨，不啬赐教。本节主要包括机器学习基础部分内容">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://szfmsmdx.oss-cn-hangzhou.aliyuncs.com/blog_pic/202405051408206.png">
<meta property="og:image" content="https://szfmsmdx.oss-cn-hangzhou.aliyuncs.com/blog_pic/202405051445577.png">
<meta property="og:image" content="https://szfmsmdx.oss-cn-hangzhou.aliyuncs.com/blog_pic/202405051450234.png">
<meta property="og:image" content="https://szfmsmdx.oss-cn-hangzhou.aliyuncs.com/blog_pic/202405051511901.png">
<meta property="og:image" content="https://szfmsmdx.oss-cn-hangzhou.aliyuncs.com/blog_pic/202405051542138.png">
<meta property="og:image" content="https://szfmsmdx.oss-cn-hangzhou.aliyuncs.com/blog_pic/202405051545774.png">
<meta property="og:image" content="https://szfmsmdx.oss-cn-hangzhou.aliyuncs.com/blog_pic/202405051602846.png">
<meta property="og:image" content="https://szfmsmdx.oss-cn-hangzhou.aliyuncs.com/blog_pic/202405051602712.png">
<meta property="og:image" content="https://szfmsmdx.oss-cn-hangzhou.aliyuncs.com/blog_pic/202405051626129.png">
<meta property="og:image" content="https://szfmsmdx.oss-cn-hangzhou.aliyuncs.com/blog_pic/202405051631677.png">
<meta property="og:image" content="https://szfmsmdx.oss-cn-hangzhou.aliyuncs.com/blog_pic/202405051645107.png">
<meta property="og:image" content="https://szfmsmdx.oss-cn-hangzhou.aliyuncs.com/blog_pic/202405051647077.png">
<meta property="og:image" content="https://szfmsmdx.oss-cn-hangzhou.aliyuncs.com/blog_pic/202405051649351.png">
<meta property="og:image" content="https://szfmsmdx.oss-cn-hangzhou.aliyuncs.com/blog_pic/202405051652868.png">
<meta property="og:image" content="https://szfmsmdx.oss-cn-hangzhou.aliyuncs.com/blog_pic/202405061227011.png">
<meta property="og:image" content="https://szfmsmdx.oss-cn-hangzhou.aliyuncs.com/blog_pic/202405061421749.png">
<meta property="article:published_time" content="2024-04-28T11:49:15.000Z">
<meta property="article:modified_time" content="2024-05-16T11:00:05.283Z">
<meta property="article:author" content="Szfmsmdx">
<meta property="article:tag" content="Deep Learning">
<meta property="article:tag" content="花书">
<meta property="article:tag" content="机器学习">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://szfmsmdx.oss-cn-hangzhou.aliyuncs.com/blog_pic/202405051408206.png">


<link rel="canonical" href="http://szf.cool/2024/04/28/%E8%8A%B1%E4%B9%A6%E2%80%94%E2%80%94%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"zh-CN","comments":true,"permalink":"http://szf.cool/2024/04/28/%E8%8A%B1%E4%B9%A6%E2%80%94%E2%80%94%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/","path":"2024/04/28/花书——机器学习基础/","title":"花书——机器学习基础"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>花书——机器学习基础 | Szf's blog ^ ^</title>
  







<link rel="dns-prefetch" href="comment.szf.cool">
  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
<style>.darkmode--activated{--body-bg-color:#282828;--content-bg-color:#333;--card-bg-color:#555;--text-color:#ccc;--blockquote-color:#bbb;--link-color:#ccc;--link-hover-color:#eee;--brand-color:#ddd;--brand-hover-color:#ddd;--table-row-odd-bg-color:#282828;--table-row-hover-bg-color:#363636;--menu-item-bg-color:#555;--btn-default-bg:#222;--btn-default-color:#ccc;--btn-default-border-color:#555;--btn-default-hover-bg:#666;--btn-default-hover-color:#ccc;--btn-default-hover-border-color:#666;--highlight-background:#282b2e;--highlight-foreground:#a9b7c6;--highlight-gutter-background:#34393d;--highlight-gutter-foreground:#9ca9b6}.darkmode--activated img{opacity:.75}.darkmode--activated img:hover{opacity:.9}.darkmode--activated code{color:#69dbdc;background:0 0}button.darkmode-toggle{z-index:9999}.darkmode-ignore,img{display:flex!important}.beian img{display:inline-block!important}</style></head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">Szf's blog ^ ^</p>
      <i class="logo-line"></i>
    </a>
      <p class="site-subtitle" itemprop="description">Yesterday you said tomorrow...</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="搜索" role="button">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup"><div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off" maxlength="80"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close" role="button">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div class="search-result-container no-result">
  <div class="search-result-icon">
    <i class="fa fa-spinner fa-pulse fa-5x"></i>
  </div>
</div>

    </div>
  </div>

</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95"><span class="nav-number">1.</span> <span class="nav-text">学习算法</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BB%BB%E5%8A%A1t"><span class="nav-number">1.1.</span> <span class="nav-text">任务T</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%80%A7%E8%83%BD%E5%BA%A6%E9%87%8Fp"><span class="nav-number">1.2.</span> <span class="nav-text">性能度量P</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%BB%8F%E9%AA%8Ce"><span class="nav-number">1.3.</span> <span class="nav-text">经验E</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%AE%B9%E9%87%8F%E8%BF%87%E6%8B%9F%E5%90%88%E4%B8%8E%E6%AC%A0%E6%8B%9F%E5%90%88"><span class="nav-number">2.</span> <span class="nav-text">容量、过拟合与欠拟合</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%B2%A1%E6%9C%89%E5%85%8D%E8%B4%B9%E5%8D%88%E9%A4%90%E5%AE%9A%E7%90%86"><span class="nav-number">2.1.</span> <span class="nav-text">没有免费午餐定理</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%AD%A3%E5%88%99%E5%8C%96"><span class="nav-number">2.2.</span> <span class="nav-text">正则化</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E8%B6%85%E5%8F%82%E6%95%B0%E5%92%8C%E9%AA%8C%E8%AF%81%E9%9B%86"><span class="nav-number">3.</span> <span class="nav-text">超参数和验证集</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BA%A4%E5%8F%89%E9%AA%8C%E8%AF%81"><span class="nav-number">3.1.</span> <span class="nav-text">交叉验证</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E6%9C%80%E5%A4%A7%E4%BC%BC%E7%84%B6%E4%BC%B0%E8%AE%A1"><span class="nav-number">4.</span> <span class="nav-text">最大似然估计</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%9D%A1%E4%BB%B6%E5%AF%B9%E6%95%B0%E4%BC%BC%E7%84%B6%E5%92%8C%E5%9D%87%E6%96%B9%E8%AF%AF%E5%B7%AE"><span class="nav-number">4.1.</span> <span class="nav-text">条件对数似然和均方误差</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%9C%80%E5%A4%A7%E4%BC%BC%E7%84%B6%E7%9A%84%E6%80%A7%E8%B4%A8"><span class="nav-number">4.2.</span> <span class="nav-text">最大似然的性质</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E8%B4%9D%E5%8F%B6%E6%96%AF%E7%BB%9F%E8%AE%A1"><span class="nav-number">5.</span> <span class="nav-text">贝叶斯统计</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%B4%9D%E5%8F%B6%E6%96%AF%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92"><span class="nav-number">5.1.</span> <span class="nav-text">贝叶斯线性回归</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%9C%80%E5%A4%A7%E5%90%8E%E9%AA%8Cmap%E4%BC%B0%E8%AE%A1"><span class="nav-number">5.2.</span> <span class="nav-text">最大后验（MAP）估计</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95"><span class="nav-number">6.</span> <span class="nav-text">监督学习算法</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%A6%82%E7%8E%87%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0"><span class="nav-number">6.1.</span> <span class="nav-text">概率监督学习</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA"><span class="nav-number">6.2.</span> <span class="nav-text">支持向量机</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95"><span class="nav-number">7.</span> <span class="nav-text">无监督学习算法</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90pca"><span class="nav-number">7.1.</span> <span class="nav-text">主成分分析（PCA）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#k-means%E8%81%9A%E7%B1%BB"><span class="nav-number">7.2.</span> <span class="nav-text">K-means聚类</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E9%9A%8F%E6%9C%BA%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D"><span class="nav-number">8.</span> <span class="nav-text">随机梯度下降</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%AD%E5%AD%98%E5%9C%A8%E7%9A%84%E4%B8%80%E4%BA%9B%E6%8C%91%E6%88%98"><span class="nav-number">9.</span> <span class="nav-text">机器学习中存在的一些挑战</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%BB%B4%E6%95%B0%E7%81%BE%E9%9A%BE"><span class="nav-number">9.1.</span> <span class="nav-text">维数灾难</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%B1%80%E9%83%A8%E4%B8%8D%E5%8F%98%E6%80%A7%E5%92%8C%E5%B9%B3%E6%BB%91%E6%AD%A3%E5%88%99%E5%8C%96"><span class="nav-number">9.2.</span> <span class="nav-text">局部不变性和平滑正则化</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%B5%81%E5%BD%A2%E5%AD%A6%E4%B9%A0"><span class="nav-number">9.3.</span> <span class="nav-text">流形学习</span></a></li></ol></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Szfmsmdx"
      src="/images/avatar.jpg">
  <p class="site-author-name" itemprop="name">Szfmsmdx</p>
  <div class="site-description" itemprop="description">To be a pure man</div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">18</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">5</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">28</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author animated">
      <span class="links-of-author-item">
        <a href="https://github.com/szfmsmdx" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;szfmsmdx" rel="noopener me" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:szfmsmdx@163.com" title="E-Mail → mailto:szfmsmdx@163.com" rel="noopener me" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>
  <div class="cc-license animated" itemprop="license">
    <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/zh-CN" class="cc-opacity" rel="noopener" target="_blank"><img src="https://cdnjs.cloudflare.com/ajax/libs/creativecommons-vocabulary/2020.11.3/assets/license_badges/small/by_nc_sa.svg" alt="Creative Commons"></a>
  </div>

        </div>
      </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://szf.cool/2024/04/28/%E8%8A%B1%E4%B9%A6%E2%80%94%E2%80%94%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpg">
      <meta itemprop="name" content="Szfmsmdx">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Szf's blog ^ ^">
      <meta itemprop="description" content="To be a pure man">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="花书——机器学习基础 | Szf's blog ^ ^">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          花书——机器学习基础
        </h1>

        

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2024-04-28 19:49:15" itemprop="dateCreated datePublished" datetime="2024-04-28T19:49:15+08:00">2024-04-28</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2024-05-16 19:00:05" itemprop="dateModified" datetime="2024-05-16T19:00:05+08:00">2024-05-16</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Deep-Learning/" itemprop="url" rel="index"><span itemprop="name">Deep Learning</span></a>
        </span>
    </span>

  
  
  <span class="post-meta-item">
    
    <span class="post-meta-item-icon">
      <i class="far fa-comment"></i>
    </span>
    <span class="post-meta-item-text">Waline：</span>
  
    <a title="waline" href="/2024/04/28/%E8%8A%B1%E4%B9%A6%E2%80%94%E2%80%94%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/#waline" itemprop="discussionUrl">
      <span class="post-comments-count waline-comment-count" data-path="/2024/04/28/%E8%8A%B1%E4%B9%A6%E2%80%94%E2%80%94%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/" itemprop="commentCount"></span>
    </a>
  </span>
  
  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>14k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>26 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    


    <div class="post-body" itemprop="articleBody"><blockquote class="blockquote-center">
<p>研读花书，皆为拙记，如有错误还望各位不惜笔墨，不啬赐教。本节主要包括<strong>机器学习基础</strong>部分内容</p>

</blockquote>
<span id="more"></span>
<blockquote>
<p>机器学习本质上属于应用统计学，更多地关注于如何用
计算机统计地估计复杂函数，不太关注为这些函数提供置信区间；因此我们会探讨两种统计学的主要方法：<strong>频率派估计</strong>和贝叶斯推断。</p>
</blockquote>
<p>大部分机器学习算法可以分为：</p>
<ul>
<li>监督学习：数据点都有一个<strong>标签（label）</strong>或者<strong>目标（target）</strong></li>
<li>无监督学习：从含有多特征的数据集中学习出这个数据集上有用的结构性质</li>
</ul>
<p>花书大部分的优化器都是采用的SGD对问题进行求解、计算的</p>
<h1 id="学习算法">学习算法</h1>
<p>机器学习算法是一种能够从数据中学习的算法。然而，我们所谓的 ‘‘学习’’
是什么意思呢？Mitchell (1997) 提供了一个简洁的定义：‘‘对于某类任务 T
和性能度量 P，一个计算机程序被认为可以从经验 E 中学习是指，通过经验 E
改进后，它在任务 T 上由性能度量 P 衡量的性能有所提升。”</p>
<h2 id="任务t">任务T</h2>
<p>通常机器学习任务定义为机器学习系统应该如何处理
<strong>样本（example）</strong></p>
<p>样本是
指我们从某些希望机器学习系统处理的对象或事件中收集到的已经量化的
<strong>特征 （feature）</strong>的集合。通常用一个向量 <span
class="math inline">\(\vec x\)</span> 来表示，其中 <span
class="math inline">\(x_i\)</span> 表示的是不同的特征</p>
<p>一般来说，常见的机器学习任务列举如下：</p>
<ul>
<li>分类</li>
<li>输入缺失型分类：通常不是学习某一个而是<strong>一组</strong>函数，每个函数对应着分类具有不同缺失输入子集，比如输入特征为n个，那么一共的缺失输入集合总数量就可能来到了<span
class="math inline">\(2^n\)</span></li>
<li>回归</li>
<li>机器翻译：输入是一种语言的符号序列，计算机程序需要将其转化为另一种语言的符号与劣，通常适用于NLP</li>
<li>结构化输出：结构化输出任务的输出是向量或者其他包含多个值的数据结构，
并且构成输出的这些不同元素间具有重要关系。例如语法分析——映射自然语言句子到语法结构树，并标记树的节点为动词、名词、副词等等。</li>
<li>异常检测</li>
<li>合成和采样：在这类任务中，机器学习程序生成一些和训练数据相似的新样本。==这是一类结构化输出任务，但是多了每个输入并非只有一个正确输出的
条件，并且我们明确希望输出有很多变化，这可以使结果看上去更加自然和真实。==</li>
<li>缺失值填补</li>
<li>去噪：（个人感觉有些类似于异常检测。。。</li>
</ul>
<h2 id="性能度量p">性能度量P</h2>
<p>对于诸如分类、缺失输入分类和转录任务，我们通常度量模型的
<strong>准确率（accuracy）</strong>
。通常，我们会更加关注机器学习算法在未观测数据上的性能如何，因为这将决
定其在实际应用中的性能。因此，我们使用 测试集（test
set）数据来评估系统性能， 将其与训练机器学习系统的训练集数据分开。</p>
<p>==性能度量的选择或许看上去简单且客观，但是选择一个与系统理想表现对应
的性能度量通常是很难的。==</p>
<p>在某些情况下，这是因为很难确定应该度量什么。例如，在执行转录任务时，我
们是应该度量系统转录整个序列的准确率，还是应该用一个更细粒度的指标，对序
列中正确的部分元素以正面评价？在执行回归任务时，我们应该更多地惩罚频繁犯一些中等错误的系统，还是较少犯错但是犯很大错误的系统？这些设计的选择取决
于应用。</p>
<h2 id="经验e">经验E</h2>
<p>本书中的大部分学习算法可以被理解为在整个
<strong>数据集（dataset）</strong>上获取经验。</p>
<p>大致说来，无监督学习涉及到观察随机向量 x 的好几个样本，试图显式或隐式
地学习出概率分布
p(x)，或者是该分布一些有意思的性质；而监督学习包含观察随 机向量 x
及其相关联的值或向量 y，然后从 x 预测 y，通常是估计 p(y | x)。</p>
<blockquote>
<p>无监督学习和监督学习不是严格定义的术语。它们之间界线通常是模糊的。很
多机器学习技术可以用于这两个任务。例如，概率的链式法则表明对于向量<span
class="math inline">\(x \in \mathbb R^n\)</span> ，联合分布可以分解成
<span class="math display">\[
p(x)=\Pi_{i=1}^n p(x_i|x_1,x_2,\cdots,x_{i-1})
\]</span></p>
<p>该分解意味我们可以将其拆分成 n
个监督学习问题，来解决表面上的无监督学习 p(x)。另外，求解监督学习问题
p(y|x) 时，也可以使用传统的无监督学习策略学习联合分布 p(x, y) ，然后推断
<span class="math inline">\(p(y|\mathrm x)=\frac{p(\mathrm x,
y)}{\sum_{y&#39;}p(\mathrm x, y&#39;)}\)</span>​</p>
</blockquote>
<p>尽管无监督学习和监督学习并非完全没有交集的正式概念，它们确实有助于粗略分
类我们研究机器学习算法时遇到的问题。传统地，人们将回归、分类或者结构化输
出问题称为监督学习。支持其他任务的密度估计通常被称为无监督学习。</p>
<p>学习范式的其他变种也是有可能的。例如，半监督学习中，一些样本有监督目
标，但其他样本没有。在多实例学习中，样本的整个集合被标记为含有或者不含有
该类的样本，但是集合中单独的样本是没有标记的。</p>
<p>有些机器学习算法并不是训练于一个固定的数据集上。例如，
<strong>强化学习（reinforcement
learning）</strong>算法会和环境进行交互，所以学习系统和它的训练过程会有反
馈回路。</p>
<p>表示数据集的常用方法是 <strong>设计矩阵（design
matrix）</strong>。设计矩阵的每一行包含
一个不同的样本。每一列对应不同的特征。</p>
<h1 id="容量过拟合与欠拟合">容量、过拟合与欠拟合</h1>
<p>机器学习的主要挑战是我们的算法必须能够在先前未观测的新输入上表现良好，
而不只是在训练集上表现良好。在先前未观测到的输入上表现良好的能力被称为
<strong>泛化（generalization）</strong>。</p>
<p>通常情况下，当我们训练机器学习模型时，我们可以使用某个训练集，在训练
集上计算一些被称为 <strong>训练误差（training
error）</strong>的度量误差，目标是降低训练误差。同时，我们在降低训练误差的过程中也希望<strong>泛化误差（generalization
error）</strong>很低</p>
<p>训练集和测试集数据通过数据集上被称为 <strong>数据生成过程（data
generating
process）</strong>的概率分布生成。通常，我们会做一系列被统称为
<strong>独立同分布假设（i.i.d.
assumption）</strong>的假设。该假设是说，每个数据集中的样本都是彼此
<strong>相互独立的（independent）</strong>，并且训练集和测试集是
<strong>同分布的（identically
distributed）</strong>，采样自相同的分布。这个假设使我们能够在单个样本的概率分布描述数据生成过程。然后相同的分布可以用来生成每一个训练样本和每一个测试样本。我们将这个共享的潜在
分布称为 <strong>数据生成分布（data generating
distribution）</strong>，记作 <span
class="math inline">\(p_{data}\)</span> 。</p>
<p>我们能观察到训练误差和测试误差之间的直接联系是，<strong>随机模型训练误差的期望和该模型测试误差的期望是一样的</strong>。我们采样得到训练集，然后挑选参数去降低训练集误差，然后采样得到测试集。在这个过程中，<strong>测试误差期望会大于或等于训练误差期望</strong>。以下是决定机器学习算法效果是否好的因素：</p>
<ol type="1">
<li>降低训练误差</li>
<li>缩小训练误差和测试误差的差距</li>
</ol>
<p>这两个因素对应机器学习的两个主要挑战：</p>
<ul>
<li><strong>欠拟合（underfitting）</strong>：模型不能在训练集上获得足够低的误差</li>
<li><strong>过拟合
（overfitting）</strong>：训练误差和测试误差之间的差距太大</li>
</ul>
<p>通过调整模型的
<strong>容量（capacity）</strong>，我们可以控制模型是否偏向于过拟合或者欠
拟合。通俗地，模型的容量是<strong>指其拟合各种函数的能力</strong>。容量低的模型可能很难拟合训练集；容量高的模型可能会过拟合，因为记住了不适用于测试集的训练集性质。</p>
<p>一种控制训练算法容量的方法是选择 <strong>假设空间（hypothesis
space）</strong>，即学习算法可以选择为解决方案的函数集。例如，线性回归算法将关于其输入的所有线性函数作为假设空间。广义线性回归的假设空间包括多项式函数，而非仅有线性函数。这样做就增加了模型的容量。</p>
<p>一次多项式<span class="math inline">\(\hat y=b+wx\)</span> 通过引入
<span class="math inline">\(x^2\)</span>
作为线性回归模型的另一个特征，我们能够学习到 x 的二次函数模型 <span
class="math inline">\(\hat y=b+w_1x+w_2x^2\)</span>
。尽管该模型是输入的二次函数，但输出仍是参数的线性函数。</p>
<p>当机器学习算法的容量适合于所执行任务的复杂度和所提供训练数据的数量时，
算法效果通常会最佳。容量不足的模型不能解决复杂任务。容量高的模型能够解决
复杂的任务，但是当其容量高于任务所需时，有可能会过拟合。</p>
<p><img src="https://szfmsmdx.oss-cn-hangzhou.aliyuncs.com/blog_pic/202405051408206.png" alt="image-20240505140857160" style="zoom:50%;" /></p>
<p>模型规定了调整参数降低训练目标时，学习算法可以从哪些函数族中选择函数。这被称为模型的
<strong>表示容量（representational capacity）</strong>
。在很多情况下，从这些函数中挑选出最优函数是非常困难的优化问题。实际中，学习算法不会真的找到最优函数，而仅是找到一个可以大大降低训练误差的函数。额外的限制因素，比如优化算法的不完美，意味着学习算法的
<strong>有效容量（effective
capacity）</strong>可能小于模型族的表示容量。</p>
<hr />
<p><strong>奥卡姆剃刀（Occam‘s
razor）</strong>原则：在同样能够解释已知观测现象的假设中，我们 应该挑选
‘‘最简单’’ 的那一个。</p>
<p>统计学习理论提供了量化模型容量的不同方法。在这些中，最有名的是
<strong>VapnikChervonenkis 维度（Vapnik-Chervonenkis dimension,
VC）</strong>。VC维定义为该分类器<strong>能够分类的训练样本的最大数目</strong>。</p>
<p>假设存在 m 个不同 x 点的训练集，分类器可以任意地标记该 m 个不同的 x
点，VC维被定义为 m 的最大可能值</p>
<p>参见<a
target="_blank" rel="noopener" href="https://tangshusen.me/2018/12/09/vc-dimension/#comments">这篇文章</a>对
VC 维的推导</p>
<p>简单来说，在该模型对应的空间中随机撒x点，然后对其中的每个点随机分配一个2类标签，使用你的模型来分类，并且要分对，请问x至多是多少。这个x就是VC维。</p>
<p>比如对于线性函数来说：</p>
<p>如果是二维空间里的线性函数的话，那么他的VC维应该是3，因为对于4个点，存在一种情况不管用什么直线都无论如何做不到完全分类的</p>
<p><img src="https://szfmsmdx.oss-cn-hangzhou.aliyuncs.com/blog_pic/202405051445577.png" alt="image-20240505144512543" style="zoom:50%;" /></p>
<p>如果选择三维空间的线性函数的话，那么该模型的VC维应该是4</p>
<p>所以VC维越大，也就是说他能够包含的情况越多，假设空间大，也就是他的容量大</p>
<hr />
<p>我们必须记住虽然更简单的函数更可能泛化，但我们仍然需要选择一个充分复杂的假设以达到低的训练误差。</p>
<p>通常，当模型容量上升时，训练误差会下降，直到其渐近最小可能误差（假设误差度量有最小值）。通常，泛化误差是一个关于模型容量的
U 形曲线函数。</p>
<p><img src="https://szfmsmdx.oss-cn-hangzhou.aliyuncs.com/blog_pic/202405051450234.png" alt="image-20240505145055192" style="zoom:60%;" /></p>
<p>为考虑容量任意高的极端情况，我们介绍
<strong>非参数（non-parametric）</strong>模型的概念。有时，非参数模型仅是一些不能实际实现的理论抽象（比如搜索所有可能概率分布的算法）。然而，我们也可以设计一些实用的非参数模型，使它们的复杂度和训练集大小有关。</p>
<h2 id="没有免费午餐定理">没有免费午餐定理</h2>
<p>机器学习的 <strong>没有免费午餐定理（no free lunch
theorem）</strong>表明 (Wolpert,
1996)，在所有可能的数据生成分布上平均之后，每一个分类算法在未事先观测的点上都有相同的错误率。换言之，在某种意义上，没有一个机器学习算法总是比其他的要好。我们能够设想的最先进的算法和简单地将
所有点归为同一类的简单算法有着相同的平均性能（在所有可能的任务上）。</p>
<h2 id="正则化">正则化</h2>
<p>没有免费午餐定理暗示我们必须在特定任务上设计性能良好的机器学习算法。</p>
<p>算法的效果不仅很大程度上受影响于假设空间的函数数量，也取决于这些函数
的具体形式。例如，我们用线性回归，从 x 预测
sin(x)，效果不会好。因此我们可以通过两种方式控制算法的性能：</p>
<ol type="1">
<li>允许使用的函数种类</li>
<li>这些函数的数量</li>
</ol>
<p>在假设空间中，相比于某一个学习算法，我们可能更偏好另一个学习算法。这
意味着两个函数都是符合条件的，但是我们更偏好其中一个。只有非偏好函数比偏
好函数在训练数据集上效果明显好很多时，我们才会考虑非偏好函数。</p>
<p>例如，我们可以加入 <strong>权重衰减（weight
decay）</strong>来修改线性回归的训练标准。带权重衰减的线性回归最小化训练集上的均方误差和正则项的和
J(w)，其偏好于平方 <span class="math inline">\(L_2\)</span>
范数较小的权重。具体如下： <span class="math display">\[
J(w)=\mathrm {MSE}_{\mathrm {train}}+\lambda w^T w
\]</span></p>
<p>其中 λ 是提前挑选的值，控制我们偏好小范数权重的程度。当 λ =
0，我们没有任何偏好。越大的 λ 偏好范数越小的权重。</p>
<p>更一般地，正则化一个学习函数 f(x; θ)
的模型，我们可以给代价函数添加被称为
<strong>正则化项（regularizer）</strong>的惩罚。在权重衰减的例子中，正则化项是
<span class="math inline">\(\Omega(w)=w^T w\)</span></p>
<p><img src="https://szfmsmdx.oss-cn-hangzhou.aliyuncs.com/blog_pic/202405051511901.png" alt="image-20240505151147854" style="zoom:67%;" /></p>
<p>在我们权重衰减的示例中，通过在最小化的目标中额外增加一项，我们明确地表示了偏好权重较小的线性函数。有很多其他方法隐式或显式地表示对不同解的偏好。总而言之，这些不同的方法都被称为
<strong>正则化（regularization）</strong>。正则化是指我们修改学习算法，使其降低<strong>泛化误差</strong>而<strong>非训练误差</strong>。</p>
<h1 id="超参数和验证集">超参数和验证集</h1>
<p>参考了<a
target="_blank" rel="noopener" href="https://blog.csdn.net/qq_24884193/article/details/104071664">这篇文章</a></p>
<p>几个数据集的辨析。。。</p>
<ul>
<li>训练集：训练集用来训练模型，即确定模型的权重和偏置这些参数，通常我们称这些参数为学习参数。</li>
<li>测试集：与训练集同分布的样本组成，只使用一次，即在训练完成后评价最终的模型时使用。</li>
<li>验证集：为了挑选超参数，比如网络层数、网络节点数、迭代次数、学习率这些都叫超参数。</li>
</ul>
<p>通常，80% 的训练数据用于训练，20% 用于验证。由于验证集是用来 ‘‘训练’’
超参数的，尽管验证集的误差通常会比训练集误差小，验证集会低估泛化误差。所有超参数优化完成之后，泛化误差可能会通过测试集来估计。</p>
<figure>
<img
src="https://szfmsmdx.oss-cn-hangzhou.aliyuncs.com/blog_pic/202405051521597.png"
alt="image-20240505152122552" />
<figcaption aria-hidden="true">image-20240505152122552</figcaption>
</figure>
<h2 id="交叉验证">交叉验证</h2>
<p>之所以出现交叉验证，主要是因为训练集较小。无法直接像前面那样只分出训练集，验证集，测试就可以了（简单交叉验证）。</p>
<p>需要说明的是，在实际情况下，<u>人们不是很喜欢用交叉验证</u>，主要是因为它会<strong>耗费较多的计算资源</strong>。一般直接把训练集按照50%-90%的比例分成训练集和验证集。但这也是根据具体情况来定的：如果超参数数量多，你可能就想用更大的验证集，而验证集的数量不够，那么最好还是用交叉验证吧。至于分成几份比较好，一般都是分成3、5和10份。</p>
<figure>
<img
src="https://szfmsmdx.oss-cn-hangzhou.aliyuncs.com/blog_pic/202405051523070.png"
alt="image-20240505152339013" />
<figcaption aria-hidden="true">image-20240505152339013</figcaption>
</figure>
<p>假设将训练集分成5份（该数目被称为折数，5-fold交叉验证），每次都用其中4份来训练模型，粉红色的那份用来验证4份训练出来的模型的准确率，记下准确率。</p>
<p><code>然后再次在这5份中取另外4份做训练集，1份做验证集，再次得到一个模型的准确率</code></p>
<p>(五选四的话，只用做五遍就ok了)
直到所有5份都做过1次验证集，也即验证集名额循环了一圈，交叉验证的过程就结束。算得这5次准确率的均值。留下准确率最高的模型，即该模型的超参数是什么样的最终模型的超参数就是这个样的。</p>
<h1 id="最大似然估计">最大似然估计</h1>
<p>考虑一组含有 m 个样本的数据集 <span class="math inline">\(\mathbb
X=\{x^{(1)},\cdots,x^{(m)}\}\)</span> ，独立地由未知的真实数据生成分布
<span class="math inline">\(p_{data}(\mathrm x)\)</span> 生成</p>
<p>令 <span class="math inline">\(p_{model}(\mathrm x;\theta)\)</span>
是一族<strong>由 <span class="math inline">\(\theta\)</span>
确定在相同空间上的概率分布</strong>。换言之， <span
class="math inline">\(p_{model}(x;\theta)\)</span> 将任意输入 x
映射到实数来估计真实概率 <span
class="math inline">\(p_{data}(x)\)</span></p>
<p>对 θ 的最大似然估计被定义为：</p>
<p><img src="https://szfmsmdx.oss-cn-hangzhou.aliyuncs.com/blog_pic/202405051542138.png" alt="image-20240505154225093" style="zoom:50%;" /></p>
<p>多个概率的乘积会因很多原因不便于计算。例如，计算中很可能会出现数值下溢。为了得到一个便于计算的等价优化问题，我们观察到似然对数不会改变其
arg max 但是将乘积转化成了便于计算的求和形式：</p>
<p><img src="https://szfmsmdx.oss-cn-hangzhou.aliyuncs.com/blog_pic/202405051545774.png" alt="image-20240505154529731" style="zoom:50%;" /></p>
<blockquote>
<p>一种解释最大似然估计的观点是将它看作最小化训练集上的经验分布 <span
class="math inline">\(\hat p_{data}\)</span>
和模型分布之间的差异，两者的差异程度可以通过KL散度衡量。即 <span
class="math inline">\(D_{KL}(\hat p_{data} || p_{model})=\mathbb
E_{\mathrm x  \sim \hat p_{model}}[\log \hat p_{data}(x)-\log
p_{model}(x)]\)</span></p>
<p>左边一项仅涉及到数据生成过程，和模型无关。这意味着当我们训练模型最小化
KL散度时，我们只需要最小化<span class="math inline">\(-\mathbb
E_{\mathrm x  \sim \hat p_{model}}[\log p_{model}(x)]\)</span></p>
</blockquote>
<h2 id="条件对数似然和均方误差">条件对数似然和均方误差</h2>
<p>最大似然估计很容易扩展到估计条件概率 <span
class="math inline">\(P(\mathrm y|\mathrm x;\theta)\)</span> ，从而给定
x 预测 y。实际上这是最常见的情况，因为这构成了大多数监督学习的基础。如果
X 表示所有的输入，Y 表示我们观测到的目标，那么条件最大似然估计是 <span
class="math inline">\(\theta_{ML}=\mathrm {arg}\max_\theta
P(Y|X;\theta)\)</span> 如果假设样本是独立同分布的，那么这可以分解成
<span class="math display">\[
\theta_{ML}=\mathrm{arg}\max_\theta \sum_{i=1}^m \log
P(y^{(i)}|x^{(i)};\theta)
\]</span> 在最大似然的角度下重新审视线性回归，在假设样本是
i.i.d，那么条件对数似然如下：</p>
<p><img src="https://szfmsmdx.oss-cn-hangzhou.aliyuncs.com/blog_pic/202405051602846.png" alt="image-20240505160206791" style="zoom:67%;" /></p>
<p>其中 <span class="math inline">\(y^{(i)}\)</span>​ 是线性回归在第 i
个输入 x (i) 上的输出，m 是训练样本的数目。对比均方 误差和对数似然</p>
<p><img src="https://szfmsmdx.oss-cn-hangzhou.aliyuncs.com/blog_pic/202405051602712.png" alt="image-20240505160242665" style="zoom:67%;" /></p>
<h2 id="最大似然的性质">最大似然的性质</h2>
<p>最大似然估计最吸引人的地方在于，它被证明当样本数目 <span
class="math inline">\(m\to \infty\)</span>
时，就收敛率而言是最好的渐近估计。</p>
<p>在合适的条件下，最大似然估计具有一致性，意味着训练样本数目趋向于无穷大时，参数的最大似然估计会收敛到参数的真实值。这些条件是：</p>
<ul>
<li>真实分布 <span class="math inline">\(p_{data}\)</span> 必须在模型族
<span class="math inline">\(p_{model}(·;\theta)\)</span>
中。否则，没有估计可以还原 <span
class="math inline">\(p_{data}\)</span></li>
<li>真实分布 <span class="math inline">\(p_{data}\)</span>
必须刚好对应一个 θ 值。否则，最大似然估计恢复出真实分布 <span
class="math inline">\(p_{data}\)</span>
后，也不能决定数据生成过程使用哪个 θ。</li>
</ul>
<h1 id="贝叶斯统计">贝叶斯统计</h1>
<blockquote>
<p>老实说这块我学的云里雾里，有时间会好好学一遍贝叶斯统计。。。</p>
</blockquote>
<p>至此我们已经讨论了 <strong>频率派统计（frequentist
statistics）</strong>方法和基于估计单一值 θ
的方法，然后基于该估计作所有的预测。另一种方法是在做预测时会考虑所有可能的
θ。后者属于 <strong>贝叶斯统计（Bayesian
statistics）</strong>的范畴。</p>
<p>贝叶斯用概率反映知识状态的确定性程度。数据集能够被直接观测到，因此不是随机的。另一方面，真实参数
θ 是未知或不确定的，因此可以表示成随机变量。</p>
<p>在观察到数据前，我们将 θ 的已知知识表示成 <strong>先验概率分布（prior
probability distribution）</strong>，p(θ)（有时简单地称为
‘‘先验’’）。一般而言，机器学习实践者会选择一个相当宽泛的（即，高熵的）先验分布，反映在观测到任何数据前参数
θ 的高度不确定性。例如，我们可能会假设先验 θ
在有限区间中均匀分布。<strong>许多先验偏好于‘‘更简单’’
的解</strong>（如小幅度的系数，或是接近常数的函数）。</p>
<p>假设我们有一组数据样本 <span
class="math inline">\(\{x^{(1)},\cdots,x^{(m)}\}\)</span>，通过贝叶斯规则结合数据似然
<span class="math inline">\(p(x^{(1)},\cdots,x^{(m)}|\theta)\)</span>
和先验，我们可以恢复数据对我们关于 θ 信念的影响：</p>
<p><img src="https://szfmsmdx.oss-cn-hangzhou.aliyuncs.com/blog_pic/202405051626129.png" alt="image-20240505162658072" style="zoom: 50%;" /></p>
<p>在贝叶斯估计常用的情景下，先验开始是相对均匀的分布或高熵的高斯分布，观测数据通常会使后验的熵下降，并集中在参数的几个可能性很高的值。</p>
<p>相对于最大似然估计，贝叶斯估计有两个重要区别。<strong>第一</strong>，不像最大似然方法预测时使用
θ 的点估计，贝叶斯方法使用 θ 的全分布。例如，在观测到 m
个样本后，下一个数据样本 x (m+1) 的预测分布如下：</p>
<p><img src="https://szfmsmdx.oss-cn-hangzhou.aliyuncs.com/blog_pic/202405051631677.png" alt="image-20240505163108624" style="zoom: 50%;" /></p>
<p>这里，每个具有正概率密度的 θ
的值有助于下一个样本的预测，其中贡献由后验密度本身加权。在观测到数据集
<span class="math inline">\(\{x^{(1)},\cdots,x^{(m)}\}\)</span>
之后，如果我们仍然非常不确定 θ
的值，那么这个不确定性会直接包含在我们所做的任何预测中。</p>
<p>贝叶斯方法和最大似然方法的<strong>第二</strong>个最大区别是由贝叶斯先验分布造成的。先验能够影响<strong>概率质量密度朝参数空间中偏好先验的区域偏移</strong>。实践中，先验通常表现为偏好更简单或更光滑的模型。对贝叶斯方法的批判认为先验是人为主观判断影响预测的来源。</p>
<blockquote>
<p>当训练数据很有限时，贝叶斯方法通常泛化得更好，但是当训练样本数目很大时，通常会有很大的计算代价。</p>
</blockquote>
<h2 id="贝叶斯线性回归">贝叶斯线性回归</h2>
<p>使用贝叶斯估计方法学习线性回归的参数</p>
<p>在线性回归中，我们学习从输入向量 <span class="math inline">\(x\in
\mathbb R^n\)</span> 预测标量 <span class="math inline">\(y\in
R\)</span> 的线性映射。该预测由向量 <span class="math inline">\(w\in
\mathbb R^n\)</span> 参数化：<span class="math inline">\(\hat y=w^T
x\)</span></p>
<p>给定一组 m 个训练样本 <span
class="math inline">\((X^{(train)},y^{(train)})\)</span>
可以表示整个训练集对 y 的预测： <span class="math inline">\(\hat
y^{(train)}=X^{(train)} w\)</span> 表示为<span
class="math inline">\(y^{(train)}\)</span> 上的高斯条件分布</p>
<p><img src="https://szfmsmdx.oss-cn-hangzhou.aliyuncs.com/blog_pic/202405051645107.png" alt="image-20240505164530045" style="zoom: 50%;" /></p>
<p>其中，我们根据标准的 MSE 公式假设 y 上的高斯方差为
1。为确定模型参数向量 w
的后验分布，我们首先需要指定一个先验分布。实数值参数通常使用高斯作为先验分布：</p>
<p><img src="https://szfmsmdx.oss-cn-hangzhou.aliyuncs.com/blog_pic/202405051647077.png" alt="image-20240505164730017" style="zoom:50%;" /></p>
<p>其中，<span class="math inline">\(\mu_0\)</span> 和 <span
class="math inline">\(\Lambda_0\)</span>
分别是先验分布的均值向量和协方差矩阵。</p>
<p>确定好先验后，我们现在可以继续确定模型参数的后验分布。</p>
<p><img src="https://szfmsmdx.oss-cn-hangzhou.aliyuncs.com/blog_pic/202405051649351.png" alt="image-20240505164924280" style="zoom: 50%;" /></p>
<p>现在我们定义 <span class="math inline">\(\Lambda_m=(X^T
X+\Lambda_0^{-1})^{-1}\)</span> 和 <span
class="math inline">\(\mu_m=\Lambda_m(X^T
y+\Lambda_0^{-1}\mu_0)\)</span>
，利用新变量可以将后验改写为高斯分布：</p>
<p><img src="https://szfmsmdx.oss-cn-hangzhou.aliyuncs.com/blog_pic/202405051652868.png" alt="image-20240505165252802" style="zoom:50%;" /></p>
<p>大多数情况下，我们设置 <span class="math inline">\(\mu_0=0\)</span>
。如果我们设置 <span class="math inline">\(\Lambda_0=\frac 1\alpha
I\)</span> ，那么 <span class="math inline">\(\mu_m\)</span> 对 w
的估计就和频率派带权重衰减惩罚 <span class="math inline">\(\alpha w^T
w\)</span>​ 的线性回归的估计是一样的。</p>
<h2 id="最大后验map估计">最大后验（MAP）估计</h2>
<p>原则上，我们应该使用参数 θ
的完整贝叶斯后验分布进行预测，但单点估计常常也是需要的。MAP
估计选择后验概率最大的点（或在 θ
是连续值的更常见情况下，概率密度最大的点）：</p>
<p><span class="math display">\[\theta_\mathrm {MAP}=\mathrm {arg}
\max_\theta p(\theta | x)=\mathrm {arg}\max_\theta \log p(x|\theta)+\log
p(\theta)\]</span></p>
<p>我们可以认出上式右边的 <span class="math inline">\(\log
p(x|\theta)\)</span> 对应着标准的对数似然项，<span
class="math inline">\(\log p(\theta)\)</span> 对应先验分布</p>
<h1 id="监督学习算法">监督学习算法</h1>
<p>粗略地说，监督学习算法是给定一组输入 x 和输出 y
的训练集，学习如何关联输入和输出。很多情况下， 由于 y
很难自动收集，需要人来提供“监督”（不过该术语仍然适用于训练集目标可以被自动收集的情况</p>
<h2 id="概率监督学习">概率监督学习</h2>
<p>花书大部分监督学习算法都是基于估计概率分布 <span
class="math inline">\(p(y|\mathrm x)\)</span>
的，可以使用最大似然估计找到对于有参分布族 <span
class="math inline">\(p(y|\mathrm x;\theta)\)</span> 最好的参数向量
<span class="math inline">\(\theta\)</span></p>
<p>显然线性回归对应分布族为 <span
class="math inline">\(p(y|x;\theta)=\mathcal N(y;\theta^T
x,I)\)</span></p>
<p>我们用于线性回归的实数正态分布是用均值参数化的。二元变量上的分布稍微复杂些，因为它的均值必须始终在
0 和 1之间。解决这个问题的一种方法是使用 logistic sigmoid
函数将线性函数的输出压缩进区间 (0, 1)。该值可以解释为概率： <span
class="math inline">\(p(y=1|x;\theta)=\sigma(\theta^T x)\)</span></p>
<p>这个方法被称为 <strong>逻辑回归（logistic
regression）</strong>这个名字有点奇怪，因为该模型用于分类而非回归。</p>
<h2 id="支持向量机">支持向量机</h2>
<p><strong>支持向量机（support vector machine,
SVM）</strong>是监督学习中最有影响力的方法之一。不同于逻辑回归的是，支持向量机不输出概率，只输出类别。当
<span class="math inline">\(w^T x+b\)</span>
为正时，支持向量机预测属于正类。类似地，当 <span
class="math inline">\(w^T+b\)</span>
为负时，支持向量机预测属于负类。</p>
<p>支持向量机的一个重要创新是 核技巧（kernel
trick）。核技巧观察到许多机器学习算法都可以写成样本间点积的形式。例如，支持向量机中的线性函数可以重写为
<span class="math display">\[
w^T x+b=b+\sum_{i=1}^m\alpha_i x^T x^{(i)}
\]</span> 其中 <span class="math inline">\(x^{(i)}\)</span>
是训练样本，<span class="math inline">\(\alpha\)</span>
是系数向量，学习算法重写为这种形式允许我们将 x 替换为特征函数 <span
class="math inline">\(\phi (x)\)</span>
的输出，点积替换为被称为核函数（kernel function）的函数 <span
class="math inline">\(k(x, x^{(i)})=\phi(x)·\phi(x^{(i)})\)</span></p>
<p>使用核估计替换点积之后，我们可以使用如下函数进行预测： <span
class="math display">\[
f(x)=b+\sum_i \alpha_i k(x,x^{(i)})
\]</span> 这个函数关于 x 是非线性的，关于 ϕ(x) 是线性的。α 和 f(x)
之间的关系也是线性的。核函数完全等价于用 ϕ(x)
预处理所有的输入，然后在新的转换空间学习线性模型。</p>
<p>核技巧有两大优势：</p>
<ol type="1">
<li>它使我们能够使用保证有效收敛的凸优化技术来学习非线性模型（关于 x
的函数）</li>
<li>核函数 k 的实现方法通常有比直接构建 ϕ(x) 再算点积高效很多。</li>
</ol>
<p>在某些情况下，<span class="math inline">\(\phi(x)\)</span>
甚至可以是无限维的，对于普通的显式方法而言，这将是无限的计算代价。在很多情况下，即使
<span class="math inline">\(\phi(x)\)</span> 是难算的，<span
class="math inline">\(k(x,x&#39;)\)</span> 却会是一个关于
x非线性的、易算的函数。假设这个映射返回一个由开头 x 个 1，随后是无限个0
的向量。我们可以写一个核函数 <span
class="math inline">\(k(x,x&#39;)=\min(x,x&#39;)\)</span>
，完全等价于对应的无限维点积。</p>
<p>最常用的核函数是 <strong>高斯核（Gaussian kernel）</strong>： <span
class="math display">\[
k(u,v)=\mathcal N(u-v;0,\sigma^2I)
\]</span> 其中 <span class="math inline">\(\mathcal
N(x;\mu,\Sigma)\)</span>​ 是标准正态密度。这个核也被称为
<strong>径向基函数（radial basis function, RBF）</strong>核，因为其值沿
v 中从 u
向外辐射的方向减小。高斯核对应于<strong>无限维空间</strong>中的点积，但是该空间的推导没有整数上最小核的示例那么直观。</p>
<p>我们可以认为高斯核在执行一种<strong>模板匹配 (template
matching)</strong>。训练标签 y 相关的训练样本 x 变成了类别 y
的模版。当测试点 x ′ 到 x
的欧几里得距离很小，对应的高斯核响应很大时，表明 x ′ 和模版 x
非常相似。该模型进而会赋予相对应的训练标签 y
较大的权重。总的来说，预测将会组合很多这种通过训练样本相似度加权的训练标签。</p>
<p>核机器的一个主要缺点是计算决策函数的成本关于训练样本的数目是线性的。因为第
i 个样本贡献 <span class="math inline">\(\alpha_i k(x,x^{(i)})\)</span>
到决策函数。支持向量机能够通过学习主要包含零的向量
α，以缓和这个缺点。那么判断新样本的类别仅需要计算非零 αi
对应的训练样本的核函数。这些训练样本被称为 <strong>支持向量（support
vector）</strong></p>
<h1 id="无监督学习算法">无监督学习算法</h1>
<p>本质上来说，无监督学习只处理“特征”而不过多将注意力放在监督信号上。</p>
<p>经典的无监督学习任务是找到数据的“最佳”表示，常见的三种包括：</p>
<ul>
<li>低维表示：低维表示尝试将 x 中的信息尽可能压缩在一个较小的表示中</li>
<li>稀疏表示：将数据集嵌入到输入项大多数为零的表示中（通常适用于<strong>增加表示维度的情况</strong></li>
<li>独立表示：试图分开数据分布中变化的来源，使得表示的维度是统计独立的。</li>
</ul>
<h2 id="主成分分析pca">主成分分析（PCA）</h2>
<p>PCA可以视作学习一种比原始输入维数更低的表示，他也学习了元素之间彼此没有线性相关的表示。</p>
<p><img src="https://szfmsmdx.oss-cn-hangzhou.aliyuncs.com/blog_pic/202405061227011.png" alt="image-20240506122705917" style="zoom:67%;" /></p>
<blockquote>
<p>PCA 学习一种线性投影，使最大方差的方向和新空间的轴对齐。(左)
原始数据包含了 x
的样本。在这个空间中，方差的方向与轴的方向并不是对齐的。(右)
变换过的数据 <span class="math inline">\(z=x^T W\)</span> 在轴 <span
class="math inline">\(z_1\)</span>
的方向上有最大的变化。第二大变化方差的方向沿着轴 <span
class="math inline">\(z_2\)</span> 。</p>
</blockquote>
<p>假设有一个 <span class="math inline">\(m\times n\)</span> 的设计矩阵
<span class="math inline">\(X\)</span> ，数据均值为0，即 <span
class="math inline">\(\mathbb E[x]=0\)</span> ，那么 <span
class="math inline">\(X\)</span> 对应的无偏协方差矩阵应该是 <span
class="math inline">\(Var[x]=\frac 1{m-1} X^T X\)</span></p>
<p>那么 PCA 通过线性变换找到一个 <span class="math inline">\(\mathrm
{Var}[z]\)</span> 是对角阵，（在忽略常数的情况下有）<span
class="math inline">\(Z=X W\)</span> ，再看 z 的协方差矩阵也就是 <span
class="math inline">\(W^T X^T X W=\Lambda\)</span> ，所以自然有 <span
class="math inline">\(X^T X=W\Lambda W^T\)</span></p>
<p>通过SVD分解也可以推导PCA，同时更能说明其PCA后的 <span
class="math inline">\(\mathrm {Var}[z]\)</span> 是对角的： <span
class="math display">\[
X^T X=(U\Sigma W^T)^T U\Sigma W^T=W \Sigma^2 W^T
\]</span> 使用 X 的 SVD 分解，X 的方差可以表示为 <span
class="math display">\[
\begin{align}
\mathrm {Var}[x]&amp;=\frac 1{m-1} X^T X \\
&amp;= \frac 1{m-1} (U\Sigma W^T)^T U\Sigma W^T \\
&amp;= \frac 1{m-1} W\Sigma^T U^T U\Sigma W^T \\
&amp;= \frac 1{m-1} W\Sigma ^2 W^T
\end{align}
\]</span> 那么再观察 z 的协方差矩阵： <span class="math display">\[
\begin{align}
\mathrm {Var}[z]&amp;=\frac 1{m-1} Z^T Z \\
&amp;=\frac 1{m-1}W^T X^T X W \\
&amp;=\frac 1{m-1} W^T W \Sigma ^2 W^T W \\
&amp;=\frac 1{m-1} \Sigma^2
\end{align}
\]</span> 以上分析指明当我们通过线性变换 W 将数据 x 投影到 z
时，得到的数据表示的协方差矩阵是对角的（即 Σ 2），立刻可得 z
中的元素是彼此无关的。</p>
<p><font color=#00FFFF>PCA
这种将数据变换为元素之间彼此不相关表示的能力是 PCA
的一个重要性质。它是消除数据中未知变化因素的简单表示示例。在 PCA
中，这个消除是通过寻找输入空间的一个旋转（由 W
确定），使得方差的主坐标和 z 相关的新表示空间的基对齐。</font></p>
<h2 id="k-means聚类">K-means聚类</h2>
<p>k-均值聚类算法将训练集分成
k个靠近彼此的不同样本聚类。因此我们可以认为该算法提供了 k-维的 one-hot
编码向量 h 以表示输入 x。当 x 属于聚类 i 时，有 <span
class="math inline">\(h_i=1\)</span> ，h 的其他项为零。</p>
<p>k-均值聚类提供的 one-hot
编码也是一种稀疏表示，因为每个输入的表示中大部分元素为零。</p>
<p>k-均值聚类初始化 k 个不同的中心点 <span
class="math inline">\(\{\mu^{(1)},\cdots,\mu^{(k)} \}\)</span>
，然后迭代交换两个不同的步骤直到收敛</p>
<ul>
<li>步骤一，每个训练样本分配到最近的中心点 <span
class="math inline">\(\mu^{(i)}\)</span> 所代表的聚类 i</li>
<li>步骤二，每一个中心点 <span class="math inline">\(\mu^{(i)}\)</span>
更新为聚类 i 中所有训练样本 <span class="math inline">\(x^{(j)}\)</span>
的均值</li>
</ul>
<blockquote>
<p>关于聚类的一个问题是聚类问题本身是病态的。这是说没有单一的标准去度量聚类的数据在真实世界中效果如何。我们可能希望找到和
一个特征相关的聚类，但是得到了一个和任务无关的，同样是合理的不同聚类。</p>
<p>例如，假设我们在包含红色卡车图片、红色汽车图片、灰色卡车图片和灰色汽车图片的数据集上运行两个聚类算法。如果每个聚类算法聚两类，那么可能一个算法将汽车和卡车各聚一类，另一个根据红色和灰色各聚一类。</p>
</blockquote>
<h1 id="随机梯度下降">随机梯度下降</h1>
<p>机器学习算法中的代价函数通常可以分解成每个样本代价函数的总和，比如：训练数据的负条件对数似然函数通常可以写成
<span class="math display">\[
J(\theta)=\mathbb E_{x,y\sim \hat p_{data}}L(x,y,\theta)=\frac 1m
\sum_{i=1}^m L(x^{(i)},y^{(i)},\theta)
\]</span> 其中 L 是每个样本的损失 <span
class="math inline">\(L(x,y,\theta)=-\log p(y|x;\theta)\)</span></p>
<p>对于这些相加的代价函数，梯度下降需要计算 <span
class="math display">\[
\nabla_\theta J(\theta)=\frac 1m \sum_{i=1}^m \nabla_\theta
L(x^{(i)},y^{(i)},\theta)
\]</span> 这个运算的计算代价是
O(m)。不过可以用小批量梯度下降去做近似，因为期望总是相同的（如果在 i.i.d
的条件下，使用小批量会比单样本的方差小一些。。）</p>
<h1 id="机器学习中存在的一些挑战">机器学习中存在的一些挑战</h1>
<p>这节主要涉及一些 <font color=red>
为何处理高维数据时在新样本上泛化特别困难，以及为何在传统机器
学习中实现泛化的机制不适合学习高维空间中复杂的函数。这些空间经常涉及巨大
的计算代价。深度学习旨在克服这些以及其他一些难题。</font></p>
<h2 id="维数灾难">维数灾难</h2>
<p>由于算法或者是一些条件的限制，当数据量上升时，涉及到的问题的规模使得我们没有办法处理。。</p>
<p>比如旅行者问题，规模是 <span class="math inline">\(O(n!)\)</span>
的，当 n 的数据量很大时，没有较好的办法能快速去解决</p>
<p>亦或是如图所示</p>
<figure>
<img
src="https://szfmsmdx.oss-cn-hangzhou.aliyuncs.com/blog_pic/202405061348858.png"
alt="image-20240506134814736" />
<figcaption aria-hidden="true">image-20240506134814736</figcaption>
</figure>
<p>当数据的相关维度增大时（从左向右），我们感兴趣的配置数目会随之指数级增长。</p>
<h2 id="局部不变性和平滑正则化">局部不变性和平滑正则化</h2>
<p>为了更好地泛化，机器学习算法需要由先验信念引导应该学习什么类型的函数。其中最广泛使用的隐式
‘‘先验’’
是<strong>平滑先验</strong>，或<strong>局部不变性先验</strong>。</p>
<p>大多数机器学习算法默认了“局部不变性”（local constancy
prior），即特征空间的变化对应于结果变化是”很小的“；而且变化往往是”平滑“（smoothness
prior）的，即变化是<strong>可微分</strong>的，即，我们学习到的模型 <span
class="math inline">\(f^*(x)\approx f^*(x+\epsilon)\)</span> ，其中
<span class="math inline">\(\epsilon\)</span> 表示一个微小的变动</p>
<ul>
<li>如果基于以上假设，我们使用m个样本，只能学到m个不同的区间了。</li>
</ul>
<p>换言之，如果我们知道对应输入 x 的答案（例如，x
是个有标签的训练样本），那么 该答案对于 x
的邻域应该也适用。如果在有些邻域中我们有几个好答案，那么我们
可以组合它们（通过某种形式的平均或插值法）以产生一个尽可能和大多数输入一致的答案。</p>
<blockquote>
<p>只要在要学习的真实函数的峰值和谷值处有足够多的样本，那么平滑性假设和相关的无参数学习算法的效果都非常好。当要学习的函数足够平滑，并且只在少数几维变化，这样做一般没问题。在高维空间中，即使是非常平滑的函数，也会在不同维度上有不同的变化方式。如果函数在不同的区间中表现不一样，那么就非常难用一组训练样本去刻画函数。如果函数是复杂的（我们想区分多于训练样本数目的大量区间），有希望很好地泛化么？</p>
<p>这些问题，即是否可以有效地表示复杂的函数以及所估计的函数是否可以很好地泛化到新的输入，答案是有。关键观点是，只要我们通过额外假设生成数据的分布来建立区域间的依赖关系，那么
O(k) 个样本足以描述多如 O(2k ) 的大量区间。</p>
</blockquote>
<h2 id="流形学习">流形学习</h2>
<p><strong>流形（manifold）</strong>指连接在一起的区域。数学上，它是指一组点，且每个点都有其邻域。给定一个任意的点，其流形局部看起来像是欧几里得空间。日常生活中，我们将地球视为二维平面，但实际上它是三维空间中的球状流形。</p>
<p><img src="https://szfmsmdx.oss-cn-hangzhou.aliyuncs.com/blog_pic/202405061421749.png" alt="image-20240506142147640" style="zoom:67%;" /></p>
<p>上图表示：训练数据位于二维空间中的一维流形中。在机器学习中，我们允许流形的维数从一个点到另一个点有所变化。这经常发生于流形和自身相交的情况中。例如，数字
“8’’ 形状的流形在大多数位置只有一维，但在中心的相交处有两维。</p>
<p>如果我们希望机器学习算法学习整个 <span class="math inline">\(\mathbb
R^n\)</span>
上有趣变化的函数，那么很多机器学习看上去是无望的。<strong>流形学习（manifold
learning）</strong>算法通过一个假设来克服这个障碍，该假设认为 <span
class="math inline">\(\mathbb R^n\)</span>
中大部分区域都是无效的输入，有意义的输入只分布在包含少量数据点的子集构成的一组流形中，而学习函数的输出中，有意义的变化都沿着流形的方向或仅发生在我们切换到另一流形时。</p>

    </div>

    
    
    

    <div>
      
      <div>
    
        <div style="text-align:center;color: #ccc;font-size:14px;">-------------本文结束<i class="fa fa-paw"></i>感谢您的阅读-------------</div>
    
</div>
      
    </div>

    <footer class="post-footer">
          

<div class="post-copyright">
<ul>
  <li class="post-copyright-author">
      <strong>本文作者： </strong>Szfmsmdx
  </li>
  <li class="post-copyright-link">
      <strong>本文链接：</strong>
      <a href="http://szf.cool/2024/04/28/%E8%8A%B1%E4%B9%A6%E2%80%94%E2%80%94%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/" title="花书——机器学习基础">http://szf.cool/2024/04/28/花书——机器学习基础/</a>
  </li>
  <li class="post-copyright-license">
      <strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/zh-CN" rel="noopener" target="_blank"><i class="fab fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！
  </li>
</ul>
</div>

          <div class="post-tags">
              <a href="/tags/Deep-Learning/" rel="tag"><i class="fa fa-tag"></i> Deep Learning</a>
              <a href="/tags/%E8%8A%B1%E4%B9%A6/" rel="tag"><i class="fa fa-tag"></i> 花书</a>
              <a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" rel="tag"><i class="fa fa-tag"></i> 机器学习</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2024/04/27/%E8%8A%B1%E4%B9%A6%E2%80%94%E2%80%94%E6%95%B0%E5%80%BC%E8%AE%A1%E7%AE%97/" rel="prev" title="花书——数值计算">
                  <i class="fa fa-angle-left"></i> 花书——数值计算
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/2024/04/29/129%E5%9C%BA%E5%8F%8C%E5%91%A8%E8%B5%9B-395%E5%9C%BA%E5%91%A8%E8%B5%9B/" rel="next" title="129场双周赛&&395场周赛">
                  129场双周赛&&395场周赛 <i class="fa fa-angle-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>





    <div class="comments" id="waline"></div>
</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">

  <div class="beian"><a href="https://beian.miit.gov.cn/" rel="noopener" target="_blank">皖ICP备2023007923号-2 </a>
      <img src="https://szfmsmdx.oss-cn-hangzhou.aliyuncs.com/blog_pic/202405231028849.png" alt=""><a href="https://beian.mps.gov.cn/#/query/webSearch?code=34018102340752" rel="noopener" target="_blank">皖公网安备34018102340752号 </a>
  </div>
  <div class="copyright">
    &copy; 
    <span itemprop="copyrightYear">2024</span>
    <span class="with-love">
      <i class="fa fa-heart"></i>
    </span>
    <span class="author" itemprop="copyrightHolder">Szfmsmdx</span>
  </div>
<div class="wordcount">
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-line"></i>
    </span>
    <span title="站点总字数">92k</span>
  </span>
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
    <span title="站点阅读时长">2:48</span>
  </span>
</div>

<!--
-->

    </div>
  </footer>

  
  <div class="back-to-top" role="button" aria-label="返回顶部">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/next-theme-pjax/0.6.0/pjax.min.js" integrity="sha256-vxLn1tSKWD4dqbMRyv940UYw4sXgMtYcK6reefzZrao=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/next-boot.js"></script><script src="/js/pjax.js"></script>

  <script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-generator-searchdb/1.4.1/search.js" integrity="sha256-1kfA5uHPf65M5cphT2dvymhkuyHPQp5A53EGZOnOLmc=" crossorigin="anonymous"></script>
<script src="/js/third-party/search/local-search.js"></script>







  




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"ams","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>


<script class="next-config" data-name="waline" type="application/json">{"lang":"zh-cn","enable":true,"serverURL":"comment.szf.cool","cssUrl":"https://unpkg.com/@waline/client@v2/dist/waline.css","commentCount":true,"pageview":false,"locale":null,"placeholder":"友好交流，善意评论 ^ ^","avatar":"mm","meta":["nick","mail","link"],"pageSize":10,"visitor":false,"comment_count":true,"requiredFields":[],"libUrl":"//unpkg.com/@waline/client@v2/dist/waline.js","dark":"body.darkmode--activated","el":"#waline","comment":true,"path":"/2024/04/28/%E8%8A%B1%E4%B9%A6%E2%80%94%E2%80%94%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/"}</script>
<link rel="stylesheet" href="https://unpkg.com/@waline/client@v2/dist/waline.css">
<script>
document.addEventListener('page:loaded', () => {
  NexT.utils.loadComments(CONFIG.waline.el).then(() =>
    NexT.utils.getScript(CONFIG.waline.libUrl, { condition: window.Waline })
  ).then(() => 
    Waline.init(Object.assign({}, CONFIG.waline,{ el: document.querySelector(CONFIG.waline.el) }))
  );
});
</script>
<script src="https://unpkg.com/darkmode-js@1.5.7/lib/darkmode-js.min.js"></script>

<script>
var options = {
  bottom: '64px',
  right: '64px',
  left: 'unset',
  time: '0.3s',
  mixColor: 'transparent',
  backgroundColor: 'transparent',
  buttonColorDark: '#100f2c',
  buttonColorLight: '#fff',
  saveInCookies: false,
  label: '🌓',
  autoMatchOsTheme: false
}
const darkmode = new Darkmode(options);
window.darkmode = darkmode;
darkmode.showWidget();
if (window.darkmode && !window.darkmode.isActivated()) {
  window.darkmode.toggle();
  var toggleButtons = document.getElementsByClassName("darkmode-toggle");
  if (toggleButtons && toggleButtons.length > 0) {
    for (i = 0; i < toggleButtons.length; i++) {
      toggleButtons[i].classList.add("darkmode-toggle--white");
    }
  }
}
</script>

</body>
</html>
