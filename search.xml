<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>花书——深度模型中的优化</title>
    <url>/2024/05/18/%E8%8A%B1%E4%B9%A6%E2%80%94%E2%80%94%E6%B7%B1%E5%BA%A6%E6%A8%A1%E5%9E%8B%E4%B8%AD%E7%9A%84%E4%BC%98%E5%8C%96/</url>
    <content><![CDATA[<blockquote class="blockquote-center">
<p>研读花书，皆为拙记，如有错误还望各位不惜笔墨，不啬赐教。本节主要包括<strong>优化</strong>部分内容</p>

</blockquote>
<span id="more"></span>
]]></content>
      <categories>
        <category>Deep Learning</category>
      </categories>
      <tags>
        <tag>花书</tag>
        <tag>Deep Learning</tag>
        <tag>优化器</tag>
      </tags>
  </entry>
  <entry>
    <title>花书——深度学习中的正则化</title>
    <url>/2024/05/16/%E8%8A%B1%E4%B9%A6%E2%80%94%E2%80%94%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%AD%E7%9A%84%E6%AD%A3%E5%88%99%E5%8C%96/</url>
    <content><![CDATA[<blockquote class="blockquote-center">
<p>研读花书，皆为拙记，如有错误还望各位不惜笔墨，不啬赐教。本节主要包括<strong>正则化</strong>部分内容</p>

</blockquote>
<span id="more"></span>
<blockquote>
<p>机器学习中的一个核心问题是设计不仅在训练数据上表现好，并且能在新输入上泛化好的算法。</p>
<p>在机器学习中，许多策略显式地被设计来减少测试误差（可能会以增大训练误差为代价）。这些策略被统称为<strong>正则化</strong>。</p>
</blockquote>
<h1 id="参数范数惩罚">参数范数惩罚</h1>
<p>许多正则化方法通过对目标函数 J 添加一个参数范数惩罚
Ω(θ)，限制模型（如神经网络、线性回归或逻辑回归）的学习能力。 <span
class="math display">\[
\tilde J(\theta ;X, y)=J(\theta;X,y)+\alpha \Omega(\theta)
\]</span> 其中 <span class="math inline">\(\alpha \in [0,
+\infty]\)</span> 是权衡范数惩罚项 Ω 和标准目标函数 <span
class="math inline">\(J(X;\theta)\)</span> 相对贡献的超参数将 α 设为 0
表示没有正则化。α 越大，对应正则化惩罚越大。</p>
<blockquote>
<p>在神经网络中，参数包括每一层仿射变换的权重和偏置，我们通常只对<strong>权重</strong>做惩罚而不对偏置做正则惩罚。</p>
<ul>
<li>每个权重会指定两个变量如何相互作用。我们需要在各种条件下观察这两个变量才能良好地拟合权重。</li>
<li>而每个偏置仅控制一个单变量。这意味着，我们不对其进行正则化也不会导致太大的方差。</li>
</ul>
<p>另外，正则化偏置参数可能会导致明显的欠拟合。因此，我们使用向量 w
表示所有应受范数惩罚影响的权重，而向量 θ 表示所有参数 (包括 w
和无需正则化的参数)。</p>
</blockquote>
<p>在神经网络的情况下，有时希望对网络的每一层使用单独的惩罚，并分配不同的
α
系数。寻找合适的多个超参数的代价很大，因此为了减少搜索空间，我们会在所有层使用相同的权重衰减。</p>
<h2 id="l2-参数正则化"><span class="math inline">\(L^2\)</span>
参数正则化</h2>
<p><strong>权重衰减（weight decay）</strong> 的 <span
class="math inline">\(L^2\)</span>
参数范数惩罚，这个正则化策略通过向目标函数添加一个正则项 <span
class="math inline">\(\Omega(\theta)=\frac 12 ||w||^2\)</span>
使得权重更加接近原点。同时 <span class="math inline">\(L^2\)</span>
也被称为岭回归或 Tikhonov 正则</p>
<p>如果假定模型没有偏置参数，那么 <span
class="math inline">\(\theta\)</span> 其实就是 <span
class="math inline">\(w\)</span> ，所以目标函数应该是 <span
class="math display">\[
\tilde J(w;X,y)=\frac \alpha 2 w^T w + J(w; X, y)
\]</span> 那么他的梯度应该是 <span class="math display">\[
\nabla_w \tilde J(w;X,y)=\alpha w + \nabla_w J(w; X, y)
\]</span> 使用单步梯度下降更新权重，即执行以下更新： <span
class="math display">\[
w \gets w - \epsilon (\alpha w + \nabla_w J(w; X ,
y))=(1-\epsilon\alpha)w-\epsilon \nabla_w J(w;X,y)
\]</span> 进一步地，令 <span class="math inline">\(w^*\)</span>
是不带正则化的最小训练误差权重向量，那么 <span
class="math inline">\(\hat J(\theta)\)</span> 在 <span
class="math inline">\(w^*\)</span>
处的一阶梯度应该是为0的，我们看他的二阶近似，有 <span
class="math display">\[
\hat J(\theta)=J(w^*) + \frac 12 (w-w^*)^T H (w-w^*)
\]</span> 其中 H 是 J 在 <span class="math inline">\(w^*\)</span>
处的黑塞矩阵，当 <span class="math inline">\(\hat J\)</span>
取最小时，应该有 <span class="math inline">\(\hat J\)</span>
的梯度最小，那么 <span class="math display">\[
\nabla_w \hat J(w)=H(w-w^*)
\]</span> 为 0</p>
<p>所以，此时添加正则项，由于加法法则，所以 <span
class="math display">\[
\alpha \tilde w + H(\tilde w - w^*)=0 \\
(H+\alpha I)\tilde w=H w^* \\
\tilde w = (H + \alpha I)^{-1} H w^*
\]</span> 当 α 趋向于 0 时，正则化的解 <span
class="math inline">\(\tilde w\)</span> 会趋向于 <span
class="math inline">\(w^*\)</span>​ ，下面再研究 <span
class="math inline">\(\alpha\)</span> 增加时的特征，由于 H
是实对称的，于是可以对其进行对角化分解，于是有 <span
class="math display">\[
\begin{aligned}
\tilde w &amp;= (Q \Lambda Q^T + \alpha I)^{-1}Q \Lambda Q^T w^* \\
&amp;=[Q(\Lambda + \alpha I)Q^T]^{-1}Q\Lambda Q^T w^* \\
&amp;=Q(\Lambda + \alpha I)^{-1}\Lambda Q^T w^*
\end{aligned}
\]</span> 我们可以看到权重衰减的效果是沿着由 H 的特征向量所定义的轴缩放
<span class="math inline">\(w^*\)</span>​</p>
<blockquote>
<p>具体来说，我们会根据 <span class="math inline">\(\frac
{\lambda_i}{\lambda_i + \alpha}\)</span> 因子缩放与 H 第 i
个特征向量对其的 <span class="math inline">\(w^*\)</span>​ 的分量</p>
<p>沿着 H 特征值较大的方向 (如 λi ≫ α)正则化的影响较小。而 λi ≪ α
的分量将会收缩到几乎为零。</p>
</blockquote>
<p><img src="https://szfmsmdx.oss-cn-hangzhou.aliyuncs.com/blog_pic/202405171131908.png" alt="image-20240517113113857" style="zoom:67%;" /></p>
<p>实线椭圆表示没有正则化目标的等值线。虚线圆圈表示 <span
class="math inline">\(L^2\)</span> 正则化项的等值线。在 <span
class="math inline">\(\tilde w\)</span>
点，这两个竞争目标达到平衡。目标函数 J 的 Hessian
的第一维特征值很小。当从 <span class="math inline">\(w^*\)</span>​
水平移动时，目标函数不会增加得太多。</p>
<p>简单来说，曲率大的地方说明黑塞阵的特征值是大的，所以在该区域移动会导致较大的变化，而如图中的等高线圆，在水平方向的特征值并不大，所以能够在水平移动较大距离</p>
<h2 id="l1-参数正则化"><span class="math inline">\(L^1\)</span>
参数正则化</h2>
<p>形式地，对模型参数 w 的 <span class="math inline">\(L^1\)</span>
正则化被定义为： <span class="math display">\[
\Omega(\theta)=||w||_1=\sum_i |w_i|
\]</span> 即各个参数的绝对值之和</p>
<p>其正则化的目标函数 <span class="math inline">\(\tilde
J(w;X,y)\)</span> 如下所示： <span class="math display">\[
\tilde J(w; X, y) = \alpha ||w||_1 + J(w; X , y)
\]</span> 对应的梯度为 <span class="math display">\[
\nabla_w \tilde J(w;X,y)=\alpha \mathrm{sign}(w) + \nabla_w J(w;X,y)
\]</span> 可以看到正则化对梯度的影响不再是线性地缩放每个
wi；而是添加了一项与sign(wi)
同号的常数。使用这种形式的梯度之后，我们不一定能得到 <span
class="math inline">\(J(X,y;w)\)</span> 二次近似的直接算术解</p>
<p>同理于 <span class="math inline">\(L^2\)</span> 正则，有 <span
class="math display">\[
\nabla_w \hat J(w) = H(w - w*)
\]</span> 同样，H 是 J 在 <span class="math inline">\(w^*\)</span> 处的
Hessian 矩阵（关于 <span class="math inline">\(w\)</span> )，同样的</p>
<p>由于 <span class="math inline">\(L^1\)</span>
惩罚项在完全一般化的黑塞阵情况下，无法得到清晰的代数表达式，所以假设其是对角的<strong>（对数据进行处理，去除相关性）</strong>，即
<span class="math inline">\(H = \mathrm{diag}([H_{1,1}, \cdots,
H_{n,n}])\)</span> ，其中 <span class="math inline">\(H_{i,i} &gt;
0\)</span> ，那么可以将 <span class="math inline">\(L^1\)</span>
正则化目标函数的二次近似分解成关于参数的求和： <span
class="math display">\[
\hat J(w; X, y) = J(w^*;X, y) + \sum_i[\frac12 H_{i,i}(w_i - w_i^*)^2 +
\alpha|w_i|]
\]</span> 所以有解析解能够使得最小化这个近似代价函数 <span
class="math display">\[
w_i = \mathrm {sign}(w_i^*)\max \{|w_i^*| - \frac\alpha {H_{i,i}},0\}
\]</span> 对每个 i, 考虑 <span class="math inline">\(w_i^*&gt;0\)</span>
的情形，会有两种可能结果：</p>
<ol type="1">
<li><span class="math inline">\(w_i^* \le \frac \alpha
{H_{i,i}}\)</span> ：最优解是 <span class="math inline">\(w_i =
0\)</span></li>
<li><span class="math inline">\(w_i^* &gt; \frac \alpha
{H_{i,i}}\)</span>：这种情况下，正则化不会将 <span
class="math inline">\(w_i\)</span> 的最优值推至0，而仅仅在那个方向上移动
<span class="math inline">\(\frac \alpha {H_{i,i}}\)</span> 的距离</li>
</ol>
<p>由 L 1正则化导出的稀疏性质已经被广泛地用于 <strong>特征选择（feature
selection）</strong>机制。特征选择从可用的特征子集选择出有意义的特征，化简机器学习问题。著名的LASSO
模型将 L 1 惩罚和线性模型结合，并使用最小二乘代价函数。L 1
惩罚使部分子集的权重为零，表明相应的特征可以被安全地忽略。</p>
<h1 id="作为约束的范数惩罚">作为约束的范数惩罚</h1>
<p>其实就是引入了最优化方法里的一些技巧（KKT条件对之类）</p>
<p>首先考虑对惩罚项施加约束条件 <span class="math display">\[
\Omega(\theta) &lt; k
\]</span> 于是构建广义拉格朗日函数 <span class="math display">\[
\mathcal L(\theta,\alpha;X,y)=J(\theta;X,y) + \alpha(\Omega(\theta) - k)
\]</span> 则最优的参数为： <span class="math display">\[
\theta^* = \arg \min_\theta \max_{\alpha, \alpha\ge 0}\mathcal
L(\theta,\alpha)
\]</span> 如果 Ω 是 L 2 范数，那么权重就是被约束在一个 L 2 球中。</p>
<p>如果 Ω 是 L 1 范数，那么权重就是被约束在一个 L 1
范数限制的区域中。</p>
<p>有时候，我们希望使用显式的限制，而不是惩罚。</p>
<p>显式约束的好处：</p>
<ol type="1">
<li>可以先计算J的梯度，再去考虑满足约束的最近的θ。</li>
<li>惩罚项可能导致目标函数非凸，非凸就会导致陷入局部最优的可能性。</li>
<li>增加一定的稳定性。</li>
</ol>
<h1 id="正则化和欠约束问题">正则化和欠约束问题</h1>
<p>机器学习中的大多数模型，都依赖于对矩阵 <span
class="math inline">\(X^T X\)</span>
求逆，只要其奇异，这些方法、模型就会失效</p>
<p>但是正则化的许多形式对应求逆 <span class="math inline">\(X^T X +
\alpha I\)</span> ，这个正则化矩阵是可以保证可逆的</p>
<h1 id="数据集增强">数据集增强</h1>
<p>让机器学习模型泛化得更好的最好办法是使用更多的数据进行训练。解决这个问题的一种方法是创建假数据并添加到训练集中。</p>
<ul>
<li><p>对于图像，可以进行图像的<strong>平移/旋转/缩放/翻转/改变光照</strong>等变化。但是不能改变标记。（6不能旋转成9，b不能镜像到d）</p></li>
<li><p>语音识别任务也可以进行数据集增强。</p></li>
<li><p>数据增强的另一种方式是在<strong>输入层注入噪声</strong>(比如去噪自编码器)。也可以在隐藏单元注入噪声（抽象层的数据集增强）</p></li>
</ul>
<h1 id="噪声鲁棒性">噪声鲁棒性</h1>
<ul>
<li>在输入层（隐层）注入噪声 = 数据增强</li>
<li>在隐层注入噪声 → dropout</li>
<li>在权重（参数）中注入噪声，应用在循环神经网络，表示权重的不确定性 =
关于权重的贝叶斯推断的随机实现</li>
<li>在输出中注入噪声，对标签中的错误（噪声）显式建模，<strong>标签平滑</strong>。</li>
</ul>
<h1 id="半监督学习">半监督学习</h1>
<p>在半监督学习的框架下，P(x) 产生的未标记样本和 P(x, y)
中的标记样本都用于估计 P(y | x) 或者根据 x 预测 y</p>
<p>在深度学习的背景下，半监督学习通常指的是学习一个表示 h =
f(x)。学习表示的目的是使相同类中的样本有类似的表示。</p>
<p>我们可以构建这样一个模型，其中生成模型 P(x) 或 P(x, y) 与判别模型 P(y
| x) 共享参数，而不用分离无监督和监督部分。</p>
<h1 id="多任务学习">多任务学习</h1>
<p>将多个<strong>相关</strong>的任务放到一起学习，合并多个任务中的样例。可以视为对参数施加了软约束（因为共享给其他任务），同时合并多任务的样例从而增加了数据量（样本数量与样本间的相关信息）。</p>
<p>该模型通常可以分为两类相关的参数：</p>
<ol type="1">
<li>具体任务的参数（只能从各自任务的样本中实现良好的泛化）。如图上层</li>
<li>所有任务共享的通用参数（从所有任务的汇集数据中获益）。如图下层</li>
</ol>
<p><img src="https://szfmsmdx.oss-cn-hangzhou.aliyuncs.com/blog_pic/202405171620309.png" alt="image-20240517162029233" style="zoom:67%;" /></p>
<h1 id="提前终止">提前终止</h1>
<p>由于过拟合会在某一参数节点后，损失不断上升，所以我们为了避免这种情况，当每次<strong>验证集误差有所改善时</strong>我们存储模型参数的副本，当训练算法终止时，返回这些参数而不是最新的参数。</p>
<p>当验证集上的误差在事先指定的循环次数内没有进一步改善时，算法就会终止。</p>
<figure>
<img
src="https://szfmsmdx.oss-cn-hangzhou.aliyuncs.com/blog_pic/202405171624943.png"
alt="image-20240517162435854" />
<figcaption aria-hidden="true">image-20240517162435854</figcaption>
</figure>
<p>提前终止是一种非常不显眼的正则化形式，它几乎不需要改变基本训练过程、目标函数或一组允许的参数值。这意味着，无需破坏学习动态就能很容易地使用提前终止。</p>
<ul>
<li>提前终止可单独使用或与其他的正则化策略结合使用。</li>
<li>提前终止需要验证集。为了更好地利用这一额外的数据，我们可以在完成提前终止的首次训练之后，进行额外的训练。有两个基本的策略都可以用于第二轮训练过程。
<ul>
<li>再次初始化模型，然后使用所有数据再次训练：我们使用第一轮提前终止训练确定的最佳步数。</li>
<li>保持从第一轮训练获得的参数，然后使用全部的数据<strong>继续</strong>训练。这里因为验证集被拿来训练，所以观察平均损失函数，继续训练到低于提前终止时的目标值</li>
</ul></li>
</ul>
<p><img src="https://szfmsmdx.oss-cn-hangzhou.aliyuncs.com/blog_pic/202405171628055.png" alt="image-20240517162817980" style="zoom:80%;" /></p>
<p><img src="https://szfmsmdx.oss-cn-hangzhou.aliyuncs.com/blog_pic/202405171628143.png" alt="image-20240517162829065" style="zoom:80%;" /></p>
<h1 id="参数绑定和参数共享">参数绑定和参数共享</h1>
<p>我们经常想要表达的一种常见依赖是某些参数应当彼此接近。考虑以下情形：我们有两个模型执行相同的分类任务（具有相同类别），但输入分布稍有不同。形式地，我们有参数为
<span class="math inline">\(w^{(A)}\)</span> 的 A 模型和参数为 <span
class="math inline">\(w^{(B)}\)</span> 的 B
模型，将这两种模型输入映射到两个不同但相关的输出：<span
class="math inline">\(\hat y^{(A)}=f(w^{(A)},x)\)</span> 和 <span
class="math inline">\(\hat y^{(B)}=f(w^{(B)},x)\)</span></p>
<p>这些任务会足够相似（或许具有相似的输入和输出分布），因此我们认为模型参数应彼此靠近。我们可以通过正则化利用此信息。使用如下形式的参数范数惩罚
<span
class="math inline">\(\Omega(w^{(A)},w^{(B)})=||w^{(A)}-w^{(A)}||_2^2\)</span>​
(也可以使用其他范数或者惩罚项)</p>
<p>参数范数惩罚是正则化参数使其彼此接近的一种方式，而更流行的方法是使用约束：强迫某些参数相等。</p>
<p>由于我们将各种模型或模型组件解释为共享唯一的一组参数，这种正则化方法通常被称为
<strong>参数共享（parameter
sharing）</strong>。好处是可以减少内存，减少训练开支。主要用于卷积神经网络。</p>
<h1 id="稀疏表示">稀疏表示</h1>
<p>之前说的加上惩罚项是直接惩罚模型的参数，另一种办法是惩罚神经网络中的激活函数单元，稀疏化激活单元。</p>
<p><img src="https://szfmsmdx.oss-cn-hangzhou.aliyuncs.com/blog_pic/202405181104497.png" alt="image-20240518110409352" style="zoom: 67%;" /></p>
<p>第一个表达式是参数稀疏的线性回归模型的例子。第二个表达式是数据 x
具有稀疏表示 h 的线性回归。也就是说，h 是 x
的一个函数，在某种意义上表示存在于 x
中的信息，但只是用一个稀疏向量表示。</p>
<p>表示的正则化可以使用参数正则化中同种类型的机制实现。 <span
class="math display">\[
\tilde J(\theta;X,y)=J(\theta;X,y) + \alpha\Omega(h)
\]</span>
此外，还有一些其他方法通过激活值的硬性约束来获得表示稀疏。例如，<strong>正交匹配追踪（orthogonal
matching pursuit）</strong> 通过解决以下约束化问题将输入值 x 编码成表示
h <span class="math display">\[
\arg \min_{h,||h_0|| &lt; k} ||x - Wh||^2
\]</span> 其中 <span class="math inline">\(||h_0||\)</span> 表示 h
中非零项的个数。当 W 被约束为正交时，我们可以高效地解决这个问题。</p>
<h1 id="bagging-和其他集成方法">Bagging 和其他集成方法</h1>
<p><strong>Bagging（bootstrap
aggregating）</strong>是通过结合几个模型降低泛化误差的技术。主要想法是分别训练几个不同的模型，然后让所有模型表决测试样例的输出。这是机器学习中常规策略的一个例子，被称为
<strong>模型平均（model
averaging）</strong>。采用这种策略的技术被称为集成方法。</p>
<p><strong>模型平均（model
averaging）</strong>奏效的原因是不同的模型通常不会在测试集上产生完全相同的误差。</p>
<p>假设我们有 k 个回归模型。假设每个模型在每个例子上的误差是
ϵi，这个误差服从零均值方差为 <span class="math inline">\(\mathbb
E[\epsilon_i^2]=v\)</span> 且协方差为 <span
class="math inline">\(\mathbb E[\epsilon_i \epsilon_j]=c\)</span>
的多维正态分布，通过所有集成模型的平均预测所得误差是 <span
class="math inline">\(\frac 1k \sum_i \epsilon_i\)</span>
，所以集成预测器平方误差的期望是 <span class="math display">\[
\mathbb E[(\frac 1k \sum_i \epsilon_i)^2] = \frac 1{k^2}\mathbb
E[\sum_i(\epsilon_i^2 + \sum_{j\neq i}\epsilon_i \epsilon_j)]=\frac 1k v
+ \frac {k-1}k c
\]</span> 在误差完全相关即 c = v 的情况下，均方误差减少到
v，所以模型平均没有任何帮助。在错误完全不相关即 c = 0
的情况下，该集成平方误差的期望仅为 <span class="math inline">\(\frac 1k
v\)</span>。这意味着集成平方误差的期望会随着集成规模增大而线性减小。换言之，平均上，集成至少与它的任何成员表现得一样好，并且如果成员的误差是独立的，集成将显著地比其成员表现得更好。</p>
<p>ensemble方法可以是将完全不同的模型结合起来，而Bagging方法则是同一种模型和目标函数，但是产生k种不同的训练集，每个训练集与原训练集所含数据量相同，但会以某一概率去掉某些样本并以其他的重复样本代替，第i个模型就在第i个训练集上进行训练。例如下图中所示，我们想鉴别某个数字是否是8，原数据包含9，6和8，第一个重新取样的数据集中仅包含6和8，9倍替换掉，则模型1会学习到需要数字有上半圈才能为8，而第二个重新取样的数据集中去掉6，仅包含9和8，则模型2会学习到数字需有下半圈才能为8，将他们集合起来就可以比较准确的推测需要同时有上下半圈才鉴定数字为8。</p>
<p><img src="https://szfmsmdx.oss-cn-hangzhou.aliyuncs.com/blog_pic/202405181200927.png" alt="image-20240518120040837" style="zoom:67%;" /></p>
<h1 id="dropout">Dropout</h1>
<p>可以看作是一个深度神经网络<strong>内部</strong>的<strong>Bagging集成近似</strong>。</p>
<p>Dropout可以看作在深度神经网络内部利用<strong>子网</strong>模拟集成。子网络是通过<strong>删除</strong>非输出单元实现的，删除可以通过乘0
<strong>系数</strong>简单实现。然后再将自网络集成起来，子网络都是来源于父网络的，权重共享。</p>
<p>Dropout可以理解做是将ensemble应用到大型神经网络的一种更为实际有效的方法。由于ensemble需要训练多个模型，对于大型神经网络，其训练和评估所需时间和存储资源较大，这种方法常常不太实际，Dropout就提供了一个更便宜的解决方案：即通过随机去掉一些节点的方法训练多个子网络，并最终将这些子网络ensemble起来，如下图所示：</p>
<p><img src="https://szfmsmdx.oss-cn-hangzhou.aliyuncs.com/blog_pic/202405181308909.png" alt="image-20240518130859819" style="zoom:67%;" /></p>
<blockquote>
<p>Dropout训练由所有子网络组成的集成，<strong>其中子网络通过从基本网络中删除非输出单元构建</strong>。我们从具有两个可见单元和两个隐藏单元的基本网络开始。这四个单元有十六个可能的子集。
右图展示了从原始网络中丢弃不同的单元子集而形成的所有十六个子网络。在这个小例子中，所得到的大部分网络没有输入单元或没有从输入连接到输出的路径。当层较宽时，丢弃所有从输入
到输出的可能路径的概率变小，所以这个问题不太可能在出现层较宽的网络中。</p>
</blockquote>
<p>其具体方法是当我们利用minibatch的算法如随机梯度下降算法来学习时，我们可以随机的选取一个binary
mask(0表示节点输出为零，1表示正常输出该节点）决定哪些输入和隐藏层节点保留，每次的mask的选择是独立的。而mask为1的概率是我们可以调控的超参数。</p>
<p><font color=#00ffff>和bagging方法相比，bagging中每个模型是完全独立的，而dropout中，模型间由于继承了父网络中的参数的子集会共享一些参数，这使得在有限的存储空间中我们可以表示多个模型。</font></p>
<p>以上是训练过程，而在做inference预测时，我们需要取所有模型的预测的均值，但是这往往计算量过多，Hinton提出inference时我们实际可以只用一个模型但其中每个节点的权重需要乘以包含这个节点的概率，这种方法称作
<strong>权重比例推断规则（weight scaling inference
rule）</strong>。实际中，我们常常把weight
scaling过程放在训练过程中，即训练中每个节点输出就乘以包含该节点的概率的倒数，则inference时只需要正常的通过一遍前馈过程即可，不需要在进行weight
scaling。</p>
<h1 id="对抗训练">对抗训练</h1>
<p>Adversarial
Training对抗训练很有意思，它让人们深入思考机器学习究竟学到了什么有效信息。这方面的工作主要是由谷歌的Szegedy和本书作者Ian
Goodfellow进行的。他们可以制造一些对抗样本迷惑神经网络，如下图中所示，他们对于熊猫图片加了一些人眼不可见的干扰，形成新样本，而新的人眼仍可鉴定为熊猫的图片却会被机器以较大置信率鉴定为长臂猿。</p>
<p><img src="https://szfmsmdx.oss-cn-hangzhou.aliyuncs.com/blog_pic/202405181320130.png" alt="image-20240518132006038" style="zoom:67%;" /></p>
<blockquote>
<p>在 ImageNet 上应用 GoogLeNet的对抗样本生成的演示。</p>
<p>通过添加一个不可察觉的小向量（其中元素等于代价函数相对于输入的梯度元素的符号），我们可以改变GoogLeNet
对此图像的分类结果。</p>
</blockquote>
]]></content>
      <categories>
        <category>Deep Learning</category>
      </categories>
      <tags>
        <tag>花书</tag>
        <tag>Deep Learning</tag>
        <tag>机器学习</tag>
        <tag>正则化</tag>
      </tags>
  </entry>
  <entry>
    <title>花书——深度前馈网络</title>
    <url>/2024/05/10/%E8%8A%B1%E4%B9%A6%E2%80%94%E2%80%94%E6%B7%B1%E5%BA%A6%E5%89%8D%E9%A6%88%E7%BD%91%E7%BB%9C/</url>
    <content><![CDATA[<blockquote class="blockquote-center">
<p>研读花书，皆为拙记，如有错误还望各位不惜笔墨，不啬赐教。本节主要包括<strong>深度前馈网络</strong>部分内容</p>

</blockquote>
<span id="more"></span>
<p><strong>深度前馈网络（deep feedforward network）</strong>，也叫作
<strong>前馈神经网络（feedforwardneural network）</strong>或者
<strong>多层感知机（multilayer perceptron,
MLP）</strong>，是典型的深度学习模型。</p>
<blockquote>
<p>目标是近似某个函数 <span class="math inline">\(f^*\)</span></p>
<p>例如对于分类器， <span class="math inline">\(y=f^*(x)\)</span> 将输入
x 映射到一个类别 y</p>
<p>前馈网络定义了一个映射 <span
class="math inline">\(y=f(x;\theta)\)</span> ，并且学习 <span
class="math inline">\(\theta\)</span>
的值，使他能够达到最佳的函数的近似</p>
</blockquote>
<p>这种模型被称为 <strong>前向（feedforward）</strong>的，是因为信息流过
x 的函数，流经用于定义 f 的中间计算过程，最终到达输出
y。在模型的输出和模型本身之间没有
<strong>反馈（feedback）</strong>连接。当前馈神经网络被扩展成包含反馈连接时，它们被称为
<strong>循环神经网络（recurrent neural network）</strong></p>
<p>前馈神经网络被称作
<strong>网络（network）</strong>是因为它们通常用许多不同函数复合在一起来表示。该模型与一个<strong>有向无环图</strong>相关联，而图描述了函数是如何复合在一起的。比如有三个函数
<span class="math inline">\(f^{(1)}、f^{(2)}、f^{(3)}\)</span>
，连接在一个链上，那么他们分别称为网络的 first layer、second layer
...</p>
<p>链的全长称为模型的 深度（depth）。正是因为这个术语才出现了
‘‘深度学习’’ 这个名字。前馈网络的最后一层被称为 <strong>输出层（output
layer）</strong>。</p>
<p>在训练过程中，我们让 <span class="math inline">\(f(x)\)</span> 去匹配
<span class="math inline">\(f^*(x)\)</span> 的值，其中训练样本 x
伴随着一个标签 <span class="math inline">\(y\approx f^*(x)\)</span></p>
<blockquote>
<p>训练样本直接指明了输出层在每一点 x 上必须做什么；它必须产生一个接近 y
的值。但是训练数据并没有直接指明<font color=red>其他层</font>应该怎么做。因为训练数据并没有给出这些层中的每一层所需的输出，所以这些层被称为
<strong>隐藏层（hidden layer）</strong>。</p>
</blockquote>
<p>网络中的每个隐藏层通常都是向量值的。这些隐藏层的维数决定了模型的
<strong>宽度（width）</strong>。向量的每个元素都可以被视为起到类似一个神经元的作用。除了将层想象成向量到向量的单个函数，我们也可以把层想象成由许多并行操作的
<strong>单元（unit）</strong>组成，每个单元表示一个向量到标量的函数。</p>
<h1 id="学习异或xor">学习异或（XOR）</h1>
<p>对于二值函数异或来说，其定义域限制在 <span
class="math inline">\(\mathbb X=\{[0,0]^T,[0,1]^T,[1,0]^T,[1,1]^T
\}\)</span> ，将该问题当成是回归问题，损失函数选择
MSE（虽然他不是一个合适的损失函数），那么整个训练集上表现的 MSE
损失函数为 <span class="math display">\[
J(\theta)=\frac 14 \sum_{x\in \mathbb X} (f^*(x)-f(x;\theta))^2
\]</span> 假设模型选择的是一个线性模型，那么，将其定义为 <span
class="math display">\[
f(x,w,b)=x^Tw + b
\]</span> 这是可以通过正规方程去使得 <span
class="math inline">\(J(\theta)\)</span>
最小的，通过求解正规方程，可以发现 <span
class="math inline">\(w=0,b=\frac 12\)</span> 。</p>
<p><img src="https://szfmsmdx.oss-cn-hangzhou.aliyuncs.com/blog_pic/202405131401989.png" alt="image-20240513140138915" style="zoom:67%;" /></p>
<blockquote>
<p>出现这种情况的原因是，[0,1] 和 [1,0]
两个点的输出都为1（左图的左上和右下），当 <span
class="math inline">\(x_1=0\)</span> 时，模型的输出必须随着 <span
class="math inline">\(x_2\)</span> 的增大而增大，当 <span
class="math inline">\(x_1=0\)</span> 时，模型的输出必须随着 <span
class="math inline">\(x_2\)</span>
的增大而减小，这是线性模型做不到的。不过可以做映射 <span
class="math inline">\(h=f^{(1)}(x;W,c)\)</span>
，让第一层的输出变为输出层的输入，就能比较好的解决这个问题。</p>
<p>而通过线性变换将 [0,1] 和 [1,0] 映射到同一个点 <span
class="math inline">\(h=[1,0]\)</span> 时（右图），就可以描述为 <span
class="math inline">\(h_1\)</span> 增大和 <span
class="math inline">\(h_2\)</span> 减小使得输出增大。</p>
</blockquote>
<p>不过 <span class="math inline">\(f^{(1)}\)</span>
具体应该是那种函数，显然简单的线性不能满足需要了，因为线性的叠加本质上还是线性的，所以需要通过非线性函数来描述特征。在线代神经网络中，默认的推荐是使用由激活函数
<span class="math inline">\(g(z)=\max \{0, z\}\)</span>
定义的<strong>整流线型单元（rectified linear unit）</strong>，或是称为
ReLU</p>
<p>将ReLU应用到上面的异或学习中，我们的整个网络就是 <span
class="math inline">\(f(x;W,c,w,b)=w^T\max \{0,W^Tx+c \}+b\)</span></p>
<h1 id="基于梯度的学习">基于梯度的学习</h1>
<p>由于引入了许多非线性的函数，因此可能最终的代价函数都变得非凸，所以神经网络的训练通常使用迭代的、基于梯度的优化，将代价函数训练到一个很小的值；而不是利用正规方程求解线性的问题。</p>
<p>用于非凸损失函数的随机梯度下降没有这种收敛性保证，并且对参数的初始值很敏感。<strong>对于前馈神经网络，将所有的权重值初始化为小随机数是很重要的。</strong></p>
<h2 id="代价函数">代价函数</h2>
<p>在大多数情况下，我们的参数模型定义了一个分布 <span
class="math inline">\(p(y|x;\theta)\)</span>
并且我们简单地使用最大似然原理。这意味着我们使用训练数据和模型预测间的交叉熵作为代价函数。</p>
<p>有时，我们使用一个更简单的方法，不是预测 y
的完整概率分布，而是仅仅预测在给定 x 的条件下 y
的某种统计量。某些专门的损失函数允许我们来训练这些估计量的预测器。</p>
<h3 id="使用最大似然学习条件分布">使用最大似然学习条件分布</h3>
<p>大多数现代的神经网络使用最大似然来训练。这意味着代价函数就是负的对数似然，它与训练数据和模型分布间的交叉熵等价。这个代价函数表示为
<span class="math display">\[
J(\theta)=-\mathbb E_{x,y\sim \hat{p_{data}}}\log p_{model}(y|x)
\]</span>
上述方程的展开形式通常会有一些项不依赖于模型的参数，我们可以舍去。例如，<span
class="math inline">\(p_{model}(y|x)=\mathcal
N(y;f(x;\theta),I)\)</span> ，那么就能重新得到均方误差代价 <span
class="math inline">\(J(\theta)=\frac 12 \mathbb E_{x,\sim
\hat{p_{data}}}||y-f(x;\theta)||^2+const\)</span> ，至少系数 1/2
和常数项不依赖于 θ。</p>
<blockquote>
<p>使用最大似然来导出代价函数的方法的一个优势是，它减轻了为每个模型设计代价函数的负担。明确一个模型
p(y | x) 则自动地确定了一个代价函数 log p(y | x)。</p>
</blockquote>
<h3 id="学习条件统计量">学习条件统计量</h3>
<blockquote>
<p>有时我们并不是想学习一个完整的概率分布 p(y | x;
θ)，而仅仅是想学习在给定x 时 y 的某个条件统计量。</p>
</blockquote>
<p>例如，我们可能有一个预测器 f(x; θ)，我们想用它来预测 y
的均值。如果我们使用一个足够强大的神经网络，我们可以认为这个神经网络能够表示一大类函数中的任何一个函数
f，这个类仅仅被一些特征所限制，例如连续性和有界，而不是具有特殊的参数形式。</p>
<p>从这个角度来看，我们可以把代价函数看作是一个
<strong>泛函（functional）</strong>而不仅仅是一个函数。泛函是函数到实数的映射。我们因此可以将学习看作是选择一个函数而不仅仅是选择一组参数。</p>
<p>我们可以设计代价泛函在我们想要的某些特殊函数处取得最小值。例如，我们可以设计一个代价泛函，使它的最小值处于一个特殊的函数上，这个函数将
x 映射到给定 x 时 y 的期望值。</p>
<p>我们使用变分法导出的第一个结果是解优化问题</p>
<p><img src="https://szfmsmdx.oss-cn-hangzhou.aliyuncs.com/blog_pic/202405141144738.png" alt="image-20240514114403659" style="zoom:67%;" /></p>
<p>得到</p>
<p><img src="https://szfmsmdx.oss-cn-hangzhou.aliyuncs.com/blog_pic/202405141144042.png" alt="image-20240514114428005" style="zoom:67%;" /></p>
<p>要求这个函数处在我们要优化的类里。换句话说，如果我们能够用无穷多的、来源于真实的数据生成分布的样本进行训练，最小化均方误差代价函数将得到一个函数，它可以用来对每个
x 的值预测出 y 的均值。</p>
<p>不同的代价函数给出不同的统计量。第二个使用变分法得到的结果是</p>
<p><img src="https://szfmsmdx.oss-cn-hangzhou.aliyuncs.com/blog_pic/202405141145424.png" alt="image-20240514114539384" style="zoom:67%;" /></p>
<p>将得到一个函数可以对每个 x 预测 y
取值的中位数，只要这个函数在我们要优化的函数族里。这个代价函数通常被称为
<strong>平均绝对误差（mean absolute error）</strong>。</p>
<p><font color=red>
可惜的是，均方误差和平均绝对误差在使用基于梯度的优化方法时往往成效不佳。一些饱和的输出单元当结合这些代价函数时会产生非常小的梯度。</font>
这就是为什么交叉熵代价函数比均方误差或者平均绝对误差更受欢迎的原因之一了，即使是在没必要估计整个
p(y | x) 分布时。</p>
<h2 id="输出单元">输出单元</h2>
<h3 id="用于高斯输出分布的线性单元">用于高斯输出分布的线性单元</h3>
<p>一种简单的输出单元是基于仿射变换的输出单元，仿射变换不具有非线性。这些单元往往被直接称为线性单元。给定特征
h，线性输出单元层产生一个向量 <span class="math inline">\(\hat y=W^T
h+b\)</span></p>
<p>线性输出层经常被用来产生条件高斯分布的均值： <span
class="math display">\[
p(y|x)=\mathcal N(y;\hat y, I)
\]</span> 最大化其对数似然此时等价于最小化均方误差。</p>
<h3
id="用于bernoulli伯努利输出分布的sigmoid单元">用于Bernoulli(伯努利)输出分布的sigmoid单元</h3>
<p>许多任务需要预测二值型变量 y
的值。具有两个类的分类问题可以归结为这种形式。</p>
<p>此时最大似然的方法是定义 y 在 x 条件下的 Bernoulli 分布。</p>
<p>Bernoulli 分布仅需单个参数来定义。神经网络只需要预测 P(y = 1 | x)
即可。为了使这个数是有效的概率，它必须处在区间 [0, 1] 中。</p>
<p>sigmoid输出单元定义为 <span class="math inline">\(\hat y=\sigma(w^T h
+ b)\)</span></p>
<p>我们可以认为 sigmoid
输出单元具有两个部分。首先，它使用一个线性层来计算 <span
class="math inline">\(z=w^T h + b\)</span>​ 。接着，它使用 sigmoid
激活函数将 z 转化成概率。</p>
<p>如果我们假定非归一化的对数概率对 y 和 z
是线性的，可以对它取指数来得到非归一化的概率。我们然后对它归一化，可以发现这服从
Bernoulli 分布，该分布受 z 的 sigmoid 变换控制：</p>
<p><img src="https://szfmsmdx.oss-cn-hangzhou.aliyuncs.com/blog_pic/202405141214607.png" alt="image-20240514121415561" style="zoom:67%;" /></p>
<blockquote>
<p>上式之所以写成 2y-1，是因为带入 <span
class="math inline">\(y&#39;=0,1\)</span>
时发现，2y-1能够得到相应的结果</p>
</blockquote>
<p>sigmoid 激活函数在 z 取非常小的负值时会饱和到 0，当 z
取非常大的正值时会饱和到
1。这种情况一旦发生，梯度会变得非常小以至于不能用来学习，无论此时模型给出的是正确还是错误的答案。因此，最大似然几乎总是训练
sigmoid 输出单元的优选方法。</p>
<h3 id="用于-multinoulli-输出分布的-softmax-单元">用于 Multinoulli
输出分布的 softmax 单元</h3>
<p>任何时候当我们想要表示一个具有 n
个可能取值的离散型随机变量的分布时，我们都可以使用 softmax
函数。它可以看作是 sigmoid 函数的扩展，其中 sigmoid
函数用来表示二值型变量的分布。</p>
<p>首先，线性层预测了未归一化的对数概率： <span class="math display">\[
z=W^T h + b
\]</span> 其中 <span class="math inline">\(z_i = \log \hat
P(y=i|x)\)</span> ，softmax 函数然后可以对 z 指数化和归一化来获得需要的
<span class="math inline">\(\hat y\)</span> 。最终，softmax 函数的形式为
<span class="math display">\[
\mathrm{softmax}(z)_i=\frac{\exp(z_i)}{\sum_j \exp(z_j)}
\]</span> 和 logistic sigmoid一样，当使用最大化对数似然训练 softmax
来输出目标值 y 时，使用指数函数工作地非常好。这种情况下，我们想要最大化
<span class="math inline">\(\log P(y = i; z) = \log \mathrm
{softmax}(z)_i\)</span>。将 softmax
定义成指数的形式是很自然的因为对数似然中的 log 可以抵消 softmax 中的
exp： <span class="math display">\[
\log \mathrm {softmax} (z)_i=z_i - \log \sum_j \exp(z)_j
\]</span> 通过观察其表达式能够发现，第一项 <span
class="math inline">\(z_i\)</span>
是不会饱和的，因此学习总是可以进行；而第二项 <span
class="math inline">\(\log \sum_j \exp (z_j)\)</span>
有一个直观的理解是，这一项可以被近似为 <span
class="math inline">\(\max_j z_j\)</span>
，所以，<font color=#00ffff> 负对数似然函数总是强烈地惩罚最活跃的不正确预测 </font>
，如果正确答案已经具有了 softmax 的最大输入，那么 <span
class="math inline">\(-z_i\)</span> 和 <span class="math inline">\(\log
\sum_j \exp(z_j)\approx \max_j z_j=z_i\)</span>
项将大致抵消。这个样本对于整体训练代价贡献很小，这个代价主要由其他未被正确分类的样本产生。</p>
<p>对于 softmax
函数，当输入值之间的差异变得极端时，这些输出值可能会饱和，当 softmax
饱和时，基于 softmax
的许多代价函数也饱和，除非他们能够转化饱和的激活函数。</p>
<p>由于对所有输入都加上一个相同常数时，softmax的输出不变： <span
class="math display">\[
\mathrm{softmax}(z)=\mathrm{softmax}(z+c) \\
=&gt;\mathrm{softmax}(z)=\mathrm{sorfmax}(z-\max_i z_i)
\]</span> 变换后的形式允许我们在对 softmax
函数求值时只有很小的数值误差，即使是当 z 包含极正或者极负的数时。观察
softmax 数值稳定的变体，可以看到 softmax 函数由它的变量偏离 maxi zi
的量来驱动。</p>
<p>当其中一个输入是最大 <span class="math inline">\((z_i=\max_i
z_i)\)</span> 并且 <span class="math inline">\(z_i\)</span>
远大于其他的输入时，相应的输出会饱和到 1。当 z_i
不是最大值并且最大值非常大时，相应的输出也会饱和到 0。</p>
<h2 id="隐藏单元">隐藏单元</h2>
<p>除非另有说明，大多数的隐藏单元都可以描述为接受输入向量 x，计算仿射变
换 z = W⊤ x + b，然后使用一个逐元素的非线性函数
g(z)。大多数隐藏单元的区别 仅仅在于激活函数 g(z) 的形式。</p>
<h3 id="relu及其扩展">ReLU及其扩展</h3>
<p>整流线性单元通常作用于仿射变换之上： <span class="math display">\[
h=g(W^T x + b)
\]</span> 当初始化仿射变换的参数时，可以将 b
的所有元素设置成一个小的正值，例如
0.1。这使得整流线性单元很可能初始时就对训练集中的大多数输入呈现激活状态，并且允许导数通过。</p>
<p>ReLU的一个缺陷是它们不能够通过基于梯度的方法学习哪些使它们激活为0的样本，所以，拓展保证了他们能够在各个位置都有梯度。</p>
<p>整流线性单元的三个扩展基于当 <span class="math inline">\(z_i &lt;
0\)</span> 时使用一个非零的斜率 <span
class="math inline">\(\alpha_i\)</span>：<span
class="math inline">\(h_i=g(z,\alpha)_i=\max(0,z_i)+\alpha_i
\min(0,z_i)\)</span></p>
<ul>
<li><strong>绝对值整流（absolute value rectification）</strong>：固定
<span class="math inline">\(\alpha=-1\)</span></li>
<li><strong>渗漏整流线性单元（Leaky ReLU）</strong>：固定 <span
class="math inline">\(\alpha\)</span> 为一个非常小的值</li>
<li><strong>参数化整流线性单元（parametric ReLU）或PReLU</strong>
：<span class="math inline">\(\alpha\)</span> 作为学习的参数</li>
</ul>
<p>maxout 单元可以学习具有多达 k 段的分段线性的凸函数。maxout
单元因此可以视为学习激活函数本身而不仅仅是单元之间的关系。使用足够大的
k，maxout 单元可以以任意的精确度来近似任何凸函数。</p>
<p>每个 maxout 单元现在由 k 个权重向量来参数化，而不仅仅是一个，所以
maxout单元通常比整流线性单元需要更多的正则化。如果训练集很大并且每个单元的块数保持很低的话，它们可以在没有正则化的情况下工作得不错</p>
<h2 id="logistic-sigmoid与双曲正切函数">logistic
sigmoid与双曲正切函数</h2>
<p>在ReLU之前，很多神经网络使用 logistic sigmoid激活函数 <span
class="math display">\[
g(z)=\sigma(z)
\]</span> 或者是双曲正切激活函数 <span class="math display">\[
g(z)=\tanh (z)=\frac{e^z-e^{-z}}{e^z+e^{-z}}
\]</span> 他们本质上是相同的，因为 <span
class="math inline">\(\tanh(z)=2\sigma(2z) - 1\)</span></p>
<h2 id="其他隐藏单元">其他隐藏单元</h2>
<p>也存在许多其他种类的隐藏单元，但它们并不常用。</p>
<ul>
<li><strong>径向基函数（radial basis function，RBF）</strong>：<span
class="math inline">\(h_i=\exp (-\frac
1{\sigma^2}||W_{:,i}-x||^2)\)</span> ，这个函数在 x 接近模板 W
时更加活跃，因为它对大部分 x 都饱和到0</li>
<li><strong>softplus函数</strong>： <span
class="math inline">\(g(a)=\zeta(a)=\log (1+e^a)\)</span>
，这是ReLU的平滑版本。softplus
表明隐藏单元类型的性能可能是非常反直觉的——因为它处处可导或者因为它不完全饱和，人们可能希望它具有优于整流线性单元的点，但根据经验来看，它并没有。</li>
<li><strong>硬双曲正切函数（hard tanh）</strong>： <span
class="math inline">\(g(a)=\max (-1, \min(1, a))\)</span></li>
</ul>
<h1 id="架构设计">架构设计</h1>
<p>大多数神经网络被组织成称为层的单元组，例如，第一层为：</p>
<p><img src="https://szfmsmdx.oss-cn-hangzhou.aliyuncs.com/blog_pic/202405161548317.png" alt="image-20240516154840245" style="zoom:50%;" /></p>
<p>第二层为：</p>
<p><img src="https://szfmsmdx.oss-cn-hangzhou.aliyuncs.com/blog_pic/202405161548922.png" alt="image-20240516154853853" style="zoom:50%;" /></p>
<p>以此类推。</p>
<p>在这些链式架构中，主要考虑的是<strong>网络的深度</strong>和<strong>每层的宽度</strong></p>
<h2 id="万能近似性质和深度">万能近似性质和深度</h2>
<blockquote>
<p><strong>万能近似定理（universal approximation
theorem）</strong>表明，一个前馈神经网络如果具有线性输出层和至少一层具有任何一种
‘‘挤压’’ 性质的激活函数（例如logistic
sigmoid激活函数）的隐藏层，只要给予网络足够数量的隐藏单元，它可以以任意的精度来近似任何从一个有限维空间到另一个有限维空间的
Borel 可测函数。</p>
</blockquote>
<p>神经网络也可以近似从任何有限维离散空间映射到另一个的任意函数。虽然原始定理最初以具有特殊激活函数的单元的形式来描述，这个激活函数当变量取绝对值非常大的正值和负值时都会饱和，万能近似定理也已经被证明对于更广泛类别的激活函数也是适用的，其中就包括现在常用的ReLU</p>
<p>万能近似定理意味着无论我们试图学习什么函数，我们知道一个大的 MLP
一定能够表示这个函数。然而，我们不能保证训练算法能够学得这个函数。</p>
<ul>
<li>用于训练的优化算法可能找不到用于期望函数的参数值</li>
<li>训练算法可能由于过拟合而选择了错误的函数</li>
</ul>
<p>具有单层的前馈网络足以表示任何函数，但是网络层可能大得不可实现，并且可能无法正确地学习和泛化。在很多情况下，使用更深的模型能够减少表示期望函数所需的单元的数量，并且可以减少泛化误差。</p>
<p>存在一些函数族能够在网络的深度大于某个值 d
时被高效地近似，而当深度被限制到小于或等于 d
时需要一个远远大于之前的模型。在很多情况下，浅层模型所需的隐藏单元的数量是
n 的指数级。</p>
<p>Montufar的主要定理指出，具有 d 个输入、深度为 l、每个隐藏层具有 n
个单元的深度整流网络可以描述的线性区域的数量是</p>
<p><img src="https://szfmsmdx.oss-cn-hangzhou.aliyuncs.com/blog_pic/202405161617141.png" alt="image-20240516161710071" style="zoom:67%;" /></p>
<p>意味着，这是深度 l 的指数级。在每个单元具有 k 个过滤器的 maxout
网络中，线性区域的数量是</p>
<p><img src="https://szfmsmdx.oss-cn-hangzhou.aliyuncs.com/blog_pic/202405161617068.png" alt="image-20240516161729003" style="zoom:67%;" /></p>
<p><img src="https://szfmsmdx.oss-cn-hangzhou.aliyuncs.com/blog_pic/202405161619644.png" alt="image-20240516161943564" style="zoom:67%;" /></p>
<blockquote>
<p>参数数量的影响。更深的模型往往表现更好。这不仅仅是因为模型更大。这项实验表明，增加卷积网络层中参数的数量，但是不增加它们的深度，在提升测试集性能方面几乎没有效果，如此图所示。</p>
</blockquote>
<h2 id="其他架构上的考虑">其他架构上的考虑</h2>
<p>一般的，层不需要连接在链中，尽管这是最常见的做法。许多架构构建了一个主链，但随后又添加了额外的架构特性，例如从层
i 到层 i + 2
或者更高层的跳跃连接。这些跳跃连接使得梯度更容易从输出层流向更接近输入的层。</p>
<p>架构设计考虑的另外一个关键点是如何将<strong>层与层之间连接起来</strong>。默认的神经网络层采用矩阵
W 描述的线性变换，每个输入单元连接到每个输出单元。</p>
<p>用于减少连接数量的策略减少了参数的数量以及用于评估网络的计算量，但通常高度依赖于问题。</p>
<h1 id="反向传播和其他的微分算法">反向传播和其他的微分算法</h1>
<p>使用前馈神经网络接收输入 x 并产生输出 <span
class="math inline">\(\hat y\)</span> 时，信息通过网络向前流动</p>
<ul>
<li>从输入 x 提供原始信息，到最终产生输出 <span
class="math inline">\(\hat y\)</span> ，这称之为
<strong>前向传播（forward propagation）</strong></li>
<li>从来自代价函数的信息通过网络向后流动，以便计算梯度称之为
<strong>反向传播（back propagation）</strong></li>
</ul>
<p>反向传播这个术语经常被误解为用于多层神经网络的整个学习算法。实际上，反向传播仅指用于计算梯度的方法，而另一种算法，例如随机梯度下降，使用该梯度来进行学习。</p>
<h2 id="计算图">计算图</h2>
<p>为了更精确地描述反向传播算法，需要引入 <strong>计算图（computational
graph）</strong></p>
<p>使用图中的每一个节点来表示一个变量。变量可以是标量、向量、矩阵、张量、或者甚至是另一类型的变量。</p>
<p>为了形式化我们的图形，我们还需引入
<strong>操作（operation）</strong>这一概念：是指一个或多个变量的简单函数。</p>
<p><img src="https://szfmsmdx.oss-cn-hangzhou.aliyuncs.com/blog_pic/202405161644450.png" alt="image-20240516164445348" style="zoom:80%;" /></p>
<blockquote>
<p>一些计算图示例</p>
</blockquote>
<h2 id="微积分中的链式法则">微积分中的链式法则</h2>
<p>反向传播是一种计算链式法则的算法，使用高效的特定运算顺序。</p>
<p>设 x 是实数，f 和 g 是从实数映射到实数的函数。假设 <span
class="math inline">\(y=g(x)\)</span> 且 <span
class="math inline">\(z=f(g(x))=f(y)\)</span> ，那么链式法则的意思是
<span class="math display">\[
\frac {dz}{dx}=\frac{dz}{dy} \frac{dy}{dx}
\]</span> 我们可以将这种标量情况进行扩展。假设 <span
class="math inline">\(x \in \mathbb R^m\)</span> , <span
class="math inline">\(y \in \mathbb R^n\)</span> ，g是从 <span
class="math inline">\(\mathbb R^m\)</span> 到 <span
class="math inline">\(\mathbb R^n\)</span> 的映射，f 是从 <span
class="math inline">\(\mathbb R^n\)</span> 到 <span
class="math inline">\(\mathbb R\)</span> 的映射，若有 <span
class="math inline">\(y=g(x)\)</span> 且 <span
class="math inline">\(z=f(y)\)</span> ，那么 <span
class="math display">\[
\frac {\partial z}{\partial x_i}=\sum_j \frac{\partial z}{\partial y_j}
\frac {\partial y_j}{\partial x_i}
\]</span> 使用向量记法，可以等价地写成 <span
class="math inline">\(\nabla_x z=(\frac {\partial y}{\partial x})^T
\nabla_y z\)</span> ，其中 <span class="math inline">\(\frac {\partial
y}{\partial x}\)</span> 就是 g 的 n x m的 Jacobian 矩阵</p>
<blockquote>
<p>通常我们将反向传播算法应用于任意维度的张量，而不仅仅用于向量。在我们运行反向传播之前，将每个张量变平为一个向量，计算一个向量值梯度，然后将该梯度重新构造成一个张量。</p>
<p>从这种重新排列的观点上看，反向传播仍然只是将 Jacobian 乘以梯度。</p>
</blockquote>
<p>为了表示值 z 关于张量 X 的梯度，我们记为 <span
class="math inline">\(\nabla_X z\)</span> ，就像 X 是向量一样。X
的索引现在有多个坐标——比如张量的维度为3，那么坐标就需要三个索引来表示。不过可以使用单个变量
i 来表示完整的<strong>索引元组</strong>，从而完全抽象出来。同样的，如果
<span class="math inline">\(Y=g(X)\)</span> 且 <span
class="math inline">\(z=f(Y)\)</span> ，那么 <span
class="math display">\[
\nabla_X z=\sum_j (\nabla_X Y_j) \frac {\partial z}{\partial Y_j}
\]</span></p>
<h2
id="递归地使用链式法则实现反向传播">递归地使用链式法则实现反向传播</h2>
<p><img src="https://szfmsmdx.oss-cn-hangzhou.aliyuncs.com/blog_pic/202405161745844.png" alt="image-20240516174554739" style="zoom:80%;" /></p>
<p>反向传播算法被设计为减少公共子表达式的数量而不考虑存储的开销。具体来说，它大约对图中的每个节点执行一个
Jacobian 乘积。</p>
<p><img src="https://szfmsmdx.oss-cn-hangzhou.aliyuncs.com/blog_pic/202405161747716.png" alt="image-20240516174745628" style="zoom:67%;" /></p>
<blockquote>
<p>图 6.9: 计算梯度时导致重复子表达式的计算图。令 w ∈ R
为图的输入。我们对链中的每一步使用相同的操作函数f : R → R，这样 x =
f(w), y = f(x), z = f(y)。为了计算 ∂z / ∂w，我们应用链式法则得到：</p>
<p><img src="https://szfmsmdx.oss-cn-hangzhou.aliyuncs.com/blog_pic/202405161748140.png" alt="image-20240516174839055" style="zoom:50%;" /></p>
<p>式 (6.52) 建议我们采用的实现方式是，仅计算 f(w)
的值一次并将它存储在变量 x 中。这是反向传播算法所采用的方法。式 (6.53)
提出了一种替代方法，其中子表达式 f(w)
出现了不止一次。在替代方法中，每次只在需要时重新计算
f(w)。当存储这些表达式的值所需的存储较少时，式 (6.52)
的反向传播方法显然是较优的，因为它减少了运行时间。然而，式 (6.53)
也是链式法则的有效实现，并且当存储受限时它是有用的。</p>
</blockquote>
<h2 id="全连接-mlp-中的反向传播">全连接 MLP 中的反向传播</h2>
<p><img src="https://szfmsmdx.oss-cn-hangzhou.aliyuncs.com/blog_pic/202405161805492.png" alt="image-20240516180512371" style="zoom:67%;" /></p>
<h2 id="符号到符号的导数">符号到符号的导数</h2>
<p>代数表达式和计算图都对
<strong>符号（symbol）</strong>或不具有特定值的变量进行操作。这些代数或者基于图的表达式被称为
<strong>符号表示（symbolic representation）</strong>。</p>
<p>当我们实际使用或者训练神经网络时，我们必须给这些符号赋特定的值。我们用一个特定的
<strong>数值（numeric value）</strong>来替代网络的符号输入 x</p>
<p>一些反向传播的方法采用
<strong><em>计算图和一组用于图的输入的数值，然后返回在这些输入值处梯度的一组数值</em></strong>
。我们将这种方法称为 <strong>符号到数值的微分</strong>。这种方法用在诸如
Torch和 Caffe之类的库中。</p>
<p>另一种方法是<strong><em>采用计算图以及添加一些额外的节点到计算图中</em></strong>，这些额外的节点提供了我们所需导数的符号描述。这是
Theano和 TensorFlow 所采用的方法。</p>
<blockquote>
<p>个人理解是动态和静态的区别？</p>
<p>因为符号到微分的方法，需要前面的节点进行建图，这是动态的</p>
<p>而添加额外节点，比如对输入构建一层额外的节点，<span
class="math inline">\(a^{(0)}\)</span> 是输入，<span
class="math inline">\(h^{(1)}=a^{(0)}\)</span>
，多构建一层，然后基于h，建立后续的图关系</p>
</blockquote>
<p><img src="https://szfmsmdx.oss-cn-hangzhou.aliyuncs.com/blog_pic/202405161810183.png" alt="image-20240516181010069" style="zoom:80%;" /></p>
<p><img src="https://szfmsmdx.oss-cn-hangzhou.aliyuncs.com/blog_pic/202405161812666.png" alt="image-20240516181200546" style="zoom:80%;" /></p>
<h2 id="一般化的反向传播">一般化的反向传播</h2>
<p>为了计算某个标量 z 关于图中它的一个祖先 x
的梯度，我们首先观察到它关于 z 的梯度由 dz dz = 1
给出。然后，我们可以计算对图中 z
的每个父节点的梯度，通过现有的梯度乘以产生 z 的操作的
Jacobian。我们继续乘以 Jacobian，以这种方式向后穿过图，直到我们到达
x。对于从 z
出发可以经过两个或更多路径向后行进而到达的任意节点，我们简单地对该节点来自不同路径上的梯度进行求和。</p>
<p>更正式地，图 G
中的每个节点对应着一个变量。为了实现最大的一般化，我们将这个变量描述为一个张量
V。张量通常可以具有任意维度，并且包含标量、向量和矩阵。</p>
<p>我们假设每个变量 V 与下列子程序相关联：</p>
<ul>
<li>get_operation：它返回用于计算 V 的操作，代表了在计算图中流入 V
的边。</li>
<li>get_consumers(V, G)：它返回一组变量，是计算图 G 中 V 的子节点。</li>
<li>get_inputs(V, G)：它返回一组变量，是计算图 G 中 V 的父节点。</li>
</ul>
<p><img src="https://szfmsmdx.oss-cn-hangzhou.aliyuncs.com/blog_pic/202405161825716.png" alt="image-20240516182538605" style="zoom:80%;" /></p>
<p><img src="https://szfmsmdx.oss-cn-hangzhou.aliyuncs.com/blog_pic/202405161825012.png" alt="image-20240516182558894" style="zoom:80%;" /></p>
<h2 id="以mlp为例的反向传播">以MLP为例的反向传播</h2>
<p>网络的计算隐藏特征层为 <span class="math inline">\(H=\max
{0,XW^{(1)}}\)</span> ，输出概率预测为 <span
class="math inline">\(HW^{(2)}\)</span> 给出，假设最后的损失函数为
cross_entropy 操作，并且给上一个正则项，那么总的代价函数为</p>
<p><img src="https://szfmsmdx.oss-cn-hangzhou.aliyuncs.com/blog_pic/202405161834171.png" alt="image-20240516183404085" style="zoom:67%;" /></p>
<p>那么计算图如下所示：</p>
<p><img src="https://szfmsmdx.oss-cn-hangzhou.aliyuncs.com/blog_pic/202405161834812.png" alt="image-20240516183421699" style="zoom:80%;" /></p>
<p>对于参数 <span class="math inline">\(\nabla_{W^{(1)}}J\)</span> 和
<span class="math inline">\(\nabla_{W^{(2)}}J\)</span>
，有两种不同的路径从 J 后退回到权重：</p>
<ul>
<li>一条通过交叉熵代价</li>
<li>一条通过权重衰减代价</li>
</ul>
<p>通过权重衰减的代价总是贡献 <span class="math inline">\(2\lambda
W^{(i)}\)</span></p>
<p>而通过交叉熵代价令 G 是由 cross_entropy 操作提供的对未归一化对数概率
<span class="math inline">\(U^{(2)}\)</span>
的梯度。反向传播算法现在需要探索两个不同的分支。在较短的分支上，它使用对矩阵乘法的第二个变量的反向传播规则，将
<span class="math inline">\(H^T G\)</span> 加到 <span
class="math inline">\(W^{(2)}\)</span>
的梯度上。另一条更长些的路径沿着网络逐步下降。首先，反向传播算法使用对矩阵乘法的第一个变量的反向传播规则，计算
<span class="math inline">\(\nabla_H J=GW^{(2)T}\)</span> 。接下来，relu
操作使用其反向传播规则对先前梯度的部分位置清零，这些位置对应着 <span
class="math inline">\(U^{(1)}\)</span> 中所有小于 0 的元素。记上述结果为
G ′。反向传播算法的最后一步是使用对 matmul
操作的第二个变量的反向传播规则，将 <span
class="math inline">\(X^TG&#39;\)</span> <strong>加到</strong> <span
class="math inline">\(W^{(1)}\)</span> 的梯度上。</p>
<blockquote>
<p>对于
MLP，计算成本主要来源于矩阵乘法。在前向传播阶段，我们乘以每个权重矩阵，得到了
O(w) 数量的乘-加，其中 w
是权重的数量。在反向传播阶段，我们乘以每个权重矩阵的转置，这具有相同的计算成本。</p>
<p>算法主要的存储成本是我们需要将输入存储到隐藏层的非线性中去。这些值从被计算时开始存储，直到反向过程回到了同一点。因此存储成本是
<span class="math inline">\(O(mn_h)\)</span>，其中 m
是小批量中样本的数目，<span class="math inline">\(n_h\)</span>
是隐藏单元的数量。</p>
</blockquote>
<h2 id="其他问题">其他问题</h2>
<ul>
<li>大多数软件实现需要支持可以返回多个张量的操作。</li>
<li>反向传播经常涉及将许多张量加在一起。在朴素方法中，将分别计算这些张量中的每一个，然后在第二步中对所有这些张量求和。朴素方法具有过高的存储瓶颈
<ul>
<li>可以通过保持一个缓冲器，并且在计算时将每个值加到该缓冲器中来避免该瓶颈。</li>
</ul></li>
<li>反向传播的现实实现还需要处理各种数据类型，例如 32 位浮点数、64
位浮点数和整型。处理这些类型的策略需要特别的设计考虑。</li>
<li>。。。</li>
</ul>
<h2 id="深度学习界以外的微分">深度学习界以外的微分</h2>
<p><strong>自动微分（automatic
differentiation）</strong>领域关心如何以算法方式计算导数。这里描述的反向传播算法只是自动微分的一种方法。它是一种称为
<strong>反向模式累加（reverse mode
accumulation）</strong>的更广泛类型的技术的特殊情况。</p>
<p>找到计算梯度的最优操作序列是 NP
完全问题，在这种意义上，它可能需要将代数表达式简化为它们最廉价的形式。</p>
<p>当图的输出数目大于输入的数目时，有时更偏向于使用另外一种形式的自动微分，称为
<strong>前向模式累加（forward mode accumulation）</strong>
。前向模式和后向模式的关系类似于左乘和右乘一系列矩阵之间的关系，例如
<span class="math display">\[
ABCD
\]</span> 其中的矩阵可以认为是 Jacobian 矩阵。例如，如果 D 是列向量，而
A
有很多行，那么这对应于一幅具有单个输出和多个输入的图，并且从最后开始乘，反向进行，只需要矩阵-向量的乘积。这对应着反向模式。相反，从左边开始乘将涉及一系列的矩阵-矩阵乘积，这使得总的计算变得更加昂贵。然而，如果
A 的行数小于 D 的列数，则从左到右乘更为便宜，这对应着前向模式。</p>
<h2 id="高阶微分">高阶微分</h2>
<p>在深度学习的相关领域，很少会计算标量函数的单个二阶导数。相反，我们通常对
Hessian 矩阵的性质比较感兴趣。如果我们有函数 <span
class="math inline">\(f:\mathbb R^n \to \mathbb R^n\)</span> ，那么
Hessian 矩阵的大小是 n × n。在典型的深度学习应用中，n
将是模型的参数数量，可能很容易达到数十亿。因此，完整的 Hessian
矩阵甚至不能表示。</p>
<p>典型的深度学习方法是使用 <strong>Krylov 方法（Krylov
method）</strong>，而不是显式地计算 Hessian 矩阵。Krylov
方法是用于执行各种操作的一组迭代技术，这些操作包括像近似求解矩阵的逆、或者近似矩阵的特征值或特征向量等，而不使用矩阵-向量乘法以外的任何操作。</p>
<p>为了在 Hesssian 矩阵上使用 Krylov 方法，我们只需要能够计算 Hessian
矩阵H 和一个任意向量 v 间的乘积即可。实现这一目标的一种直观方法是 <span
class="math display">\[
Hv=\nabla_x [(\nabla_x f(x))^T v]
\]</span></p>
]]></content>
      <categories>
        <category>Deep Learning</category>
      </categories>
      <tags>
        <tag>花书</tag>
        <tag>Deep Learning</tag>
        <tag>机器学习</tag>
        <tag>前馈神经网络</tag>
      </tags>
  </entry>
  <entry>
    <title>129场双周赛&amp;&amp;395场周赛</title>
    <url>/2024/04/29/129%E5%9C%BA%E5%8F%8C%E5%91%A8%E8%B5%9B-395%E5%9C%BA%E5%91%A8%E8%B5%9B/</url>
    <content><![CDATA[<p>补一下这周打的两场周赛的题。。。</p>
<span id="more"></span>
<h1 id="场双周赛">129场双周赛</h1>
<p><a
href="https://leetcode.cn/problems/find-all-possible-stable-binary-arrays-ii/description/">题目链接</a></p>
<blockquote>
<p>给你 3 个正整数 <code>zero</code> ，<code>one</code> 和
<code>limit</code> 。</p>
<p>一个二进制数组 <code>arr</code> 如果满足以下条件，那么我们称它是
<strong>稳定的</strong> ：</p>
<ul>
<li>0 在 <code>arr</code> 中出现次数 <strong>恰好</strong> 为
<code>zero</code> 。</li>
<li>1 在 <code>arr</code> 中出现次数 <strong>恰好</strong> 为
<code>one</code> 。</li>
<li><code>arr</code> 中每个长度超过<code>limit</code> 的子数组都同时包含
0 和 1 。</li>
</ul>
<p>请你返回 <strong>稳定</strong> 二进制数组的 <em>总</em> 数目。</p>
<p>由于答案可能很大，将它对 <code>1e9 + 7</code> <strong>取余</strong>
后返回。</p>
</blockquote>
<p>额额笔者一开始的思考是从组合数学去做， 首先，不考虑 limit
的情况下，一共的选法应该是 <span
class="math inline">\(C_{zoro+one}^{one}\)</span>
，组合数用杨辉三角就可以做</p>
<p>然后考虑 limit 的情况，只要减去不合法的情况即可</p>
<p>那么也就是说，选取 [limit + 1， max(zero, one)] 个连续的 1 或
0，这些都是不合法的，选完后整体法看作一个整体再插入这个数组中，但是没做出来</p>
<p>！！！错的地方在于</p>
<p>比如 limit = 1 时，选取了 2
个连续的0，这显然是不合法的，但是如果这两个被看作整体和另一个0构成了三个不合法的0，这是与取连续三个0相冲突的，所以应该用容斥原理去考虑。。。</p>
<p>虽然最后一点时间发现了问题但是已经没有时间作修改了。。。</p>
<hr />
<p>来看看正解吧，首先应该是一个经典的dp问题？原因是我们可以拆分成子规模的问题，比如说，我们记合法的数应该是
dfs(i, j, k) ，其中i、j、k分别表示0的个数、1的个数和最后一位是0还是1</p>
<p>那么举个例子，dfs(i, j, 0) 应该由 dfs(i-1, j, 0) 和 dfs(i-1, j, 1)
给出</p>
<p>但是考虑到 dfs(i-1,j,0) 表示的意义是 i-1
个0，j个1，最后一位是0，那么就有可能已经出现了连续 limit
个的情况，那么这时候，状态转移到 dfs(i, j, 0)
时，末尾再添一个0就爆了</p>
<p>==所以要排除连续 limit 个 0 的情况== ，也就是说，我们要排除
dfs(i-1,j,0) 中末尾连续 limit 个 0的情况，由于 dfs
表示的是合法的情况，所以既然末尾已经是 limit
个了，那么已经达到上限，所以倒数第 limit + 1
个位置一定是1，总的方案也应该是dfs(i-1-limit, j , 1)</p>
<p>所以 dfs(i, j, 0) = dfs(i-1,j,0)+dfs(i-1,j,1)-dfs(i-1-limit,j,1)</p>
<p>接下来考虑以下递归的边界问题？</p>
<ul>
<li>首先是 i - 1 - limit处，如果 i &lt;= limit 那显然没有必要计算了</li>
<li>其次是只放一个数字的情况，如果 i=0 且 k=1 且 j &lt;=
limit，那么只有一种情况，只放0同理</li>
</ul>
<p>最后的递归入口为 dfs(zero, one, 0) + dfs(zero, one, 1)</p>
<p>所以给出 cpp 代码如下：</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">typedef</span> <span class="type">unsigned</span> <span class="type">long</span> <span class="type">long</span> ull;</span><br><span class="line"><span class="keyword">typedef</span> <span class="type">long</span> <span class="type">long</span> ll;</span><br><span class="line"><span class="keyword">typedef</span> pair&lt;<span class="type">int</span>, <span class="type">int</span>&gt; pii;</span><br><span class="line"><span class="keyword">typedef</span> vector&lt;<span class="type">int</span>&gt; vi;</span><br><span class="line"><span class="type">const</span> <span class="type">int</span> mod = <span class="number">1e9</span> + <span class="number">7</span>;</span><br><span class="line"><span class="type">const</span> <span class="type">int</span> INF = <span class="number">0x3f3f3f3f</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    vector&lt;vector&lt;array&lt;<span class="type">int</span>, 2&gt;&gt;&gt; memo;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="type">int</span> <span class="title">dfs</span><span class="params">(<span class="type">int</span> i, <span class="type">int</span> j, <span class="type">int</span> l, <span class="type">int</span> k)</span></span>&#123;</span><br><span class="line">        <span class="keyword">if</span>(i == <span class="number">0</span>) <span class="keyword">return</span> j &lt;= l &amp;&amp; k == <span class="number">1</span>;</span><br><span class="line">        <span class="keyword">if</span>(j == <span class="number">0</span>) <span class="keyword">return</span> i &lt;= l &amp;&amp; k == <span class="number">0</span>;</span><br><span class="line">        <span class="type">int</span> &amp;res = memo[i][j][k];   <span class="comment">// 取引用;</span></span><br><span class="line">        <span class="keyword">if</span>(res != <span class="number">-1</span>) <span class="keyword">return</span> res;</span><br><span class="line">        <span class="keyword">if</span>(k == <span class="number">0</span>)&#123;</span><br><span class="line">            res = ((ll)<span class="built_in">dfs</span>(i<span class="number">-1</span>,j,l,<span class="number">0</span>)+<span class="built_in">dfs</span>(i<span class="number">-1</span>,j,l,<span class="number">1</span>)+(i&gt;l?mod-<span class="built_in">dfs</span>(i-l<span class="number">-1</span>,j,l,<span class="number">1</span>):<span class="number">0</span>))%mod;</span><br><span class="line">        &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">            res = ((ll)<span class="built_in">dfs</span>(i,j<span class="number">-1</span>,l,<span class="number">0</span>)+<span class="built_in">dfs</span>(i,j<span class="number">-1</span>,l,<span class="number">1</span>)+(j&gt;l?mod-<span class="built_in">dfs</span>(i,j-l<span class="number">-1</span>,l,<span class="number">0</span>):<span class="number">0</span>))%mod;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> res;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="type">int</span> <span class="title">numberOfStableArrays</span><span class="params">(<span class="type">int</span> zero, <span class="type">int</span> one, <span class="type">int</span> limit)</span> </span>&#123;</span><br><span class="line">        memo.<span class="built_in">resize</span>(zero+<span class="number">1</span>, vector&lt;array&lt;<span class="type">int</span>, <span class="number">2</span>&gt;&gt;(one+<span class="number">1</span>, &#123;<span class="number">-1</span>,<span class="number">-1</span>&#125;));</span><br><span class="line">        <span class="keyword">return</span> (<span class="built_in">dfs</span>(zero, one, limit, <span class="number">0</span>)%mod + <span class="built_in">dfs</span>(zero, one, limit, <span class="number">1</span>)%mod)%mod;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>
<p>观察状态转移方程：</p>
<p><span
class="math display">\[f[i][j][0]=f[i-1][j][0]+f[i-1][j][1]-f[i-limit-1][j][1]\]</span></p>
<p><span
class="math display">\[f[i][j][1]=f[i][j-1][0]+f[i][j-1][1]-f[i][j-limit-1][0]\]</span></p>
<p>于是我们可以改写成递推写法，稍微高效一些。。。</p>
<p>代码如下：</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">typedef</span> <span class="type">unsigned</span> <span class="type">long</span> <span class="type">long</span> ull;</span><br><span class="line"><span class="keyword">typedef</span> <span class="type">long</span> <span class="type">long</span> ll;</span><br><span class="line"><span class="keyword">typedef</span> pair&lt;<span class="type">int</span>, <span class="type">int</span>&gt; pii;</span><br><span class="line"><span class="keyword">typedef</span> vector&lt;<span class="type">int</span>&gt; vi;</span><br><span class="line"><span class="type">const</span> <span class="type">int</span> mod = <span class="number">1e9</span> + <span class="number">7</span>;</span><br><span class="line"><span class="type">const</span> <span class="type">int</span> INF = <span class="number">0x3f3f3f3f</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="type">int</span> <span class="title">numberOfStableArrays</span><span class="params">(<span class="type">int</span> zero, <span class="type">int</span> one, <span class="type">int</span> limit)</span> </span>&#123;</span><br><span class="line">        vector&lt;vector&lt;array&lt;<span class="type">int</span>, 2&gt;&gt;&gt; <span class="built_in">f</span>(zero + <span class="number">1</span>, vector&lt;array&lt;<span class="type">int</span>, <span class="number">2</span>&gt;&gt;(one + <span class="number">1</span>, &#123;<span class="number">0</span>, <span class="number">0</span>&#125;));</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span>(<span class="type">int</span> i = <span class="number">1</span>; i &lt;= <span class="built_in">min</span>(limit, one); ++i) f[<span class="number">0</span>][i][<span class="number">1</span>] = <span class="number">1</span>;</span><br><span class="line">        <span class="keyword">for</span>(<span class="type">int</span> i = <span class="number">1</span>; i &lt;= <span class="built_in">min</span>(limit, zero); ++i) f[i][<span class="number">0</span>][<span class="number">0</span>] = <span class="number">1</span>;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span>(<span class="type">int</span> i = <span class="number">1</span>; i &lt;= zero; ++i)&#123;</span><br><span class="line">            <span class="keyword">for</span>(<span class="type">int</span> j = <span class="number">1</span>; j &lt;= one; ++j)&#123;</span><br><span class="line">                f[i][j][<span class="number">0</span>] = ((ll)f[i<span class="number">-1</span>][j][<span class="number">0</span>]+f[i<span class="number">-1</span>][j][<span class="number">1</span>] + (i&gt;limit? mod - f[i-limit<span class="number">-1</span>][j][<span class="number">1</span>] : <span class="number">0</span>))%mod;</span><br><span class="line">                f[i][j][<span class="number">1</span>] = ((ll)f[i][j<span class="number">-1</span>][<span class="number">0</span>]+f[i][j<span class="number">-1</span>][<span class="number">1</span>] + (j&gt;limit? mod - f[i][j - limit - <span class="number">1</span>][<span class="number">0</span>] : <span class="number">0</span>))%mod;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> (f[zero][one][<span class="number">0</span>]+f[zero][one][<span class="number">1</span>])%mod;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>
<h1 id="场周赛">395场周赛</h1>
<p><a
href="https://leetcode.cn/problems/find-the-median-of-the-uniqueness-array/description/">题目连接在这里</a></p>
<blockquote>
<p>给你一个整数数组 <code>nums</code> 。数组 <code>nums</code> 的
<strong>唯一性数组</strong> 是一个按元素从小到大排序的数组，包含了
<code>nums</code> 的所有非空子数组中不同元素的个数。</p>
<p>换句话说，这是由所有 <code>0 &lt;= i &lt;= j &lt; nums.length</code>
的 <code>distinct(nums[i..j])</code> 组成的递增数组。</p>
<p>其中，<code>distinct(nums[i..j])</code> 表示从下标 <code>i</code>
到下标 <code>j</code> 的子数组中不同元素的数量。</p>
<p>返回 <code>nums</code> <strong>唯一性数组</strong> 的
<strong>中位数</strong> 。</p>
<p><strong>注意</strong>，数组的 <strong>中位数</strong>
定义为有序数组的中间元素。如果有两个中间元素，则取值较小的那个。</p>
</blockquote>
<p>额额，应该是一个二分 + 滑窗 nlogn去解决。。。</p>
<p>首先，一共是有 <span class="math inline">\(m = \frac
{n(n+1)}2\)</span> 个子数组的（一个等差数列求和即可）</p>
<p>那么中位数的下标是 <span class="math inline">\(m-1/2\)</span>
下取整的，中位数的个数是 <span class="math inline">\(k=m + 1 /
2\)</span> 下取整也就是 <span class="math inline">\(k = m/2\)</span>
上取整了。。。</p>
<p>==！那么，只需要二分中位数，记作 upper，问题就变成了 distinct 值 &lt;
upper 的子数组有多少个== (√)</p>
<p>并且记子数组个数为 cnt ，如果 k &lt; cnt ，那么自然说明二分的 upper
小了（数组长度与 distinct 呈现单调的关系），那么就更新二分的左边界</p>
<p>那如何去计算 distinct 值小于 upper 的子数组的个数？</p>
<p>用滑窗滑一轮 O(n) 即可。。。</p>
<p>用一个哈希表 freq 统计出现的个数，枚举窗口右端点，如果 freq 大小大于
upper，那么就不断移除左端点元素（就是出现次数 -1， 如果为 0 直接从 freq
移除</p>
<p>那么一共就有 r - l + 1 个子数组</p>
<p>代码如下：</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="type">int</span> <span class="title">medianOfUniquenessArray</span><span class="params">(vector&lt;<span class="type">int</span>&gt;&amp; nums)</span> </span>&#123;</span><br><span class="line">        <span class="type">int</span> n = nums.<span class="built_in">size</span>();</span><br><span class="line">        ll k = ((ll)n * (n + <span class="number">1</span>) / <span class="number">2</span> + <span class="number">1</span>) / <span class="number">2</span>;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">auto</span> check = [&amp;](<span class="type">int</span> upper)&#123;</span><br><span class="line">            ll cnt = <span class="number">0</span>;</span><br><span class="line">            <span class="type">int</span> l = <span class="number">0</span>;</span><br><span class="line">            unordered_map&lt;<span class="type">int</span>, <span class="type">int</span>&gt; freq;</span><br><span class="line">            <span class="keyword">for</span>(<span class="type">int</span> r = <span class="number">0</span>; r &lt; n; ++r)&#123;</span><br><span class="line">                freq[nums[r]] ++;</span><br><span class="line">                <span class="keyword">while</span>(freq.<span class="built_in">size</span>() &gt; upper)&#123;</span><br><span class="line">                    <span class="type">int</span> out = nums[l++];</span><br><span class="line">                    <span class="keyword">if</span>(--freq[out] == <span class="number">0</span>)&#123;</span><br><span class="line">                        freq.<span class="built_in">erase</span>(out);</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">                cnt += r - l + <span class="number">1</span>;</span><br><span class="line">                <span class="keyword">if</span>(cnt &gt;= k)&#123;</span><br><span class="line">                    <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">        &#125;;</span><br><span class="line"></span><br><span class="line">        <span class="type">int</span> left = <span class="number">0</span>, right = n;</span><br><span class="line">        <span class="keyword">while</span>(left + <span class="number">1</span>&lt; right)&#123;</span><br><span class="line">            <span class="type">int</span> mid = (left + right) / <span class="number">2</span>;</span><br><span class="line">            (<span class="built_in">check</span>(mid) ? right : left) = mid;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> right;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>Leetcode</category>
      </categories>
      <tags>
        <tag>DP</tag>
        <tag>周赛</tag>
        <tag>Leetcode</tag>
      </tags>
  </entry>
  <entry>
    <title>花书——机器学习基础</title>
    <url>/2024/04/28/%E8%8A%B1%E4%B9%A6%E2%80%94%E2%80%94%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/</url>
    <content><![CDATA[<blockquote class="blockquote-center">
<p>研读花书，皆为拙记，如有错误还望各位不惜笔墨，不啬赐教。本节主要包括<strong>机器学习基础</strong>部分内容</p>

</blockquote>
<span id="more"></span>
<blockquote>
<p>机器学习本质上属于应用统计学，更多地关注于如何用
计算机统计地估计复杂函数，不太关注为这些函数提供置信区间；因此我们会探讨两种统计学的主要方法：<strong>频率派估计</strong>和贝叶斯推断。</p>
</blockquote>
<p>大部分机器学习算法可以分为：</p>
<ul>
<li>监督学习：数据点都有一个<strong>标签（label）</strong>或者<strong>目标（target）</strong></li>
<li>无监督学习：从含有多特征的数据集中学习出这个数据集上有用的结构性质</li>
</ul>
<p>花书大部分的优化器都是采用的SGD对问题进行求解、计算的</p>
<h1 id="学习算法">学习算法</h1>
<p>机器学习算法是一种能够从数据中学习的算法。然而，我们所谓的 ‘‘学习’’
是什么意思呢？Mitchell (1997) 提供了一个简洁的定义：‘‘对于某类任务 T
和性能度量 P，一个计算机程序被认为可以从经验 E 中学习是指，通过经验 E
改进后，它在任务 T 上由性能度量 P 衡量的性能有所提升。”</p>
<h2 id="任务t">任务T</h2>
<p>通常机器学习任务定义为机器学习系统应该如何处理
<strong>样本（example）</strong></p>
<p>样本是
指我们从某些希望机器学习系统处理的对象或事件中收集到的已经量化的
<strong>特征 （feature）</strong>的集合。通常用一个向量 <span
class="math inline">\(\vec x\)</span> 来表示，其中 <span
class="math inline">\(x_i\)</span> 表示的是不同的特征</p>
<p>一般来说，常见的机器学习任务列举如下：</p>
<ul>
<li>分类</li>
<li>输入缺失型分类：通常不是学习某一个而是<strong>一组</strong>函数，每个函数对应着分类具有不同缺失输入子集，比如输入特征为n个，那么一共的缺失输入集合总数量就可能来到了<span
class="math inline">\(2^n\)</span></li>
<li>回归</li>
<li>机器翻译：输入是一种语言的符号序列，计算机程序需要将其转化为另一种语言的符号与劣，通常适用于NLP</li>
<li>结构化输出：结构化输出任务的输出是向量或者其他包含多个值的数据结构，
并且构成输出的这些不同元素间具有重要关系。例如语法分析——映射自然语言句子到语法结构树，并标记树的节点为动词、名词、副词等等。</li>
<li>异常检测</li>
<li>合成和采样：在这类任务中，机器学习程序生成一些和训练数据相似的新样本。==这是一类结构化输出任务，但是多了每个输入并非只有一个正确输出的
条件，并且我们明确希望输出有很多变化，这可以使结果看上去更加自然和真实。==</li>
<li>缺失值填补</li>
<li>去噪：（个人感觉有些类似于异常检测。。。</li>
</ul>
<h2 id="性能度量p">性能度量P</h2>
<p>对于诸如分类、缺失输入分类和转录任务，我们通常度量模型的
<strong>准确率（accuracy）</strong>
。通常，我们会更加关注机器学习算法在未观测数据上的性能如何，因为这将决
定其在实际应用中的性能。因此，我们使用 测试集（test
set）数据来评估系统性能， 将其与训练机器学习系统的训练集数据分开。</p>
<p>==性能度量的选择或许看上去简单且客观，但是选择一个与系统理想表现对应
的性能度量通常是很难的。==</p>
<p>在某些情况下，这是因为很难确定应该度量什么。例如，在执行转录任务时，我
们是应该度量系统转录整个序列的准确率，还是应该用一个更细粒度的指标，对序
列中正确的部分元素以正面评价？在执行回归任务时，我们应该更多地惩罚频繁犯一些中等错误的系统，还是较少犯错但是犯很大错误的系统？这些设计的选择取决
于应用。</p>
<h2 id="经验e">经验E</h2>
<p>本书中的大部分学习算法可以被理解为在整个
<strong>数据集（dataset）</strong>上获取经验。</p>
<p>大致说来，无监督学习涉及到观察随机向量 x 的好几个样本，试图显式或隐式
地学习出概率分布
p(x)，或者是该分布一些有意思的性质；而监督学习包含观察随 机向量 x
及其相关联的值或向量 y，然后从 x 预测 y，通常是估计 p(y | x)。</p>
<blockquote>
<p>无监督学习和监督学习不是严格定义的术语。它们之间界线通常是模糊的。很
多机器学习技术可以用于这两个任务。例如，概率的链式法则表明对于向量<span
class="math inline">\(x \in \mathbb R^n\)</span> ，联合分布可以分解成
<span class="math display">\[
p(x)=\Pi_{i=1}^n p(x_i|x_1,x_2,\cdots,x_{i-1})
\]</span></p>
<p>该分解意味我们可以将其拆分成 n
个监督学习问题，来解决表面上的无监督学习 p(x)。另外，求解监督学习问题
p(y|x) 时，也可以使用传统的无监督学习策略学习联合分布 p(x, y) ，然后推断
<span class="math inline">\(p(y|\mathrm x)=\frac{p(\mathrm x,
y)}{\sum_{y&#39;}p(\mathrm x, y&#39;)}\)</span>​</p>
</blockquote>
<p>尽管无监督学习和监督学习并非完全没有交集的正式概念，它们确实有助于粗略分
类我们研究机器学习算法时遇到的问题。传统地，人们将回归、分类或者结构化输
出问题称为监督学习。支持其他任务的密度估计通常被称为无监督学习。</p>
<p>学习范式的其他变种也是有可能的。例如，半监督学习中，一些样本有监督目
标，但其他样本没有。在多实例学习中，样本的整个集合被标记为含有或者不含有
该类的样本，但是集合中单独的样本是没有标记的。</p>
<p>有些机器学习算法并不是训练于一个固定的数据集上。例如，
<strong>强化学习（reinforcement
learning）</strong>算法会和环境进行交互，所以学习系统和它的训练过程会有反
馈回路。</p>
<p>表示数据集的常用方法是 <strong>设计矩阵（design
matrix）</strong>。设计矩阵的每一行包含
一个不同的样本。每一列对应不同的特征。</p>
<h1 id="容量过拟合与欠拟合">容量、过拟合与欠拟合</h1>
<p>机器学习的主要挑战是我们的算法必须能够在先前未观测的新输入上表现良好，
而不只是在训练集上表现良好。在先前未观测到的输入上表现良好的能力被称为
<strong>泛化（generalization）</strong>。</p>
<p>通常情况下，当我们训练机器学习模型时，我们可以使用某个训练集，在训练
集上计算一些被称为 <strong>训练误差（training
error）</strong>的度量误差，目标是降低训练误差。同时，我们在降低训练误差的过程中也希望<strong>泛化误差（generalization
error）</strong>很低</p>
<p>训练集和测试集数据通过数据集上被称为 <strong>数据生成过程（data
generating
process）</strong>的概率分布生成。通常，我们会做一系列被统称为
<strong>独立同分布假设（i.i.d.
assumption）</strong>的假设。该假设是说，每个数据集中的样本都是彼此
<strong>相互独立的（independent）</strong>，并且训练集和测试集是
<strong>同分布的（identically
distributed）</strong>，采样自相同的分布。这个假设使我们能够在单个样本的概率分布描述数据生成过程。然后相同的分布可以用来生成每一个训练样本和每一个测试样本。我们将这个共享的潜在
分布称为 <strong>数据生成分布（data generating
distribution）</strong>，记作 <span
class="math inline">\(p_{data}\)</span> 。</p>
<p>我们能观察到训练误差和测试误差之间的直接联系是，<strong>随机模型训练误差的期望和该模型测试误差的期望是一样的</strong>。我们采样得到训练集，然后挑选参数去降低训练集误差，然后采样得到测试集。在这个过程中，<strong>测试误差期望会大于或等于训练误差期望</strong>。以下是决定机器学习算法效果是否好的因素：</p>
<ol type="1">
<li>降低训练误差</li>
<li>缩小训练误差和测试误差的差距</li>
</ol>
<p>这两个因素对应机器学习的两个主要挑战：</p>
<ul>
<li><strong>欠拟合（underfitting）</strong>：模型不能在训练集上获得足够低的误差</li>
<li><strong>过拟合
（overfitting）</strong>：训练误差和测试误差之间的差距太大</li>
</ul>
<p>通过调整模型的
<strong>容量（capacity）</strong>，我们可以控制模型是否偏向于过拟合或者欠
拟合。通俗地，模型的容量是<strong>指其拟合各种函数的能力</strong>。容量低的模型可能很难拟合训练集；容量高的模型可能会过拟合，因为记住了不适用于测试集的训练集性质。</p>
<p>一种控制训练算法容量的方法是选择 <strong>假设空间（hypothesis
space）</strong>，即学习算法可以选择为解决方案的函数集。例如，线性回归算法将关于其输入的所有线性函数作为假设空间。广义线性回归的假设空间包括多项式函数，而非仅有线性函数。这样做就增加了模型的容量。</p>
<p>一次多项式<span class="math inline">\(\hat y=b+wx\)</span> 通过引入
<span class="math inline">\(x^2\)</span>
作为线性回归模型的另一个特征，我们能够学习到 x 的二次函数模型 <span
class="math inline">\(\hat y=b+w_1x+w_2x^2\)</span>
。尽管该模型是输入的二次函数，但输出仍是参数的线性函数。</p>
<p>当机器学习算法的容量适合于所执行任务的复杂度和所提供训练数据的数量时，
算法效果通常会最佳。容量不足的模型不能解决复杂任务。容量高的模型能够解决
复杂的任务，但是当其容量高于任务所需时，有可能会过拟合。</p>
<p><img src="https://szfmsmdx.oss-cn-hangzhou.aliyuncs.com/blog_pic/202405051408206.png" alt="image-20240505140857160" style="zoom:50%;" /></p>
<p>模型规定了调整参数降低训练目标时，学习算法可以从哪些函数族中选择函数。这被称为模型的
<strong>表示容量（representational capacity）</strong>
。在很多情况下，从这些函数中挑选出最优函数是非常困难的优化问题。实际中，学习算法不会真的找到最优函数，而仅是找到一个可以大大降低训练误差的函数。额外的限制因素，比如优化算法的不完美，意味着学习算法的
<strong>有效容量（effective
capacity）</strong>可能小于模型族的表示容量。</p>
<hr />
<p><strong>奥卡姆剃刀（Occam‘s
razor）</strong>原则：在同样能够解释已知观测现象的假设中，我们 应该挑选
‘‘最简单’’ 的那一个。</p>
<p>统计学习理论提供了量化模型容量的不同方法。在这些中，最有名的是
<strong>VapnikChervonenkis 维度（Vapnik-Chervonenkis dimension,
VC）</strong>。VC维定义为该分类器<strong>能够分类的训练样本的最大数目</strong>。</p>
<p>假设存在 m 个不同 x 点的训练集，分类器可以任意地标记该 m 个不同的 x
点，VC维被定义为 m 的最大可能值</p>
<p>参见<a
href="https://tangshusen.me/2018/12/09/vc-dimension/#comments">这篇文章</a>对
VC 维的推导</p>
<p>简单来说，在该模型对应的空间中随机撒x点，然后对其中的每个点随机分配一个2类标签，使用你的模型来分类，并且要分对，请问x至多是多少。这个x就是VC维。</p>
<p>比如对于线性函数来说：</p>
<p>如果是二维空间里的线性函数的话，那么他的VC维应该是3，因为对于4个点，存在一种情况不管用什么直线都无论如何做不到完全分类的</p>
<p><img src="https://szfmsmdx.oss-cn-hangzhou.aliyuncs.com/blog_pic/202405051445577.png" alt="image-20240505144512543" style="zoom:50%;" /></p>
<p>如果选择三维空间的线性函数的话，那么该模型的VC维应该是4</p>
<p>所以VC维越大，也就是说他能够包含的情况越多，假设空间大，也就是他的容量大</p>
<hr />
<p>我们必须记住虽然更简单的函数更可能泛化，但我们仍然需要选择一个充分复杂的假设以达到低的训练误差。</p>
<p>通常，当模型容量上升时，训练误差会下降，直到其渐近最小可能误差（假设误差度量有最小值）。通常，泛化误差是一个关于模型容量的
U 形曲线函数。</p>
<p><img src="https://szfmsmdx.oss-cn-hangzhou.aliyuncs.com/blog_pic/202405051450234.png" alt="image-20240505145055192" style="zoom:60%;" /></p>
<p>为考虑容量任意高的极端情况，我们介绍
<strong>非参数（non-parametric）</strong>模型的概念。有时，非参数模型仅是一些不能实际实现的理论抽象（比如搜索所有可能概率分布的算法）。然而，我们也可以设计一些实用的非参数模型，使它们的复杂度和训练集大小有关。</p>
<h2 id="没有免费午餐定理">没有免费午餐定理</h2>
<p>机器学习的 <strong>没有免费午餐定理（no free lunch
theorem）</strong>表明 (Wolpert,
1996)，在所有可能的数据生成分布上平均之后，每一个分类算法在未事先观测的点上都有相同的错误率。换言之，在某种意义上，没有一个机器学习算法总是比其他的要好。我们能够设想的最先进的算法和简单地将
所有点归为同一类的简单算法有着相同的平均性能（在所有可能的任务上）。</p>
<h2 id="正则化">正则化</h2>
<p>没有免费午餐定理暗示我们必须在特定任务上设计性能良好的机器学习算法。</p>
<p>算法的效果不仅很大程度上受影响于假设空间的函数数量，也取决于这些函数
的具体形式。例如，我们用线性回归，从 x 预测
sin(x)，效果不会好。因此我们可以通过两种方式控制算法的性能：</p>
<ol type="1">
<li>允许使用的函数种类</li>
<li>这些函数的数量</li>
</ol>
<p>在假设空间中，相比于某一个学习算法，我们可能更偏好另一个学习算法。这
意味着两个函数都是符合条件的，但是我们更偏好其中一个。只有非偏好函数比偏
好函数在训练数据集上效果明显好很多时，我们才会考虑非偏好函数。</p>
<p>例如，我们可以加入 <strong>权重衰减（weight
decay）</strong>来修改线性回归的训练标准。带权重衰减的线性回归最小化训练集上的均方误差和正则项的和
J(w)，其偏好于平方 <span class="math inline">\(L_2\)</span>
范数较小的权重。具体如下： <span class="math display">\[
J(w)=\mathrm {MSE}_{\mathrm {train}}+\lambda w^T w
\]</span></p>
<p>其中 λ 是提前挑选的值，控制我们偏好小范数权重的程度。当 λ =
0，我们没有任何偏好。越大的 λ 偏好范数越小的权重。</p>
<p>更一般地，正则化一个学习函数 f(x; θ)
的模型，我们可以给代价函数添加被称为
<strong>正则化项（regularizer）</strong>的惩罚。在权重衰减的例子中，正则化项是
<span class="math inline">\(\Omega(w)=w^T w\)</span></p>
<p><img src="https://szfmsmdx.oss-cn-hangzhou.aliyuncs.com/blog_pic/202405051511901.png" alt="image-20240505151147854" style="zoom:67%;" /></p>
<p>在我们权重衰减的示例中，通过在最小化的目标中额外增加一项，我们明确地表示了偏好权重较小的线性函数。有很多其他方法隐式或显式地表示对不同解的偏好。总而言之，这些不同的方法都被称为
<strong>正则化（regularization）</strong>。正则化是指我们修改学习算法，使其降低<strong>泛化误差</strong>而<strong>非训练误差</strong>。</p>
<h1 id="超参数和验证集">超参数和验证集</h1>
<p>参考了<a
href="https://blog.csdn.net/qq_24884193/article/details/104071664">这篇文章</a></p>
<p>几个数据集的辨析。。。</p>
<ul>
<li>训练集：训练集用来训练模型，即确定模型的权重和偏置这些参数，通常我们称这些参数为学习参数。</li>
<li>测试集：与训练集同分布的样本组成，只使用一次，即在训练完成后评价最终的模型时使用。</li>
<li>验证集：为了挑选超参数，比如网络层数、网络节点数、迭代次数、学习率这些都叫超参数。</li>
</ul>
<p>通常，80% 的训练数据用于训练，20% 用于验证。由于验证集是用来 ‘‘训练’’
超参数的，尽管验证集的误差通常会比训练集误差小，验证集会低估泛化误差。所有超参数优化完成之后，泛化误差可能会通过测试集来估计。</p>
<figure>
<img
src="https://szfmsmdx.oss-cn-hangzhou.aliyuncs.com/blog_pic/202405051521597.png"
alt="image-20240505152122552" />
<figcaption aria-hidden="true">image-20240505152122552</figcaption>
</figure>
<h2 id="交叉验证">交叉验证</h2>
<p>之所以出现交叉验证，主要是因为训练集较小。无法直接像前面那样只分出训练集，验证集，测试就可以了（简单交叉验证）。</p>
<p>需要说明的是，在实际情况下，<u>人们不是很喜欢用交叉验证</u>，主要是因为它会<strong>耗费较多的计算资源</strong>。一般直接把训练集按照50%-90%的比例分成训练集和验证集。但这也是根据具体情况来定的：如果超参数数量多，你可能就想用更大的验证集，而验证集的数量不够，那么最好还是用交叉验证吧。至于分成几份比较好，一般都是分成3、5和10份。</p>
<figure>
<img
src="https://szfmsmdx.oss-cn-hangzhou.aliyuncs.com/blog_pic/202405051523070.png"
alt="image-20240505152339013" />
<figcaption aria-hidden="true">image-20240505152339013</figcaption>
</figure>
<p>假设将训练集分成5份（该数目被称为折数，5-fold交叉验证），每次都用其中4份来训练模型，粉红色的那份用来验证4份训练出来的模型的准确率，记下准确率。</p>
<p><code>然后再次在这5份中取另外4份做训练集，1份做验证集，再次得到一个模型的准确率</code></p>
<p>(五选四的话，只用做五遍就ok了)
直到所有5份都做过1次验证集，也即验证集名额循环了一圈，交叉验证的过程就结束。算得这5次准确率的均值。留下准确率最高的模型，即该模型的超参数是什么样的最终模型的超参数就是这个样的。</p>
<h1 id="最大似然估计">最大似然估计</h1>
<p>考虑一组含有 m 个样本的数据集 <span class="math inline">\(\mathbb
X=\{x^{(1)},\cdots,x^{(m)}\}\)</span> ，独立地由未知的真实数据生成分布
<span class="math inline">\(p_{data}(\mathrm x)\)</span> 生成</p>
<p>令 <span class="math inline">\(p_{model}(\mathrm x;\theta)\)</span>
是一族<strong>由 <span class="math inline">\(\theta\)</span>
确定在相同空间上的概率分布</strong>。换言之， <span
class="math inline">\(p_{model}(x;\theta)\)</span> 将任意输入 x
映射到实数来估计真实概率 <span
class="math inline">\(p_{data}(x)\)</span></p>
<p>对 θ 的最大似然估计被定义为：</p>
<p><img src="https://szfmsmdx.oss-cn-hangzhou.aliyuncs.com/blog_pic/202405051542138.png" alt="image-20240505154225093" style="zoom:50%;" /></p>
<p>多个概率的乘积会因很多原因不便于计算。例如，计算中很可能会出现数值下溢。为了得到一个便于计算的等价优化问题，我们观察到似然对数不会改变其
arg max 但是将乘积转化成了便于计算的求和形式：</p>
<p><img src="https://szfmsmdx.oss-cn-hangzhou.aliyuncs.com/blog_pic/202405051545774.png" alt="image-20240505154529731" style="zoom:50%;" /></p>
<blockquote>
<p>一种解释最大似然估计的观点是将它看作最小化训练集上的经验分布 <span
class="math inline">\(\hat p_{data}\)</span>
和模型分布之间的差异，两者的差异程度可以通过KL散度衡量。即 <span
class="math inline">\(D_{KL}(\hat p_{data} || p_{model})=\mathbb
E_{\mathrm x  \sim \hat p_{model}}[\log \hat p_{data}(x)-\log
p_{model}(x)]\)</span></p>
<p>左边一项仅涉及到数据生成过程，和模型无关。这意味着当我们训练模型最小化
KL散度时，我们只需要最小化<span class="math inline">\(-\mathbb
E_{\mathrm x  \sim \hat p_{model}}[\log p_{model}(x)]\)</span></p>
</blockquote>
<h2 id="条件对数似然和均方误差">条件对数似然和均方误差</h2>
<p>最大似然估计很容易扩展到估计条件概率 <span
class="math inline">\(P(\mathrm y|\mathrm x;\theta)\)</span> ，从而给定
x 预测 y。实际上这是最常见的情况，因为这构成了大多数监督学习的基础。如果
X 表示所有的输入，Y 表示我们观测到的目标，那么条件最大似然估计是 <span
class="math inline">\(\theta_{ML}=\mathrm {arg}\max_\theta
P(Y|X;\theta)\)</span> 如果假设样本是独立同分布的，那么这可以分解成
<span class="math display">\[
\theta_{ML}=\mathrm{arg}\max_\theta \sum_{i=1}^m \log
P(y^{(i)}|x^{(i)};\theta)
\]</span> 在最大似然的角度下重新审视线性回归，在假设样本是
i.i.d，那么条件对数似然如下：</p>
<p><img src="https://szfmsmdx.oss-cn-hangzhou.aliyuncs.com/blog_pic/202405051602846.png" alt="image-20240505160206791" style="zoom:67%;" /></p>
<p>其中 <span class="math inline">\(y^{(i)}\)</span>​ 是线性回归在第 i
个输入 x (i) 上的输出，m 是训练样本的数目。对比均方 误差和对数似然</p>
<p><img src="https://szfmsmdx.oss-cn-hangzhou.aliyuncs.com/blog_pic/202405051602712.png" alt="image-20240505160242665" style="zoom:67%;" /></p>
<h2 id="最大似然的性质">最大似然的性质</h2>
<p>最大似然估计最吸引人的地方在于，它被证明当样本数目 <span
class="math inline">\(m\to \infty\)</span>
时，就收敛率而言是最好的渐近估计。</p>
<p>在合适的条件下，最大似然估计具有一致性，意味着训练样本数目趋向于无穷大时，参数的最大似然估计会收敛到参数的真实值。这些条件是：</p>
<ul>
<li>真实分布 <span class="math inline">\(p_{data}\)</span> 必须在模型族
<span class="math inline">\(p_{model}(·;\theta)\)</span>
中。否则，没有估计可以还原 <span
class="math inline">\(p_{data}\)</span></li>
<li>真实分布 <span class="math inline">\(p_{data}\)</span>
必须刚好对应一个 θ 值。否则，最大似然估计恢复出真实分布 <span
class="math inline">\(p_{data}\)</span>
后，也不能决定数据生成过程使用哪个 θ。</li>
</ul>
<h1 id="贝叶斯统计">贝叶斯统计</h1>
<blockquote>
<p>老实说这块我学的云里雾里，有时间会好好学一遍贝叶斯统计。。。</p>
</blockquote>
<p>至此我们已经讨论了 <strong>频率派统计（frequentist
statistics）</strong>方法和基于估计单一值 θ
的方法，然后基于该估计作所有的预测。另一种方法是在做预测时会考虑所有可能的
θ。后者属于 <strong>贝叶斯统计（Bayesian
statistics）</strong>的范畴。</p>
<p>贝叶斯用概率反映知识状态的确定性程度。数据集能够被直接观测到，因此不是随机的。另一方面，真实参数
θ 是未知或不确定的，因此可以表示成随机变量。</p>
<p>在观察到数据前，我们将 θ 的已知知识表示成 <strong>先验概率分布（prior
probability distribution）</strong>，p(θ)（有时简单地称为
‘‘先验’’）。一般而言，机器学习实践者会选择一个相当宽泛的（即，高熵的）先验分布，反映在观测到任何数据前参数
θ 的高度不确定性。例如，我们可能会假设先验 θ
在有限区间中均匀分布。<strong>许多先验偏好于‘‘更简单’’
的解</strong>（如小幅度的系数，或是接近常数的函数）。</p>
<p>假设我们有一组数据样本 <span
class="math inline">\(\{x^{(1)},\cdots,x^{(m)}\}\)</span>，通过贝叶斯规则结合数据似然
<span class="math inline">\(p(x^{(1)},\cdots,x^{(m)}|\theta)\)</span>
和先验，我们可以恢复数据对我们关于 θ 信念的影响：</p>
<p><img src="https://szfmsmdx.oss-cn-hangzhou.aliyuncs.com/blog_pic/202405051626129.png" alt="image-20240505162658072" style="zoom: 50%;" /></p>
<p>在贝叶斯估计常用的情景下，先验开始是相对均匀的分布或高熵的高斯分布，观测数据通常会使后验的熵下降，并集中在参数的几个可能性很高的值。</p>
<p>相对于最大似然估计，贝叶斯估计有两个重要区别。<strong>第一</strong>，不像最大似然方法预测时使用
θ 的点估计，贝叶斯方法使用 θ 的全分布。例如，在观测到 m
个样本后，下一个数据样本 x (m+1) 的预测分布如下：</p>
<p><img src="https://szfmsmdx.oss-cn-hangzhou.aliyuncs.com/blog_pic/202405051631677.png" alt="image-20240505163108624" style="zoom: 50%;" /></p>
<p>这里，每个具有正概率密度的 θ
的值有助于下一个样本的预测，其中贡献由后验密度本身加权。在观测到数据集
<span class="math inline">\(\{x^{(1)},\cdots,x^{(m)}\}\)</span>
之后，如果我们仍然非常不确定 θ
的值，那么这个不确定性会直接包含在我们所做的任何预测中。</p>
<p>贝叶斯方法和最大似然方法的<strong>第二</strong>个最大区别是由贝叶斯先验分布造成的。先验能够影响<strong>概率质量密度朝参数空间中偏好先验的区域偏移</strong>。实践中，先验通常表现为偏好更简单或更光滑的模型。对贝叶斯方法的批判认为先验是人为主观判断影响预测的来源。</p>
<blockquote>
<p>当训练数据很有限时，贝叶斯方法通常泛化得更好，但是当训练样本数目很大时，通常会有很大的计算代价。</p>
</blockquote>
<h2 id="贝叶斯线性回归">贝叶斯线性回归</h2>
<p>使用贝叶斯估计方法学习线性回归的参数</p>
<p>在线性回归中，我们学习从输入向量 <span class="math inline">\(x\in
\mathbb R^n\)</span> 预测标量 <span class="math inline">\(y\in
R\)</span> 的线性映射。该预测由向量 <span class="math inline">\(w\in
\mathbb R^n\)</span> 参数化：<span class="math inline">\(\hat y=w^T
x\)</span></p>
<p>给定一组 m 个训练样本 <span
class="math inline">\((X^{(train)},y^{(train)})\)</span>
可以表示整个训练集对 y 的预测： <span class="math inline">\(\hat
y^{(train)}=X^{(train)} w\)</span> 表示为<span
class="math inline">\(y^{(train)}\)</span> 上的高斯条件分布</p>
<p><img src="https://szfmsmdx.oss-cn-hangzhou.aliyuncs.com/blog_pic/202405051645107.png" alt="image-20240505164530045" style="zoom: 50%;" /></p>
<p>其中，我们根据标准的 MSE 公式假设 y 上的高斯方差为
1。为确定模型参数向量 w
的后验分布，我们首先需要指定一个先验分布。实数值参数通常使用高斯作为先验分布：</p>
<p><img src="https://szfmsmdx.oss-cn-hangzhou.aliyuncs.com/blog_pic/202405051647077.png" alt="image-20240505164730017" style="zoom:50%;" /></p>
<p>其中，<span class="math inline">\(\mu_0\)</span> 和 <span
class="math inline">\(\Lambda_0\)</span>
分别是先验分布的均值向量和协方差矩阵。</p>
<p>确定好先验后，我们现在可以继续确定模型参数的后验分布。</p>
<p><img src="https://szfmsmdx.oss-cn-hangzhou.aliyuncs.com/blog_pic/202405051649351.png" alt="image-20240505164924280" style="zoom: 50%;" /></p>
<p>现在我们定义 <span class="math inline">\(\Lambda_m=(X^T
X+\Lambda_0^{-1})^{-1}\)</span> 和 <span
class="math inline">\(\mu_m=\Lambda_m(X^T
y+\Lambda_0^{-1}\mu_0)\)</span>
，利用新变量可以将后验改写为高斯分布：</p>
<p><img src="https://szfmsmdx.oss-cn-hangzhou.aliyuncs.com/blog_pic/202405051652868.png" alt="image-20240505165252802" style="zoom:50%;" /></p>
<p>大多数情况下，我们设置 <span class="math inline">\(\mu_0=0\)</span>
。如果我们设置 <span class="math inline">\(\Lambda_0=\frac 1\alpha
I\)</span> ，那么 <span class="math inline">\(\mu_m\)</span> 对 w
的估计就和频率派带权重衰减惩罚 <span class="math inline">\(\alpha w^T
w\)</span>​ 的线性回归的估计是一样的。</p>
<h2 id="最大后验map估计">最大后验（MAP）估计</h2>
<p>原则上，我们应该使用参数 θ
的完整贝叶斯后验分布进行预测，但单点估计常常也是需要的。MAP
估计选择后验概率最大的点（或在 θ
是连续值的更常见情况下，概率密度最大的点）：</p>
<p><span class="math display">\[\theta_\mathrm {MAP}=\mathrm {arg}
\max_\theta p(\theta | x)=\mathrm {arg}\max_\theta \log p(x|\theta)+\log
p(\theta)\]</span></p>
<p>我们可以认出上式右边的 <span class="math inline">\(\log
p(x|\theta)\)</span> 对应着标准的对数似然项，<span
class="math inline">\(\log p(\theta)\)</span> 对应先验分布</p>
<h1 id="监督学习算法">监督学习算法</h1>
<p>粗略地说，监督学习算法是给定一组输入 x 和输出 y
的训练集，学习如何关联输入和输出。很多情况下， 由于 y
很难自动收集，需要人来提供“监督”（不过该术语仍然适用于训练集目标可以被自动收集的情况</p>
<h2 id="概率监督学习">概率监督学习</h2>
<p>花书大部分监督学习算法都是基于估计概率分布 <span
class="math inline">\(p(y|\mathrm x)\)</span>
的，可以使用最大似然估计找到对于有参分布族 <span
class="math inline">\(p(y|\mathrm x;\theta)\)</span> 最好的参数向量
<span class="math inline">\(\theta\)</span></p>
<p>显然线性回归对应分布族为 <span
class="math inline">\(p(y|x;\theta)=\mathcal N(y;\theta^T
x,I)\)</span></p>
<p>我们用于线性回归的实数正态分布是用均值参数化的。二元变量上的分布稍微复杂些，因为它的均值必须始终在
0 和 1之间。解决这个问题的一种方法是使用 logistic sigmoid
函数将线性函数的输出压缩进区间 (0, 1)。该值可以解释为概率： <span
class="math inline">\(p(y=1|x;\theta)=\sigma(\theta^T x)\)</span></p>
<p>这个方法被称为 <strong>逻辑回归（logistic
regression）</strong>这个名字有点奇怪，因为该模型用于分类而非回归。</p>
<h2 id="支持向量机">支持向量机</h2>
<p><strong>支持向量机（support vector machine,
SVM）</strong>是监督学习中最有影响力的方法之一。不同于逻辑回归的是，支持向量机不输出概率，只输出类别。当
<span class="math inline">\(w^T x+b\)</span>
为正时，支持向量机预测属于正类。类似地，当 <span
class="math inline">\(w^T+b\)</span>
为负时，支持向量机预测属于负类。</p>
<p>支持向量机的一个重要创新是 核技巧（kernel
trick）。核技巧观察到许多机器学习算法都可以写成样本间点积的形式。例如，支持向量机中的线性函数可以重写为
<span class="math display">\[
w^T x+b=b+\sum_{i=1}^m\alpha_i x^T x^{(i)}
\]</span> 其中 <span class="math inline">\(x^{(i)}\)</span>
是训练样本，<span class="math inline">\(\alpha\)</span>
是系数向量，学习算法重写为这种形式允许我们将 x 替换为特征函数 <span
class="math inline">\(\phi (x)\)</span>
的输出，点积替换为被称为核函数（kernel function）的函数 <span
class="math inline">\(k(x, x^{(i)})=\phi(x)·\phi(x^{(i)})\)</span></p>
<p>使用核估计替换点积之后，我们可以使用如下函数进行预测： <span
class="math display">\[
f(x)=b+\sum_i \alpha_i k(x,x^{(i)})
\]</span> 这个函数关于 x 是非线性的，关于 ϕ(x) 是线性的。α 和 f(x)
之间的关系也是线性的。核函数完全等价于用 ϕ(x)
预处理所有的输入，然后在新的转换空间学习线性模型。</p>
<p>核技巧有两大优势：</p>
<ol type="1">
<li>它使我们能够使用保证有效收敛的凸优化技术来学习非线性模型（关于 x
的函数）</li>
<li>核函数 k 的实现方法通常有比直接构建 ϕ(x) 再算点积高效很多。</li>
</ol>
<p>在某些情况下，<span class="math inline">\(\phi(x)\)</span>
甚至可以是无限维的，对于普通的显式方法而言，这将是无限的计算代价。在很多情况下，即使
<span class="math inline">\(\phi(x)\)</span> 是难算的，<span
class="math inline">\(k(x,x&#39;)\)</span> 却会是一个关于
x非线性的、易算的函数。假设这个映射返回一个由开头 x 个 1，随后是无限个0
的向量。我们可以写一个核函数 <span
class="math inline">\(k(x,x&#39;)=\min(x,x&#39;)\)</span>
，完全等价于对应的无限维点积。</p>
<p>最常用的核函数是 <strong>高斯核（Gaussian kernel）</strong>： <span
class="math display">\[
k(u,v)=\mathcal N(u-v;0,\sigma^2I)
\]</span> 其中 <span class="math inline">\(\mathcal
N(x;\mu,\Sigma)\)</span>​ 是标准正态密度。这个核也被称为
<strong>径向基函数（radial basis function, RBF）</strong>核，因为其值沿
v 中从 u
向外辐射的方向减小。高斯核对应于<strong>无限维空间</strong>中的点积，但是该空间的推导没有整数上最小核的示例那么直观。</p>
<p>我们可以认为高斯核在执行一种<strong>模板匹配 (template
matching)</strong>。训练标签 y 相关的训练样本 x 变成了类别 y
的模版。当测试点 x ′ 到 x
的欧几里得距离很小，对应的高斯核响应很大时，表明 x ′ 和模版 x
非常相似。该模型进而会赋予相对应的训练标签 y
较大的权重。总的来说，预测将会组合很多这种通过训练样本相似度加权的训练标签。</p>
<p>核机器的一个主要缺点是计算决策函数的成本关于训练样本的数目是线性的。因为第
i 个样本贡献 <span class="math inline">\(\alpha_i k(x,x^{(i)})\)</span>
到决策函数。支持向量机能够通过学习主要包含零的向量
α，以缓和这个缺点。那么判断新样本的类别仅需要计算非零 αi
对应的训练样本的核函数。这些训练样本被称为 <strong>支持向量（support
vector）</strong></p>
<h1 id="无监督学习算法">无监督学习算法</h1>
<p>本质上来说，无监督学习只处理“特征”而不过多将注意力放在监督信号上。</p>
<p>经典的无监督学习任务是找到数据的“最佳”表示，常见的三种包括：</p>
<ul>
<li>低维表示：低维表示尝试将 x 中的信息尽可能压缩在一个较小的表示中</li>
<li>稀疏表示：将数据集嵌入到输入项大多数为零的表示中（通常适用于<strong>增加表示维度的情况</strong></li>
<li>独立表示：试图分开数据分布中变化的来源，使得表示的维度是统计独立的。</li>
</ul>
<h2 id="主成分分析pca">主成分分析（PCA）</h2>
<p>PCA可以视作学习一种比原始输入维数更低的表示，他也学习了元素之间彼此没有线性相关的表示。</p>
<p><img src="https://szfmsmdx.oss-cn-hangzhou.aliyuncs.com/blog_pic/202405061227011.png" alt="image-20240506122705917" style="zoom:67%;" /></p>
<blockquote>
<p>PCA 学习一种线性投影，使最大方差的方向和新空间的轴对齐。(左)
原始数据包含了 x
的样本。在这个空间中，方差的方向与轴的方向并不是对齐的。(右)
变换过的数据 <span class="math inline">\(z=x^T W\)</span> 在轴 <span
class="math inline">\(z_1\)</span>
的方向上有最大的变化。第二大变化方差的方向沿着轴 <span
class="math inline">\(z_2\)</span> 。</p>
</blockquote>
<p>假设有一个 <span class="math inline">\(m\times n\)</span> 的设计矩阵
<span class="math inline">\(X\)</span> ，数据均值为0，即 <span
class="math inline">\(\mathbb E[x]=0\)</span> ，那么 <span
class="math inline">\(X\)</span> 对应的无偏协方差矩阵应该是 <span
class="math inline">\(Var[x]=\frac 1{m-1} X^T X\)</span></p>
<p>那么 PCA 通过线性变换找到一个 <span class="math inline">\(\mathrm
{Var}[z]\)</span> 是对角阵，（在忽略常数的情况下有）<span
class="math inline">\(Z=X W\)</span> ，再看 z 的协方差矩阵也就是 <span
class="math inline">\(W^T X^T X W=\Lambda\)</span> ，所以自然有 <span
class="math inline">\(X^T X=W\Lambda W^T\)</span></p>
<p>通过SVD分解也可以推导PCA，同时更能说明其PCA后的 <span
class="math inline">\(\mathrm {Var}[z]\)</span> 是对角的： <span
class="math display">\[
X^T X=(U\Sigma W^T)^T U\Sigma W^T=W \Sigma^2 W^T
\]</span> 使用 X 的 SVD 分解，X 的方差可以表示为 <span
class="math display">\[
\begin{align}
\mathrm {Var}[x]&amp;=\frac 1{m-1} X^T X \\
&amp;= \frac 1{m-1} (U\Sigma W^T)^T U\Sigma W^T \\
&amp;= \frac 1{m-1} W\Sigma^T U^T U\Sigma W^T \\
&amp;= \frac 1{m-1} W\Sigma ^2 W^T
\end{align}
\]</span> 那么再观察 z 的协方差矩阵： <span class="math display">\[
\begin{align}
\mathrm {Var}[z]&amp;=\frac 1{m-1} Z^T Z \\
&amp;=\frac 1{m-1}W^T X^T X W \\
&amp;=\frac 1{m-1} W^T W \Sigma ^2 W^T W \\
&amp;=\frac 1{m-1} \Sigma^2
\end{align}
\]</span> 以上分析指明当我们通过线性变换 W 将数据 x 投影到 z
时，得到的数据表示的协方差矩阵是对角的（即 Σ 2），立刻可得 z
中的元素是彼此无关的。</p>
<p><font color=#00FFFF>PCA
这种将数据变换为元素之间彼此不相关表示的能力是 PCA
的一个重要性质。它是消除数据中未知变化因素的简单表示示例。在 PCA
中，这个消除是通过寻找输入空间的一个旋转（由 W
确定），使得方差的主坐标和 z 相关的新表示空间的基对齐。</font></p>
<h2 id="k-means聚类">K-means聚类</h2>
<p>k-均值聚类算法将训练集分成
k个靠近彼此的不同样本聚类。因此我们可以认为该算法提供了 k-维的 one-hot
编码向量 h 以表示输入 x。当 x 属于聚类 i 时，有 <span
class="math inline">\(h_i=1\)</span> ，h 的其他项为零。</p>
<p>k-均值聚类提供的 one-hot
编码也是一种稀疏表示，因为每个输入的表示中大部分元素为零。</p>
<p>k-均值聚类初始化 k 个不同的中心点 <span
class="math inline">\(\{\mu^{(1)},\cdots,\mu^{(k)} \}\)</span>
，然后迭代交换两个不同的步骤直到收敛</p>
<ul>
<li>步骤一，每个训练样本分配到最近的中心点 <span
class="math inline">\(\mu^{(i)}\)</span> 所代表的聚类 i</li>
<li>步骤二，每一个中心点 <span class="math inline">\(\mu^{(i)}\)</span>
更新为聚类 i 中所有训练样本 <span class="math inline">\(x^{(j)}\)</span>
的均值</li>
</ul>
<blockquote>
<p>关于聚类的一个问题是聚类问题本身是病态的。这是说没有单一的标准去度量聚类的数据在真实世界中效果如何。我们可能希望找到和
一个特征相关的聚类，但是得到了一个和任务无关的，同样是合理的不同聚类。</p>
<p>例如，假设我们在包含红色卡车图片、红色汽车图片、灰色卡车图片和灰色汽车图片的数据集上运行两个聚类算法。如果每个聚类算法聚两类，那么可能一个算法将汽车和卡车各聚一类，另一个根据红色和灰色各聚一类。</p>
</blockquote>
<h1 id="随机梯度下降">随机梯度下降</h1>
<p>机器学习算法中的代价函数通常可以分解成每个样本代价函数的总和，比如：训练数据的负条件对数似然函数通常可以写成
<span class="math display">\[
J(\theta)=\mathbb E_{x,y\sim \hat p_{data}}L(x,y,\theta)=\frac 1m
\sum_{i=1}^m L(x^{(i)},y^{(i)},\theta)
\]</span> 其中 L 是每个样本的损失 <span
class="math inline">\(L(x,y,\theta)=-\log p(y|x;\theta)\)</span></p>
<p>对于这些相加的代价函数，梯度下降需要计算 <span
class="math display">\[
\nabla_\theta J(\theta)=\frac 1m \sum_{i=1}^m \nabla_\theta
L(x^{(i)},y^{(i)},\theta)
\]</span> 这个运算的计算代价是
O(m)。不过可以用小批量梯度下降去做近似，因为期望总是相同的（如果在 i.i.d
的条件下，使用小批量会比单样本的方差小一些。。）</p>
<h1 id="机器学习中存在的一些挑战">机器学习中存在的一些挑战</h1>
<p>这节主要涉及一些 <font color=red>
为何处理高维数据时在新样本上泛化特别困难，以及为何在传统机器
学习中实现泛化的机制不适合学习高维空间中复杂的函数。这些空间经常涉及巨大
的计算代价。深度学习旨在克服这些以及其他一些难题。</font></p>
<h2 id="维数灾难">维数灾难</h2>
<p>由于算法或者是一些条件的限制，当数据量上升时，涉及到的问题的规模使得我们没有办法处理。。</p>
<p>比如旅行者问题，规模是 <span class="math inline">\(O(n!)\)</span>
的，当 n 的数据量很大时，没有较好的办法能快速去解决</p>
<p>亦或是如图所示</p>
<figure>
<img
src="https://szfmsmdx.oss-cn-hangzhou.aliyuncs.com/blog_pic/202405061348858.png"
alt="image-20240506134814736" />
<figcaption aria-hidden="true">image-20240506134814736</figcaption>
</figure>
<p>当数据的相关维度增大时（从左向右），我们感兴趣的配置数目会随之指数级增长。</p>
<h2 id="局部不变性和平滑正则化">局部不变性和平滑正则化</h2>
<p>为了更好地泛化，机器学习算法需要由先验信念引导应该学习什么类型的函数。其中最广泛使用的隐式
‘‘先验’’
是<strong>平滑先验</strong>，或<strong>局部不变性先验</strong>。</p>
<p>大多数机器学习算法默认了“局部不变性”（local constancy
prior），即特征空间的变化对应于结果变化是”很小的“；而且变化往往是”平滑“（smoothness
prior）的，即变化是<strong>可微分</strong>的，即，我们学习到的模型 <span
class="math inline">\(f^*(x)\approx f^*(x+\epsilon)\)</span> ，其中
<span class="math inline">\(\epsilon\)</span> 表示一个微小的变动</p>
<ul>
<li>如果基于以上假设，我们使用m个样本，只能学到m个不同的区间了。</li>
</ul>
<p>换言之，如果我们知道对应输入 x 的答案（例如，x
是个有标签的训练样本），那么 该答案对于 x
的邻域应该也适用。如果在有些邻域中我们有几个好答案，那么我们
可以组合它们（通过某种形式的平均或插值法）以产生一个尽可能和大多数输入一致的答案。</p>
<blockquote>
<p>只要在要学习的真实函数的峰值和谷值处有足够多的样本，那么平滑性假设和相关的无参数学习算法的效果都非常好。当要学习的函数足够平滑，并且只在少数几维变化，这样做一般没问题。在高维空间中，即使是非常平滑的函数，也会在不同维度上有不同的变化方式。如果函数在不同的区间中表现不一样，那么就非常难用一组训练样本去刻画函数。如果函数是复杂的（我们想区分多于训练样本数目的大量区间），有希望很好地泛化么？</p>
<p>这些问题，即是否可以有效地表示复杂的函数以及所估计的函数是否可以很好地泛化到新的输入，答案是有。关键观点是，只要我们通过额外假设生成数据的分布来建立区域间的依赖关系，那么
O(k) 个样本足以描述多如 O(2k ) 的大量区间。</p>
</blockquote>
<h2 id="流形学习">流形学习</h2>
<p><strong>流形（manifold）</strong>指连接在一起的区域。数学上，它是指一组点，且每个点都有其邻域。给定一个任意的点，其流形局部看起来像是欧几里得空间。日常生活中，我们将地球视为二维平面，但实际上它是三维空间中的球状流形。</p>
<p><img src="https://szfmsmdx.oss-cn-hangzhou.aliyuncs.com/blog_pic/202405061421749.png" alt="image-20240506142147640" style="zoom:67%;" /></p>
<p>上图表示：训练数据位于二维空间中的一维流形中。在机器学习中，我们允许流形的维数从一个点到另一个点有所变化。这经常发生于流形和自身相交的情况中。例如，数字
“8’’ 形状的流形在大多数位置只有一维，但在中心的相交处有两维。</p>
<p>如果我们希望机器学习算法学习整个 <span class="math inline">\(\mathbb
R^n\)</span>
上有趣变化的函数，那么很多机器学习看上去是无望的。<strong>流形学习（manifold
learning）</strong>算法通过一个假设来克服这个障碍，该假设认为 <span
class="math inline">\(\mathbb R^n\)</span>
中大部分区域都是无效的输入，有意义的输入只分布在包含少量数据点的子集构成的一组流形中，而学习函数的输出中，有意义的变化都沿着流形的方向或仅发生在我们切换到另一流形时。</p>
]]></content>
      <categories>
        <category>Deep Learning</category>
      </categories>
      <tags>
        <tag>花书</tag>
        <tag>Deep Learning</tag>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title>花书——数值计算</title>
    <url>/2024/04/27/%E8%8A%B1%E4%B9%A6%E2%80%94%E2%80%94%E6%95%B0%E5%80%BC%E8%AE%A1%E7%AE%97/</url>
    <content><![CDATA[<blockquote class="blockquote-center">
<p>研读花书，皆为拙记，如有错误还望各位不惜笔墨，不啬赐教。本节主要包括<strong>数值计算</strong>部分内容</p>

</blockquote>
<span id="more"></span>
<blockquote>
<p>这里的数值计算通常是指通过迭代过程更新解的估计值来解决数学问题的算法，而不是通过解析过程推导出公式来提供正确解的方法</p>
<p>常见的操作包括优化（找到最小化或最大化函数值的参数）和线性方程组的求解</p>
</blockquote>
<h1 id="上溢和下溢">上溢和下溢</h1>
<p>==连续数学在数字计算机上的根本困难是，我们需要通过有限数量的位模式来表示无限多的实数==</p>
<p>这意味着我们在计算机中表示实数时，几乎总会引入一些<strong>近似误差</strong>。</p>
<ul>
<li><strong>下溢（underflow）</strong>：当接近零的数被四舍五入为
零时发生下溢。</li>
<li><strong>上溢（overflow）</strong>：当大量级的数被近似为 <span
class="math inline">\(+\infty\)</span> 或 <span
class="math inline">\(-\infty\)</span> 时发生上溢</li>
</ul>
<p>softmax函数通常用于预测与范畴分布相关联的概率，定义为：<span
class="math inline">\(\mathrm
{softmax}(x)_i=\frac{\exp(x_i)}{\sum_{j=1}^n\exp(x_j)}\)</span></p>
<p>考虑所有 x 都是常数 c 的情况，这使得所有的输出都应该是 1/n：</p>
<ul>
<li>当 c 很大时，计算 exp(c) 会爆发上溢</li>
<li>当 c 是个很小的负数时，使得 exp(c) 发生下溢，使得 exp(c)
几乎为0，那么分母就会无意义</li>
</ul>
<p>但是 softmax 在计算时可以通过 softmax(z) 来解决，其中 <span
class="math inline">\(z=x-\max_i\ x_i\)</span></p>
<p>去除一个最大值使得 z 中的元素最大值为 0，这样
exp(0)=1既保证了分母不会为0，同时也保证了分子不会发生上溢</p>
<h1 id="病态问题">病态问题</h1>
<p>参考<a
href="%5B病态问题及条件数%20%7C%20断鸿声里，立尽斜阳%20(flat2010.github.io)%5D(https://flat2010.github.io/2018/06/30/病态问题及条件数/)">这篇博客</a></p>
<p>基于条件数的定义：定义方程组关于A的条件数为：<span
class="math inline">\(k(A)=||A|| · ||A^{-1}||\)</span></p>
<p>所以假定 <span class="math inline">\(\vec b\)</span> 的变化为 <span
class="math inline">\(\Delta \vec b\)</span> ，对应解的变化为 <span
class="math inline">\(\Delta \vec x\)</span></p>
<p>所以有 <span class="math inline">\(A(x+\Delta x)=b+\Delta b\)</span>
由于方程组 <span class="math inline">\(Ax=b\)</span></p>
<p>所以有 <span class="math inline">\(A·\Delta x=\Delta b\)</span>
，由于条件数假定了 A 的非奇异的，因此存在逆矩阵，所以 <span
class="math inline">\(\Delta x=A^{-1}b\)</span></p>
<p>根据范数的三角不等式有 <span class="math inline">\(||\Delta x || = ||
A^{-1} \Delta b ||\le ||A^{-1}|| · ||b||\)</span></p>
<p>同样地，对原方程做同样的处理有 <span class="math inline">\(||A|| ·
||x|| \ge ||Ax|| = ||b||\)</span></p>
<p>于是能够得到 <span class="math inline">\(\frac {||\Delta x ||}{||A||·
||x|} \le \frac{||A^{-1}|| · ||\Delta b|| }{||b ||}\)</span></p>
<p><img src="https://szfmsmdx.oss-cn-hangzhou.aliyuncs.com/blog_pic/202404281529331.png" alt="image-20240428152927261" style="zoom:50%;" /></p>
<p><img src="https://szfmsmdx.oss-cn-hangzhou.aliyuncs.com/blog_pic/202404281530640.png" alt="image-20240428153016606" style="zoom:50%;" /></p>
<blockquote>
<p>所以条件数本质上是输出相对于输出变化的灵敏度系数</p>
</blockquote>
<h3 id="二范数条件数">二范数条件数</h3>
<p>如果取得是二范数 <span class="math inline">\(||·||_2\)</span>
那么条件数为 <span class="math inline">\(k(A) = \frac
{\sigma_{\max}(A)}{\sigma_{\min}(A)}\)</span>​</p>
<p>其中 <span class="math inline">\(\sigma_{\max}\)</span> 、<span
class="math inline">\(\sigma_{\min}\)</span> 分别是矩阵 A
的最大、最小奇异值</p>
<ul>
<li>当 A 为正规阵时， <span
class="math inline">\(k(A)=\frac{|\lambda_{\max}(A)|}{|\lambda_{\min}(A)|}\)</span>
，这里为最大、最小特征值</li>
<li>当 A 为酉矩阵时，<span class="math inline">\(k(A)=1\)</span></li>
<li>当 A 为奇异阵时，由于A逆不存在所以 <span
class="math inline">\(k(A)\to \infty\)</span></li>
</ul>
<h3 id="机器学习和条件数">机器学习和条件数</h3>
<figure>
<img
src="https://szfmsmdx.oss-cn-hangzhou.aliyuncs.com/blog_pic/202404271622235.png"
alt="image-20240427162132277" />
<figcaption aria-hidden="true">image-20240427162132277</figcaption>
</figure>
<blockquote>
<p>病态的根源是矩阵的列向量相关性过大！</p>
</blockquote>
<h2 id="基于梯度的优化方法">基于梯度的优化方法</h2>
<p>方向导数是函数 <span class="math inline">\(f(x + \alpha u)\)</span>
关于 <span class="math inline">\(\alpha\)</span> 的导数（在 <span
class="math inline">\(\alpha\)</span> 为0时取到），当 <span
class="math inline">\(\alpha=0\)</span> 时有 <span
class="math inline">\(\frac \partial {\partial \alpha}f(x + \alpha
u)=u^T \nabla_x f(x)\)</span></p>
<p>而计算方向导数为了得到使得 f 下降最快的方向 （梯度下降）</p>
<p><img src="https://szfmsmdx.oss-cn-hangzhou.aliyuncs.com/blog_pic/202404281918701.png" alt="image-20240428191800654" style="zoom:50%;" /></p>
<p>关于梯度，是看的是函数增长的速度，二阶导数是对曲率的衡量。假设我
们有一个二次函数（虽然很多实践中的函数都不是二次的，但至少在局部可以很好
地用二次近似）。如果这样的函数具有零二阶导数，那就没有曲率。也就是一条完全
平坦的线，仅用梯度就可以预测它的值。</p>
<p>有时我们需要计算输入和输出都为向量的函数的所有偏导数。包含所有这样的
偏导数的矩阵被称为 Jacobian 矩阵。具体来说，如果我们有一个函数：<span
class="math inline">\(f:\mathbb R^m \to \mathbb R^n\)</span> ，f 的
Jacobian 矩阵 <strong>J</strong><span class="math inline">\(\in \mathbb
R^{n\times m}\)</span> 定义为 <span class="math inline">\(J_{i,j}=\frac
\partial {\partial x_j}f(x)_i\)</span></p>
<p>当我们的函数具有多维输入时，二阶导数也有很多。我们可以将这些导数合并
成一个矩阵，称为 <strong>Hessian 矩阵</strong>。Hessian 矩阵 H(f)(x)
定义为</p>
<p><img src="https://szfmsmdx.oss-cn-hangzhou.aliyuncs.com/blog_pic/202404281919062.png" alt="image-20240428191907022" style="zoom:50%;" /></p>
<p>微分算子在任何二阶偏导连续的点处可交换，也就是它们的顺序可以互换
=&gt; 因为 Hessian
矩阵是实对称的，我们可以将其分解成一组实特征值和一组特征向量的正交基。在特定方向
d 上的二阶导数可以写成 <span class="math inline">\(d^T Hd\)</span> 。当
d 是 H 的一个特征向量时，这个方向的二阶导
数就是对应的特征值。对于其他的方向
d，方向二阶导数是所有特征值的加权平均， 权重在 0 和 1 之间，且与 d
夹角越小的特征向量的权重越大。最大特征值确定最
大二阶导数，最小特征值确定最小二阶导数。</p>
<blockquote>
<p>当作梯度下降时，我们通过泰勒公式去近似看一次下降能表现得多好（新的点x应该是
<span class="math inline">\(x^{(0)-\epsilon g}\)</span> ,g为梯度）</p>
<p>那么有近似</p>
<p><img src="https://szfmsmdx.oss-cn-hangzhou.aliyuncs.com/blog_pic/202404281924928.png" alt="image-20240428192440882" style="zoom:67%;" /></p>
<p>其中有 3
项：函数的原始值、函数斜率导致的预期改善、函数曲率导致的校正</p>
<p>当 最后一项太大时，梯度下降实际上是可能向上移动的。当 g ⊤Hg
为零或负时，近似的泰勒级数表明增加 ϵ 将永远使 f 下降。</p>
<p>当 <span class="math inline">\(g^⊤Hg\)</span> 为正时，通
过计算可得，使近似泰勒级数下降最多的最优步长为</p>
<p><img src="https://szfmsmdx.oss-cn-hangzhou.aliyuncs.com/blog_pic/202404281925071.png" alt="image-20240428192555024" style="zoom: 67%;" /></p>
<p>最坏的情况下，g 与 H 最大特征值 <span
class="math inline">\(λ_{\max}\)</span> 对应的特征向量对齐，则最优步长是
<span class="math inline">\(\frac {1} {λ{_\max}}\)</span> 。</p>
</blockquote>
<p>对于鞍点或局部极值点的讨论：</p>
<ul>
<li>当 Hessian
是正定的（所有特征值都是正的），则该临界点是局部极小点。</li>
<li>当 Hessian
是负定的（所有特征值都是负的），这个点就是局部极大点。</li>
<li>如 果 Hessian 的特征值中至少一个是正的且至少一个是负的，那么 x 是 f
某个横截面 的局部极大点，却是另一个横截面的局部极小点。</li>
</ul>
]]></content>
      <categories>
        <category>Deep Learning</category>
      </categories>
      <tags>
        <tag>花书</tag>
        <tag>Deep Learning</tag>
        <tag>数值计算</tag>
      </tags>
  </entry>
  <entry>
    <title>花书——线代and概率论</title>
    <url>/2024/04/15/%E8%8A%B1%E4%B9%A6%E2%80%94%E2%80%94%E7%BA%BF%E4%BB%A3and%E6%A6%82%E7%8E%87%E8%AE%BA/</url>
    <content><![CDATA[<blockquote class="blockquote-center">
<p>研读花书，皆为拙记，如有错误还望各位不惜笔墨，不啬赐教。本节主要包括<strong>线性代数</strong>与<strong>概率论</strong>部分内容</p>

</blockquote>
<span id="more"></span>
<p>当然，笔者所记不能是花书所有的内容，只能是笔者自己觉得有提升、有意义的部分，所以很多课堂上已经掌握的内容就没有纳入本文的内容了</p>
<h1 id="引言">引言</h1>
<p>一些人工智能项目旨在将世界所包含的知识用形式化的语言进行硬编码
(hard-code)
，而计算机可以使用逻辑推理规则自动理解这些形式化语言中的声明。这就是人工智能的<strong>知识库</strong>
(knowledge base)</p>
<p>依靠硬编码的知识体系面对的困难表明，AI系统需要具备自己获取知识的能力，即从原始数据中提取模式的能力——机器学习
(machine learning) 的能力</p>
<p>对于很多任务来说，我们很难知道应该提取哪些特征，解决这个问题的途径之一就是使用机器学习来发掘表示本身，而不仅仅把表示映射到输出，这种方法称之为<strong>表示学习</strong>
(representation learning)</p>
<blockquote>
<p>一个典型例子是自编码器 (autoencoder)。自编码器由一个编码器 (encoder)
和一个解码器 (decoder)
函数构成。编码器函数将输入数据转为一种不同的表示，而解码器函数则将这个新的表示转换到原来的形式。<br />
而我们的目标是：数据经过编码器和解码器能够尽可能多地保留信息，同时，新的表示有一些好的特性。。。</p>
</blockquote>
<p>设计特征或设计用于学习特征的算法时，我们的目标通常是分离出能够解释观察数据的
<strong>变差因素</strong> (factors of variation) 。</p>
<blockquote>
<p>这些因素可能是现实世界中观察不到的物体或者不可观测的力，但会影响可观测的量<br />
比如分析语音记录时，变差因素包括说话者的年龄、性别等等。。。</p>
</blockquote>
<p>深度学习 (deep learning)
通过其他较为简单的表示来表示复杂表达，解决了表示学习中的核心问题</p>
<blockquote>
<p>如在图片理解这个任务上，计算机是难以理解以像素集合的图像的<br />
直接处理，让机器学习或评估几乎是不可能的<br />
深度学习将复杂的映射分解为一系列嵌套的简单的映射<br />
输入展示在可见层 (visible
layer)，命名的原因是它包含我们能够观察到的变量<br />
然后经过一系列的隐藏层 (hiden layer)</p>
</blockquote>
<p>评估模型深度的方式主要有两种：</p>
<ul>
<li>评估计算流深度</li>
<li>评估概念关联深度
<ul>
<li>比如一个AI系统观察到一只眼睛在阴影中的脸部图象时，他可能看到一只眼睛。但是当其检测到脸部的存在后，系统可能推断第二只眼睛也是存在的。这种情况下，概念图包括两层关系（关于眼睛的层和关于脸的层），但如果我们细化每个概念的估计需要额外的
n 次计算，也就是说计算的图包含 2n 层</li>
</ul></li>
</ul>
<hr />
<h1 id="线性代数">线性代数</h1>
<p>在这里，矩阵被看作是列向量集合，即 <span
class="math inline">\(A=[\alpha_1,\alpha_2,\alpha_3,\cdots,\alpha_n]\)</span>
，而每个列向量被看作是方向的描述<br />
因此 <span class="math inline">\(Ax=\sum_i x_iA_{:,i}\)</span>
可以被描述为我们需要沿着第<span
class="math inline">\(i\)</span>个向量的方向走<span
class="math inline">\(x_i\)</span>的步长</p>
<p>一组向量的 <strong>生成子空间(span)</strong>
是原始向量线性组合后所能抵达的点的集合<br />
也就是说，方程 <span
class="math inline">\(Ax=b\)</span>是否有解，相当于：确定向量<span
class="math inline">\(b\)</span>是否在<span
class="math inline">\(A\)</span>的列向量集合的生成子空间中，这个特殊的生成子空间也被称为
<span class="math inline">\(A\)</span> 的<strong>列空间(column
space)</strong> 或是<span
class="math inline">\(A\)</span>的<strong>值域(range)</strong> 。</p>
<h2 id="讨论方程-axb-是否有解">讨论方程 <span
class="math inline">\(Ax=b\)</span> 是否有解？</h2>
<blockquote>
<p>前提我们记 <span class="math inline">\(A\)</span> 的形状为 <span
class="math inline">\(m \times n\)</span></p>
</blockquote>
<p>那么一方面，若要求该方程对任意的<span
class="math inline">\(b\)</span>都要有解，由于<span
class="math inline">\(Ax\)</span> 看作是<span
class="math inline">\(A\)</span>的第<span
class="math inline">\(i\)</span>个列向量，所以<span
class="math inline">\(Ax\)</span>自然有<span
class="math inline">\(n\)</span>个，故若<span
class="math inline">\(Ax\)</span>的列空间是整个<span
class="math inline">\(\mathbb{R}^m\)</span>要求<span
class="math inline">\(n\ge m\)</span><br />
（当然，这只是必要条件，这些列向量会存在冗余，这种现象称之为
<strong>线性相关</strong> ）</p>
<p><br></p>
<p>另一方面，如果想要该矩阵可逆，需要使得方程对每一个 <span
class="math inline">\(b\)</span> 至多有一个解。</p>
<blockquote>
<p>这一句可能有些同学一开始看会愣一下，笔者在这里卡了一下，作者的意思是
<span class="math inline">\(Ax=b\)</span>，既然<span
class="math inline">\(A\)</span>是广义上可逆的，那么 <span
class="math inline">\(x=A^{-1}b\)</span> ，也就是说，x是 <span
class="math inline">\(A^{-1}\)</span>
的列空间中的一个向量，所以说，x至多也只能有一个，不可能在<span
class="math inline">\(A\)</span>可逆的情况下一个 <span
class="math inline">\(b\)</span> 向量对应多个 <span
class="math inline">\(x\)</span> 向量</p>
</blockquote>
<p>所以该矩阵需要确保至多有 <span class="math inline">\(m\)</span>
个列，当然，这是个充分条件<br />
综上所述，如果方程有唯一解那么 <span class="math inline">\(A\)</span>
矩阵首先要是方阵，其次他所有的列向量都要是无关的</p>
<h2 id="范数">范数</h2>
<p>在机器学习中，我们通过称为 <strong>范数</strong>
(norm)的函数来衡量向量的大小，形式上 <span
class="math inline">\(L^p\)</span>范数的定义为： <span
class="math display">\[ | | x | | _ { p } = \left( \sum _ { i } | x _ {
i } | ^ { p } \right) ^ { \frac { 1 } { p } }\]</span><br />
范数能够将向量映射为非负值<br />
直观上来看，向量<span
class="math inline">\(x\)</span>的范数衡量了从原点到点<span
class="math inline">\(x\)</span>的距离，更严格地说，范数满足如下性质：</p>
<ul>
<li><span class="math inline">\(f(x)=0 \rightarrow x=\vec
0\)</span></li>
<li><span class="math inline">\(f(x+y) \le f(x) + f(y)\)</span></li>
<li><span class="math inline">\(\forall \alpha \in R , f (\alpha x) =
|{\alpha}|f(x)\)</span></li>
</ul>
<p>举几个例子吧！</p>
<ul>
<li>当 <span class="math inline">\(p = 2\)</span> 时，<span
class="math inline">\(L^2\)</span> 范数称为
<strong>欧几里得范数</strong>(Euclidean norm)：表示从原点出发到向量
<span class="math inline">\(x\)</span>
确定的点的欧几里得距离。通常情况也用 <span
class="math inline">\(L^2\)</span> 范数的平方去代替 <span
class="math inline">\(L^2\)</span>
范数，不过也有利有弊，一方面，平方在原点附近的增长非常缓慢，另一方面，平方的导数只取决于对应的元素</li>
<li>当 <span class="math inline">\(p = 1\)</span> 时， <span
class="math inline">\(L^1\)</span>
范数却能够给我们区分零元素和非零元素， <span
class="math inline">\(L^1\)</span>
范数的计算也非常简单，即各位的绝对值和</li>
<li>当 <span class="math inline">\(p = 0\)</span> 时，这种 <span
class="math inline">\(L^0\)</span>
范数用来统计向量中非零元素的个数，有时我们会用他来衡量一个向量的大小，但这个概念对于前面数学上的严格定义是不对的，首先第三条就不满足了。。。</li>
<li>当 <span class="math inline">\(p = +\infty\)</span>
时，也被称为<strong>最大范数</strong>(max
norm)，这个范数表示向量中具有最大幅值的元素的绝对值：<span
class="math inline">\(||x||_\infty=\max_i |x_i|\)</span></li>
<li>当然，在机器学习中，我们有时也希望有类似的定义能够去衡量矩阵的大小，这要引入
<strong>Frobenius范数</strong>(Frobenius norm)： <span
class="math inline">\(||A||_F=\sqrt{\sum_{i,j}A^2_{i,j}}\)</span>
（类似于向量的 L2 范数？）</li>
</ul>
<h2 id="特征分解">特征分解</h2>
<blockquote>
<p>许多数学对象可以通过将它们分解成多个组成部分或者找到它们的一些属性而更好地理解，这些属性是通用的，而不是由我们选择表示它们的方式产生的。<br />
例如：$12=2×3×3 $，从中我们可以获得一些信息，比如12不能被5整除</p>
</blockquote>
<p>通过矩阵的<strong>特征分解</strong>(eigen
decomposition)我们同样能够获得一些信息，方阵 A 的 特征向量（eigen
vector）是指与 A 相乘后相当于对该向量进行缩放的非零向量 v：<span
class="math inline">\(Av=\lambda v\)</span><br />
其中 <span class="math inline">\(\lambda\)</span>
称为这个特征向量对应的特征值(eigen value)。（类似地，我们也可以定义
左特征向量（left eigen vector）<span class="math inline">\(v^⊤A =
λv^⊤\)</span>，但是通常我们更关注 右特征向量（right eigen
vector））。显然，对特征向量缩放后的向量依然是矩阵的特征向量。<br />
假设矩阵 A 有 n 个线性无关的特征向量 <span
class="math inline">\(\{v^{(1)},\cdots,v^{(n)}\}\)</span> 对应着特征值
<span
class="math inline">\(\{\lambda^{(1)},\cdots,\lambda^{(n)}\}\)</span> ,
因此 A 的特征分解可以记作： <span class="math inline">\(A=V\rm
diag(\lambda)V^{-1}\)</span><br />
不过值得注意的是，并不是所有实矩阵都总有实特征分解，有些特征分解存在但是涉及到复数，不过值得注意的是，所有的<strong><em>实对称阵</em></strong>都可以分解成实特征向量和实特征值：<span
class="math inline">\(A=Q \Lambda Q^T\)</span>，其中 <span
class="math inline">\(Q\)</span> 是 <span
class="math inline">\(A\)</span> 的特征向量组成的正交矩阵，<span
class="math inline">\(\Lambda\)</span> 是对角矩阵。<br />
<img
src="https://szfmsmdx.oss-cn-hangzhou.aliyuncs.com/blog_pic/202404251812391.png"
alt="image-20240425181235305" /></p>
<h2 id="奇异值分解">奇异值分解</h2>
<p>对于非方阵矩阵，我们同样希望能够进行分解，换句话说我们希望将特征分解的范围拓展<br />
这种分解方法被称为 <strong>奇异值分解</strong>（singular value
decomposition, SVD），将矩阵分解为 奇异向量（singular vector）和
<strong>奇异值</strong>（singular value）<br />
同样地，奇异值分解为：<span class="math inline">\(A=UDV^T\)</span><br />
假设 A 是一个 <span class="math inline">\(m\times
n\)</span>的矩阵,那么U是一个 <span class="math inline">\(m\times
m\)</span>的矩阵,D是一个<span class="math inline">\(m\times
n\)</span>的矩阵,V是一个 <span class="math inline">\(n \times n\)</span>
的矩阵<br />
矩阵 U 和 V 都定义为 <strong>正交矩阵</strong>，而矩阵 D
定义为<strong>对角矩阵</strong>。注意，矩阵 D 不一定是方阵。<br />
对角矩阵 D 对角线上的元素被称为矩阵 A 的
<strong>奇异值</strong>（singular value）。矩阵U 的列向量被称为
<strong>左奇异向量</strong>（left singular vector），矩阵 V 的列向量被称
<strong>右奇异向量</strong>（right singular vector）。<br />
事实上，我们可以用与 A 相关的特征分解去解释 A 的奇异值分解。A 的
左奇异向量（left singular vector）是 <span
class="math inline">\(AA^⊤\)</span> 的特征向量。A 的 右奇异向量（right
singular vector）是 <span class="math inline">\(A^⊤A\)</span>
的特征向量。A 的非零奇异值是 <span class="math inline">\(A^⊤A\)</span>
特征值的平方根，同时也是<span class="math inline">\(AA^⊤\)</span>
特征值的平方根。<br />
奇异值分解的推导如下：</p>
<p><img src="https://szfmsmdx.oss-cn-hangzhou.aliyuncs.com/blog_pic/202404261123476.jpg" alt="53ad37ea04a14b4601663211b5ac23d" style="zoom: 50%;" /></p>
<blockquote>
<p><a
href="https://www.bilibili.com/video/BV1544y1v7Am/?spm_id_from=333.337.search-card.all.click">参考视频</a></p>
</blockquote>
<h2 id="mp伪逆">MP伪逆</h2>
<p>对于非方阵而言，其逆运算没有定义，因此我们也需要对逆的概念进行拓展<br />
如果矩阵 A 的行数大于列数，那么上述方程可能没有解。如果矩阵 A
的行数小于列数，那么上述矩阵可能有多个解<br />
Moore-Penrose 伪逆（Moore-Penrose
pseudoinverse）使我们在这类问题上取得了一定的进展。矩阵 A
的伪逆定义为：$ A ^ { + } = _ { } ( A ^ { T } A + I ) ^ { - 1 } A ^ { T
}$</p>
<p>而计算伪逆通常使用奇异值分解：$ A ^ { + } = V D ^ { + } U ^ { T }
$<br />
其中，矩阵 U，D 和 V 是矩阵 A奇异值分解后得到的矩阵。对角矩阵 D
的伪逆<span class="math inline">\(D^+\)</span>
是其非零元素<strong>取倒数之后再转置</strong>得到的。<br />
MP伪逆在方程解的问题中也有很多应用，比如：</p>
<ul>
<li>当 <span class="math inline">\(n &gt; m\)</span> 时，<span
class="math inline">\(x = A^+ y\)</span>是所有方程可行解中<span
class="math inline">\(L_2\)</span>范数最小的那个</li>
<li>当 <span class="math inline">\(n &gt; m\)</span>​
时，MP伪逆得到的解使得 <span class="math inline">\(Ax\)</span>​ 与 <span
class="math inline">\(y\)</span>​​ 的欧几里得距离最小</li>
</ul>
<h2 id="迹运算">迹运算</h2>
<p>迹运算主要集中于以下几点</p>
<ul>
<li><span class="math inline">\(Tr(A) = \sum_i A_{i,i}\)</span></li>
<li><span class="math inline">\(Tr(A) = Tr(A^T)\)</span></li>
<li><span class="math inline">\(||A||_F=\sqrt{Tr(AA^T)}\)</span></li>
<li>$ Tr ( {_ { i = 1 } ^ { n }} {F ^ { ( i ) }} ) = Tr ( F ^ { ( n ) }
{_ { i = 1 } ^ { n - 1 }} {F ^ { ( i ) }} ) $​
(多个矩阵相乘得到的方阵的迹，和将这些矩阵中的最后一个挪到最前面之后相乘的迹是相同的)</li>
</ul>
<h2 id="行列式">行列式</h2>
<p>行列式，记作 det(A)，是一个将方阵 A
映射到实数的函数。行列式等于矩阵特征值的乘积。行列式的绝对值可以用来衡量矩阵参与矩阵乘法后空间扩大或者缩小了多少。如果行列式是
0，那么空间至少沿着某一维完全收缩了，使其失去了所有的体积。如果行列式是
1，那么这个转换保持空间体积不变。</p>
<h2 id="主成分分析">主成分分析</h2>
<p>主成分分析（principal components analysis, PCA）</p>
<hr />
<p>首先，假设我们有 <strong>m 个 n 维的样本 <span
class="math inline">\(\{x_1,x_2, \cdots,x_m\},x_i\in
R^n\)</span></strong> ，我们希望使用某种压缩，对于每个 <span
class="math inline">\(x_i\)</span> 找到对应的 <strong><span
class="math inline">\(c_i \in R^l\)</span> ，一般的 $l &lt; n $</strong>
，进行压缩</p>
<ul>
<li>对于编码器 <span class="math inline">\(f(x) = c\)</span></li>
<li>对于解码器 <span class="math inline">\(x \approx g
(c)=g(f(x))\)</span></li>
</ul>
<p>为了简化解码函数的选择，我们可以通过矩阵乘法将其映射回 n 维空间，即
<span class="math inline">\(g (c) = Dc\)</span></p>
<p>注意到，当 c
等比例扩大缩小时，D同样等比例缩小扩大，可以得到无数个结果，因此我们限制
D 中的所有列向量都有单位范数</p>
<p>同样，为了让编码问题更简单，PCA限制 D
的列向量彼此正交（D不是严格意义上的正交阵）</p>
<p>我们如何去找到这个 c
，使得他能够被精确解码？这里给出的方法是最小化原始向量 <span
class="math inline">\(x\)</span> 与重构向量 <span
class="math inline">\(g(c^*)\)</span> 的距离，即 <span
class="math inline">\(c^*={\rm arg\ min}_c ||x-g(c)||_2\)</span>
，由于二范数非负性，所以我们可以通过平方进行优化</p>
<p><img src="https://szfmsmdx.oss-cn-hangzhou.aliyuncs.com/blog_pic/202404261520572.png" alt="image-20240426152011469" style="zoom:33%;" /></p>
<p>由于第一项并不依赖于 c ，所以可以再度简化为 <span
class="math inline">\(c^*={\rm arg\ min}_c -2 x^T g (c)+g (c)^T g
(c)\)</span></p>
<p>带入 <span class="math inline">\(g (c)\)</span> 的定于，以及 <span
class="math inline">\(D\)</span>
矩阵的列向量互相正交的一些约定，我们有</p>
<p><span class="math display">\[c^* = {\rm arg\ min}_c - 2 x ^ { T } D c
+ c ^ { T } c\]</span></p>
<p>通过向量微积分，能够推导得到结论：</p>
<p><img src="https://szfmsmdx.oss-cn-hangzhou.aliyuncs.com/blog_pic/202404261229634.png" alt="image-20240426122900565" style="zoom:33%;" /></p>
<p>所以推导得到编码函数也就能够写成 <span
class="math inline">\(f(x)=D^Tx\)</span></p>
<p>那么，另一方面该如何选取投影矩阵 D ？</p>
<p>当然这个 D
是作用在全体样本上，与刚刚的思路类似，我们需要使得误差矩阵的
<strong>Frobenius 范数</strong>最小，也就是 <span
class="math inline">\(D^*={\rm arg\ min}_D
\sqrt{\sum_{i,j}(x_j^{(i)}-r(x^{(i)})_j)^2}{\rm \ subject\ to\
}D^TD=I_l\)</span> 其中 <span
class="math inline">\(r(x)=g(f(x))=DD^Tx\)</span></p>
<p>首先考虑一维的情况，问题简化为：</p>
<p><span class="math display">\[d^*={\rm arg\ min}_d
\sqrt{\sum_{i}(x^{(i)}-d{d^T}{x^{(i)}})^2}{\rm \ subject\ to\
}||d||_2=1\]</span></p>
<p>由于 <span class="math inline">\(d^T x\)</span>
是标量，可以重排位置，参考花书</p>
<p><img src="https://szfmsmdx.oss-cn-hangzhou.aliyuncs.com/blog_pic/202404261243605.png" alt="image-20240426124335517" style="zoom: 50%;" /></p>
<p>这里求和号实际上不方便理解，因此我们写成矩阵的形式，重新表述为<span
class="math display">\[d^*={\rm arg\ min}_d ||X-Xd{d^T}||_F^2\ {\rm
subject\ to\ }d^Td=1\]</span></p>
<p><img src="https://szfmsmdx.oss-cn-hangzhou.aliyuncs.com/blog_pic/202404261248438.png" alt="image-20240426124828340" style="zoom:67%;" /></p>
<p><img src="https://szfmsmdx.oss-cn-hangzhou.aliyuncs.com/blog_pic/202404261253697.png" alt="image-20240426125338607" style="zoom:50%;" /></p>
<p>这里将 <span class="math inline">\(X^TX\)</span> 处理为对称阵，那么
<span class="math inline">\(y^TX^TXy\)</span>
就是一个二次型，那么显然他的最大值应该是最大特征值，对应y向量为对应特征值的特征向量。。。</p>
<h1 id="概率论">概率论</h1>
<p>不确定信息的来源：</p>
<ul>
<li>被建模系统内在的随机性。这个好理解，比如打扑克，洗牌当然存在一定的随机性</li>
<li>不完全观测。这个也好理解，我们没有办法总能够观察到所有驱动系统行为的变量。</li>
<li>不完全建模。当我们使用一些必须舍弃某些观测信息的模型时，舍弃的信息会导致模型的预测出现不确定性。</li>
</ul>
<p>在引入贝叶斯概率后，概率呈现出两种意义：</p>
<ul>
<li>直接与事件发生的概率相联系——<strong>频率派概率(frequentist
probability)</strong></li>
<li>涉及到确定性水平——<strong>贝叶斯概率(Bayesian
probability)</strong></li>
</ul>
<h2 id="概率分布">概率分布</h2>
<h3 id="离散型随机变量">离散型随机变量</h3>
<p>离散型变量的概率分布可以用 <strong>概率质量函数(probability mass
function, PMF)</strong> 来描述，通常用大写字母 P
来表示PMF。PMF可以同时作用于多个随机变量，这多个变量的概率分布被称为<strong>联合概率分布(joint
probability
distribution)</strong>，如果一个函数是随机变量x的PMF，必须要满足一些条件：</p>
<ul>
<li>P定义域为所有x的状态集合</li>
<li><span class="math inline">\(\forall x\in {\rm x},0\le P(x)\le
1\)</span></li>
<li><span class="math inline">\(\sum_{x\in {\rm x}}P(x)=1\)</span>​
，这条性质称为归一化的(normalized)</li>
</ul>
<h3 id="连续性随机变量">连续性随机变量</h3>
<p>同理，这里的PMF被描述为概率密度函数<strong>(probability density
function，PDF)</strong> ，同样的，他需要满足几个条件：</p>
<ul>
<li>p定义域是所有x可能的状态的集合</li>
<li><span class="math inline">\(\forall x\in {\rm x},p(x)\ge 0\)</span>
（！这里不要求 <span class="math inline">\(p(x)\le 1\)</span>
因为一个点处的密度哪怕是非常大，也不一定能够影响总体的概率</li>
<li><span class="math inline">\(\int p(x) \mathrm dx=1\)</span></li>
</ul>
<p>概率密度函数 <span class="math inline">\(p(x)\)</span>
并没有直接对特定的状态给出概率，相对的，它给出了落在面积为 <span
class="math inline">\(δx\)</span> 的无限小的区域内的概率为 <span
class="math inline">\(p(x)δx\)</span>。</p>
<h2 id="边缘概率">边缘概率</h2>
<p>若我们知道一组变量的联合概率分布，但只需要了解其中一个子集的分布，那么我们可以如下计算：</p>
<ul>
<li>离散型</li>
</ul>
<p><img src="https://szfmsmdx.oss-cn-hangzhou.aliyuncs.com/blog_pic/202404261538617.png" alt="image-20240426153859533" style="zoom: 50%;" /></p>
<ul>
<li>连续型</li>
</ul>
<p><img src="https://szfmsmdx.oss-cn-hangzhou.aliyuncs.com/blog_pic/202404261539368.png" alt="image-20240426153922287" style="zoom:50%;" /></p>
<p>​</p>
<h2 id="条件概率">条件概率</h2>
<p>条件概率可以通过公式 <span class="math inline">\(P(\mathrm
y=y|\mathrm x=x)=\frac {P(\mathrm y=y,\mathrm x=x)}{P(\mathrm
x=x)}\)</span></p>
<blockquote>
<p>这里需要注意的是，不要把条件概率和计算当采用某个动作后会发生什么相混淆。假定某个人说德语，那么他是德国人的条件概率是非常高的，但是如果随机选择的一个人会说德语，他的国籍不会因此而改变。计算一个行动的后果被称为
<strong>干预查询（intervention query）</strong>。干预查询属于
<strong>因果模型（causal modeling）</strong>的范畴</p>
</blockquote>
<p>参考了网络上的一些解释，笔者的浅显理解是，调剂概率强调状态而干预查询强调改变</p>
<p>例如<span class="math inline">\(P(销售量|汉堡的价格为20元)\)</span>
与 <span class="math inline">\(P(销售量|降价至20元)\)</span>
，前者是条件概率而后者则是干预查询</p>
<h2 id="独立与条件独立">独立与条件独立</h2>
<p>对于两个随机变量x和y，广义的独立是他们的某种概率可以拆分成互不相关的两个因子的乘积形式，即认为他们是独立的</p>
<ul>
<li>独立：<span class="math inline">\(\forall x\in \mathrm x,y\in
\mathrm y,p(\mathrm x=x,\mathrm y=y)=p(\mathrm x=x)p(\mathrm
y=y)\)</span></li>
<li>条件独立：<span class="math inline">\(\forall x\in \mathrm x,y\in
\mathrm x,z\in \mathrm z,p(\mathrm x=x,\mathrm y=y|\mathrm
z=z)=p(\mathrm x=x|\mathrm z=z)p(\mathrm y=y|\mathrm z=z)\)</span></li>
</ul>
<p>可以使用一种较为简单的记号去描述他们的独立性</p>
<ul>
<li>x和y相互独立：<span class="math inline">\(x⊥y\)</span></li>
<li>x和y在给定条件为z时条件独立：<span
class="math inline">\(x⊥y|z\)</span></li>
</ul>
<h2 id="常用概率分布">常用概率分布</h2>
<h3 id="bernoulli-分布">Bernoulli 分布</h3>
<p>伯努利分布是单个二值随机变量的分布。他由单个参数 <span
class="math inline">\(\phi\in [0,1]\)</span> 控制，他具有一些性质：</p>
<p><img src="https://szfmsmdx.oss-cn-hangzhou.aliyuncs.com/blog_pic/202404261625113.png" alt="image-20240426162520022" style="zoom:50%;" /></p>
<h3 id="multinoulli-分布">Multinoulli 分布</h3>
<p><strong>Multinoulli 分布（multinoulli distribution） </strong> 或者
<strong>范畴分布（categorical distribution）</strong> 是指在具有 k
个不同状态的单个离散型随机变量上的分布，其中 k 是一个有限值</p>
<p>Multinoulli分布由向量 <span class="math inline">\(p\in
[0,1]^{k-1}\)</span> 参数化，其中每一个分量 <span
class="math inline">\(p_i\)</span> 表示第 i 个状态的概率，第 k
个状态可以通过 <span class="math inline">\(1-1^Tp\)</span>
给出，因此需要加以限制 <span class="math inline">\(1^Tp\le
1\)</span></p>
<h3 id="高斯分布">高斯分布</h3>
<p>实数上最常用的分布就是<strong>正态分布（normal
distribution），也称为高斯分布 （Gaussian distribution）</strong>：</p>
<p><img src="https://szfmsmdx.oss-cn-hangzhou.aliyuncs.com/blog_pic/202404261635773.png" alt="image-20240426163530679" style="zoom:50%;" /></p>
<p><img src="https://szfmsmdx.oss-cn-hangzhou.aliyuncs.com/blog_pic/202404261637708.png" style="zoom: 50%;" /></p>
<p>正态分布由两个参数控制，<span class="math inline">\(\mu \in
R\)</span> 和 <span class="math inline">\(\sigma\in (0,\infty)\)</span>
。<span class="math inline">\(\mu\)</span>
给出了参数的均值，也是分布的均值，标准差用 <span
class="math inline">\(\sigma\)</span> 控制，方差为 <span
class="math inline">\(\sigma^2\)</span></p>
<p>通常情况下，为了控制正态分布，不会选择直接控制 <span
class="math inline">\(\sigma\)</span> 而是控制 <span
class="math inline">\(\beta = \sigma^{-1}\)</span></p>
<p><img src="https://szfmsmdx.oss-cn-hangzhou.aliyuncs.com/blog_pic/202404261641779.png" alt="image-20240426164110685" style="zoom: 50%;" /></p>
<p>一般地，当缺乏对某个实数上分布的先验知识而不知道怎么选择分布时，正态分布是默认比较好的选择，有两个原因：</p>
<ul>
<li>一方面，根据<strong>中心极限定理</strong>，很多独立随机变量的和近似服从正态分布。故实际上一些复杂的系统可以被建模成正态分布的噪声</li>
<li>另一方面，在具有相同方差的所有可能的概率分布中，正态分布在实数上具有<strong>最大的不确定性</strong>。</li>
</ul>
<p>同样的，将正态分布推广到高维形式，其参数推广为矩阵化：</p>
<p><img src="https://szfmsmdx.oss-cn-hangzhou.aliyuncs.com/blog_pic/202404261647279.png" alt="image-20240426164738186" style="zoom:50%;" /></p>
<p><img src="https://szfmsmdx.oss-cn-hangzhou.aliyuncs.com/blog_pic/202404261647004.png" alt="image-20240426164752911" style="zoom:50%;" /></p>
<p>同样我们通过精度阵对概率密度函数进行控制</p>
<h3 id="指数分布与laplace分布">指数分布与Laplace分布</h3>
<p>在深度学习中，我们经常会需要一个在 x = 0 点处取得边界点 (sharp point)
的分布。为了实现这一目的，我们可以使用 <strong>指数分布（exponential
distribution）</strong>：</p>
<p><img src="https://szfmsmdx.oss-cn-hangzhou.aliyuncs.com/blog_pic/202404261649599.png" alt="image-20240426164952506" style="zoom: 50%;" /></p>
<p>指数分布使用<strong>指示函数(indicator function) <span
class="math inline">\(1_{x\ge 0}\)</span></strong>
来使得x为负时的值为0</p>
<p>一个与指数分布类似的函数是拉普拉斯分布，它能够控制在任意一点<span
class="math inline">\(\mu\)</span> 处设置概率质量的峰值：</p>
<p><img src="https://szfmsmdx.oss-cn-hangzhou.aliyuncs.com/blog_pic/202404261651323.png" alt="image-20240426165159227" style="zoom:50%;" /></p>
<h3 id="dirac分布和经验分布">Dirac分布和经验分布</h3>
<p>既然拉普拉斯分布可以控制峰值，那么有没有一种分布可以控制将所有的质量集中在一个点处呢？答案是有的，可以通过
<strong>Dirac delta函数（Dirac delta
function，也称单位脉冲函数）</strong> <span
class="math inline">\(\delta(x)\)</span> 定义概率密度函数来实现：<span
class="math inline">\(p(x)=\delta(x-\mu)\)</span></p>
<p>Dirac
delta函数被定义为在除了0以外的所有点的值都为0，但是积分为1，也就是说，他在<span
class="math inline">\(\mu\)</span>​ 处无限窄也无限高</p>
<p><img src="https://szfmsmdx.oss-cn-hangzhou.aliyuncs.com/blog_pic/202404261710997.png" alt="image-20240426171020870" style="zoom:50%;" /></p>
<p><strong>说明</strong>:</p>
<ul>
<li>严格来说狄拉克δ函数不能算是一个函数，而是一种<strong>数学对象</strong>,
因为满足以上条件的函数是不存在的, 但是我们可以用分布的概念来解释,
因此称为<strong>狄拉克分布</strong>或者<strong>δ分布</strong></li>
<li>它是一种极简单的<strong>广义函数</strong>. 广义函数是一种数学对象,
依据积分性质而定义. 我们可以把狄拉克δ函数想成一系列函数的极限点,
这一系列函数把除0以外的所有点的概率密度越变越小</li>
</ul>
<p>Dirac狄拉克分布经常作为<strong>经验分布（empirical
distribution）</strong>的一个组成部分出现：</p>
<p><img src="https://szfmsmdx.oss-cn-hangzhou.aliyuncs.com/blog_pic/202404261706821.png" alt="image-20240426170656725" style="zoom:50%;" /></p>
<p>经验分布将概率密度<span class="math inline">\(\frac 1m\)</span>​
赋给m个点，这些点是给定的数据集或者采样的集合。只有在定义连续型随机变量的经验分布时，Dirac
delta
函数才是必要的。对于离散型随机变量，情况更加简单：经验分布可以被定义成一个
Multinoulli
分布，对于每一个可能的输入，其概率可以简单地设为在训练集上那个输入值的
<strong>经验频率（empirical frequency）</strong>。</p>
<h3 id="分布混合">分布混合</h3>
<p>通过组合一些简单的概率分布来定义新的概率分布也是很常见的。一种通用的组合方法是构造
<strong>混合分布（mixture distribution）</strong>。混合分布由一些组件
(component) 分布构成。</p>
<p>简单来说就是根据范畴分布将一个概率密度集合进行抽样并组合</p>
<p>可以看 <a
href="%5B混合分布(mixture%20distribution)-CSDN博客%5D(https://blog.csdn.net/tanghonghanhaoli/article/details/90543917)">这篇博客</a></p>
<h2 id="常用函数的有用性质">常用函数的有用性质</h2>
<p>logistic sigmoid函数作为深度学习中的经典函数：<span
class="math inline">\(\sigma(x)=\frac 1{1+\exp(-x)}\)</span></p>
<p>logistic sigmoid 函数通常用来产生 Bernoulli 分布中的参数
ϕ，因为它的范围是(0, 1)，处在 ϕ 的有效取值范围内。</p>
<p><img src="https://szfmsmdx.oss-cn-hangzhou.aliyuncs.com/blog_pic/202404261724580.png" alt="image-20240426172450473" style="zoom:50%;" /></p>
<p>sigmoid 函数 在变量取绝对值非常大的正值或负值时会出现
<strong>饱和（saturate）</strong>现象，意味着函数会
变得很平，并且对输入的微小改变会变得不敏感。</p>
<p>另一个常见函数是 <strong>softplus函数</strong>：<span
class="math inline">\(\zeta(x)=\log(1+\exp(x))\)</span> ,softplus
函数可以用来产生正态分布的 β 和 σ 参数，因为它的范围是 <span
class="math inline">\((0,\infty)\)</span></p>
<p><img src="https://szfmsmdx.oss-cn-hangzhou.aliyuncs.com/blog_pic/202404261727966.png" alt="image-20240426172710851" style="zoom:50%;" /></p>
<blockquote>
<p>一些有用的性质。。。</p>
<p><img src="https://szfmsmdx.oss-cn-hangzhou.aliyuncs.com/blog_pic/202404261727209.png" alt="image-20240426172733099" style="zoom:67%;" /></p>
</blockquote>
<p>函数 <span class="math inline">\(\sigma^{-1}(x)\)</span>
在统计学中被称为 <strong>分对数（logit）</strong></p>
<h2 id="贝叶斯规则">贝叶斯规则</h2>
<p>通过贝叶斯规则，我们能够在已知 <span class="math inline">\(P(\mathrm
y|\mathrm x)\)</span> 时计算 <span class="math inline">\(P(\mathrm x |
\mathrm y)\)</span></p>
<p><span class="math inline">\(P(\mathrm x | \mathrm y)=\frac{P(\mathrm
x) P(\mathrm y | \mathrm x)}{P(\mathrm y)}\)</span></p>
<p>通常情况下，通过 [全概率公式](<a
href="https://baike.baidu.com/item/全概率公式/9980676#:~:text=若事件A1，A2，…构成一个%20完备事件组%20且都有正概率，则对任意一个事件B，有如下公式成立：%20P%20(B)%3DP%20(BA1)%2BP%20(BA2)%2B...%2BP%20(BAn)%3DP,(B%7CA2)P%20(A2)%20%2B...%20%2B%20P%20(B%7CAn)P%20(An).%20此公式即为全概率公式。">全概率公式_百度百科
(baidu.com)</a> 计算 <span class="math inline">\(P(y)\)</span>
就不需要实现知道 y 的信息了</p>
<h2
id="关于连续性随机变量的一些技术问题">关于连续性随机变量的一些技术问题</h2>
<p>涉及到处理那种相互之间有确定性函数关系
的连续型变量。假设我们有两个随机变量 x 和 y 满足 y = g(x)，其中 g
是可逆的、 连续可微的函数。</p>
<blockquote>
<p>可能有人会想 <span class="math inline">\(p_y(y) =
p_x(g^{−1}(y))\)</span>。但实际上这并不对。</p>
</blockquote>
<p>假设我们有两个标量值随机变量 x 和 y，并且满足 y = x/2 以及 x ∼ U(0,
1)。如果我们使用 <span class="math inline">\(p_y(y) =
p_x(2y)\)</span>，那么 <span class="math inline">\(p_y\)</span> 除了区间
[0, 1/2 ] 以外都为 0，并且在这个区间上的值为 1。这意味着<span
class="math inline">\(\int p_y(y)dy=\frac12\)</span>​</p>
<p>而这违背了概率密度的定义 (积分为
1)。这个常见错误之所以错是因为它没有考虑 到引入函数 g
后造成的空间变形。</p>
<p>我们需要让他在无穷小区域内的概率相同，也就是 <span
class="math inline">\(p(x)\delta x\)</span>相同</p>
<p><img src="https://szfmsmdx.oss-cn-hangzhou.aliyuncs.com/blog_pic/202404271353082.png" alt="image-20240427135350894" style="zoom:50%;" /></p>
<h2 id="信息论">信息论</h2>
<p>在机器学习中，我们也可以把信息论应用于连续型变量，
此时某些消息长度的解释不再适用。</p>
<p>信息论的基本想法是一个不太可能的事件居然发生了，要比一个非常可能的事
件发生，能提供更多的信息。</p>
<p>我们想要通过这种基本想法来量化信息。特别地，</p>
<ul>
<li>非常可能发生的事件信息量要比较少，并且极端情况下，确保能够发生的事件
应该没有信息量。</li>
<li>较不可能发生的事件具有更高的信息量。</li>
<li>独立事件应具有增量的信息。例如，投掷的硬币两次正面朝上传递的信息量，
应该是投掷一次硬币正面朝上的信息量的两倍。</li>
</ul>
<p>为了满足上述三个性质，我们定义一个事件 x = x 的
<strong>自信息（self-information）</strong> 为 <span
class="math display">\[ I(x) = -\log P(x)\]</span></p>
<p>其中 <span class="math inline">\(\log\)</span> 就是以自然对数 e
为底的 ln 函数</p>
<p>我们定义的 I(x) 单 位是 奈特（nats）。一奈特是以 1/e
的概率观测到一个事件时获得的信息量。</p>
<p>亦有教材定义使用底数为 2 的对数，单位是
<strong>比特（bit）</strong>或者
香农（shannons）；通过比特度量的信息只是通过奈特度量信息的常数倍。</p>
<p>我们可以用 <strong>香农熵（Shannon entropy）</strong>来对整个概
率分布中的不确定性总量进行量化：</p>
<p><span class="math inline">\(H(x)=\mathbb E_{x\sim P}[I(x)]=-\mathbb
E_{x\sim P}[\log P(x)]\)</span> ，也记作 H(P)。</p>
<p>换言之，一个分布的香农熵是指
==遵循这个分布的事件所产生的期望信息总量== 。</p>
<p>当 x 是连续的，香农熵被称为 <strong>微分熵（differential
entropy）</strong></p>
<p><img src="https://szfmsmdx.oss-cn-hangzhou.aliyuncs.com/blog_pic/202404271413992.png" alt="image-20240427141352865" style="zoom: 67%;" /></p>
<blockquote>
<p><strong>二值随机变量</strong>的香农熵。该图说明了更接近确定性的分布是如何具有较低的香农熵，而更
接近均匀分布的分布是如何具有较高的香农熵。水平轴是
p，表示二值随机变量等于 1 的概率。熵由 <span class="math inline">\((p −
1)\log(1 − p) − p \log p\)</span> 给出。当 p 接近 0
时，分布几乎是确定的，因为随机变量几乎总是 0。当 p 接近 1
时，分布也几乎是确定的，因为随机变量几乎总是 1。当 p = 0.5
时，熵是最大的， 因为分布在两个结果（0 和 1）上是均匀的。</p>
<p>推导：</p>
<p>二值分布记其随机变量为 x，那么 <span class="math inline">\(p(\mathrm
x=x)={p^x} (1-p)^{1-x}\)</span></p>
<p>所以 <span class="math inline">\(H(x)=E(-\log ({p^x}
{(1-p)^{1-x}}))=E(-x\log p+(x-1)\log (1-p))\)</span></p>
<p>由于 <span class="math inline">\(E(x)=p\)</span> ，于是有 <span
class="math inline">\(H(x)=(p-1)\log (1-p)-p\log p\)</span></p>
</blockquote>
<p>对于同一个随机变量 x 有两个单独的概率分布 <span
class="math inline">\(P(x)\)</span> 和 <span
class="math inline">\(Q(x)\)</span>​ ，使用 <strong>KL
散度（Kullback-Leibler (KL) divergence）</strong>
来衡量这两个分布的差异：</p>
<p><span class="math display">\[D_{KL}(P\ ||\ Q)=\mathbb E_{x\sim
P}[\log \frac{P(x)}{Q(x)}]=\mathbb E_{x\sim P}[\log P(x)-\log
Q(x)]\]</span></p>
<p>KL散度衡量的是，当我们使用一种编码，这种编码能够使得概率分布Q产生的消息程度最短，在使用这种编码发送一种由概率分布P产生的消息时，所产生的额外信息</p>
<p>KL 散度有很多有用的性质，最重要的是它是非负的。KL 散度为 0 当且仅当 P
和 Q 在离散型变量的情况下是相同的分布，或者在连续型变量的情况下是 ‘‘几乎
处处’’ 相同的。</p>
<p>==因为 KL
散度是非负的并且衡量的是两个分布之间的差异，它经常被用作分布之间的某种距离。==
然而，它并不是真的距离因为它不是对称的：对于某 些 P 和 Q，<span
class="math inline">\(D_{KL}(P||Q)\neq D_{KL}(Q||P)\)</span>
。这种非对称性意味着选择 <span
class="math inline">\(D_{KL}(P||Q)\)</span> 还是<span
class="math inline">\(D_{KL}(Q||P)\)</span>影响很大。</p>
<p><img src="https://szfmsmdx.oss-cn-hangzhou.aliyuncs.com/blog_pic/202404271443153.png" alt="image-20240427144308978" style="zoom:67%;" /></p>
<p>!!!一个和 KL 散度密切联系的量是
<strong>交叉熵（cross-entropy）</strong><span class="math inline">\(H(P,
Q) = H(P) + D_{KL}(P||Q)\)</span>，它和 KL
散度很像但是缺少左边一项：</p>
<p><span class="math display">\[H(P,Q)=-\mathbb E_{x\sim P}\log
Q(x)\]</span></p>
<p>针对 Q 最小化交叉熵等价于最小化 KL 散度，因为 Q
并不参与被省略的那一项。</p>
<p>在信息论的计算过程中，经常会遇到 <span class="math inline">\(0\log
0\)</span> 的表达式，按照惯例，将它处理成 <span
class="math inline">\(\lim_{x\to 0}x\log x=0\)</span></p>
]]></content>
      <categories>
        <category>Deep Learning</category>
      </categories>
      <tags>
        <tag>花书</tag>
        <tag>Deep Learning</tag>
        <tag>线性代数</tag>
        <tag>概率论</tag>
        <tag>信息论</tag>
      </tags>
  </entry>
  <entry>
    <title>hexo中的markdown语法技巧</title>
    <url>/2024/04/10/hexo%E4%B8%AD%E7%9A%84markdown%E8%AF%AD%E6%B3%95%E6%8A%80%E5%B7%A7/</url>
    <content><![CDATA[<p>参考这篇<a
href="https://www.imczw.com/post/tech/hexo-next-tags-markdown.html">博客</a>，效果确实是很震惊</p>
<span id="more"></span>
<h1 id="居中引用">居中引用</h1>
<blockquote class="blockquote-center">
<p>hello world</p>

</blockquote>
<h1 id="彩色-tag">彩色 tag</h1>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&#123;% label [class] @ 标签内容 %&#125;</span><br><span class="line">class：</span><br><span class="line"> - default: 浅灰色,可留空</span><br><span class="line"> - primary: 浅紫色</span><br><span class="line"> - success: 浅绿色</span><br><span class="line"> - info: 浅蓝色</span><br><span class="line"> - warning: 浅黄色</span><br><span class="line"> - danger: 浅红色</span><br></pre></td></tr></table></figure>
<mark class="label [warning]"> test</mark>
<p>注意，由于之前安装的darkmode插件导致了前端css被覆盖，所以实际显示的颜色可能有不同</p>
<h1 id="彩色-note">彩色 note</h1>
<p>我个人很喜欢这个东西 ^ ^</p>
<p>(为了避免编译错误，所以放在不同的行了。。。) <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&#123;% note [class]%&#125; </span><br><span class="line">内容</span><br><span class="line">&#123;% endnote %&#125;</span><br><span class="line"></span><br><span class="line">class: </span><br><span class="line"> - default: 灰色</span><br><span class="line"> - primary: 紫色</span><br><span class="line"> - success: 绿色</span><br><span class="line"> - info: 蓝色</span><br><span class="line"> - warning: 黄色</span><br><span class="line"> - danger: 红色</span><br></pre></td></tr></table></figure></p>
<details class="note "><summary><p>[info]</p>
</summary>
<p>hello world</p>

</details>
<h1 id="切换-tabs">切换 tabs</h1>
<p>这个也不错，感觉之后不论是放代码还是放对比的东西，比上下文叠放好用多了。。。<br />
<div class="tabs" id="[class]"><ul class="nav-tabs"><li class="tab active"><a href="#[class]-1">Tab1</a></li><li class="tab"><a href="#[class]-2">Tab2</a></li><li class="tab"><a href="#[class]-3">3号Tab</a></li></ul><div class="tab-content"><div class="tab-pane active" id="[class]-1"><p>这里是1号Tab内容</p></div><div class="tab-pane" id="[class]-2"><p>这里是2号Tab内容</p></div><div class="tab-pane" id="[class]-3"><p>这里是3号Tab内容，上面的tab也可以改 ^ ^</p></div></div></div></p>
<h1 id="按钮-btn">按钮 Btn</h1>
<p>使用自带 FontAwesome 图标，在 <a
href="https://fontawesome.com/v4/icons/">FontAwesome</a>
上找到相应的图标名<br />
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&#123;% btn [地址] , [内容] , [图标 规格] %&#125;</span><br><span class="line"></span><br><span class="line">[地址]：当使用外链绝对引用时，无按钮，样式类似Markdown链接跳转</span><br><span class="line">[图标 规格]: 使用无`fa-`开头的FontAwesome图标，可以用以下规格定义图标大小</span><br><span class="line">fa-fw | fa-lg | fa-2x | fa-3x | fa-4x | fa-5x</span><br><span class="line"></span><br></pre></td></tr></table></figure> 但是不知道为什么显示不出来 hhh</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&#123;% btn [https://www.baidu.com/] [百度] [fa-home-2x]&#125;  </span><br><span class="line"></span><br><span class="line">&#123;% btn [https://www.baidu.com/] [百度] [fa-home]&#125;  </span><br></pre></td></tr></table></figure>
<p>哎哎，显示不出来我就给他用多行代码的形式注释掉了。。。</p>
]]></content>
      <categories>
        <category>hexo</category>
      </categories>
      <tags>
        <tag>hexo</tag>
        <tag>markdown</tag>
      </tags>
  </entry>
  <entry>
    <title>nexT美化</title>
    <url>/2024/04/09/nextT%E7%BE%8E%E5%8C%96/</url>
    <content><![CDATA[<p>额额。。。此贴的所有操作都是基于 <code>next8</code>
版本进行的。。。<br />
版本对应不上可能导致相关文件找不到，还请各位善用搜索引擎</p>
<span id="more"></span>
<h1 id="可切换模式按钮">可切换模式按钮</h1>
<p><del>参考了这个佬的<a
href="https://haomingzhang.com/hexo_3/">博客</a></del><br />
<del>在路径<code>themes/next/_vendors.yml</code> 下添加
<code>darkmode.js</code> 的cdn</del><br />
<del>然后我在next目录下没有找到 <code>_config.next.yml</code>
但是我添加在 <code>_config.yml</code> 里</del></p>
<p><br></p>
<p>失败了。。。<br />
可能是版本不对，更新给文件删了吧。。。</p>
<hr />
<p>我又找到了另一个佬的<a
href="https://www.techgrow.cn/posts/abf4aee1.html">博客</a><br />
废话我不多说了，有用！<br />
然后如果要自定义可以在这个路径下进行修改<br />
<code>\[root]\node_modules\hexo-next-darkmode\lib</code><br />
root 就是你的博客根目录，其他 npm install 的包也在这里，可以找一找
hh</p>
<h1 id="文章底部添加">文章底部添加</h1>
<p>额原理就是加一个html的div标签<br />
在路径 <code>\themes\next\layout\_macro</code> 中新建
<code>passage-end-tag.swig</code> 文件,并添加以下内容：<br />
<figure class="highlight html"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">div</span>&gt;</span></span><br><span class="line">    &#123;% if not is_index %&#125;</span><br><span class="line">        <span class="tag">&lt;<span class="name">div</span> <span class="attr">style</span>=<span class="string">&quot;text-align:center;color: #ccc;font-size:14px;&quot;</span>&gt;</span>-------------本文结束<span class="tag">&lt;<span class="name">i</span> <span class="attr">class</span>=<span class="string">&quot;fa fa-paw&quot;</span>&gt;</span><span class="tag">&lt;/<span class="name">i</span>&gt;</span>感谢您的阅读-------------<span class="tag">&lt;/<span class="name">div</span>&gt;</span></span><br><span class="line">    &#123;% endif %&#125;</span><br><span class="line"><span class="tag">&lt;/<span class="name">div</span>&gt;</span></span><br></pre></td></tr></table></figure></p>
<p>接着打开<code>\themes\next\layout\_macro\post.swig</code>文件，在post-body
之后， post-footer
之前添加如下画红色部分代码（post-footer之前两个DIV，在END POST
BODY之后的部分）： <figure class="highlight html"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">div</span>&gt;</span></span><br><span class="line">  &#123;% if not is_index %&#125;</span><br><span class="line">    &#123;% include &#x27;passage-end-tag.swig&#x27; %&#125;</span><br><span class="line">  &#123;% endif %&#125;</span><br><span class="line"><span class="tag">&lt;/<span class="name">div</span>&gt;</span></span><br></pre></td></tr></table></figure></p>
<p>但是我是没有 <code>post.swig</code> 文件的，我操作了
<code>post.njk</code> 文件，同样可以</p>
<p>然后打开主题配置文件(_config.yml),在末尾添加： <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># 文章末尾添加“本文结束”标记</span><br><span class="line">passage_end_tag:</span><br><span class="line">  enabled: true</span><br></pre></td></tr></table></figure></p>
<h1 id="把底部-标签换成图标">把底部 # 标签换成图标</h1>
<p>修改模板<code>/themes/next/layout/_macro/post.njk</code> 在尾部找到
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&#123;%- set tag_indicate = &#x27;&lt;i class=&quot;fa fa-tag&quot;&gt;&lt;/i&gt;&#x27; if theme.tag_icon else &#x27;#&#x27; %&#125;</span><br><span class="line"></span><br><span class="line"># 删去 if theme.tag_icon else &#x27;#&#x27; 即可</span><br><span class="line">&#123;%- set tag_indicate = &#x27;&lt;i class=&quot;fa fa-tag&quot;&gt;&lt;/i&gt;&#x27;%&#125;</span><br><span class="line">重新渲染后可正确使用</span><br></pre></td></tr></table></figure></p>
<h1 id="首页折叠">首页折叠</h1>
<p>起因是我觉得每次进主页都直接看全文很麻烦。。。<br />
然后去搜了下压缩结果是静态资源的压缩，在这篇<a
href="https://akilar.top/posts/49b73b87/">博客</a>里<br />
但我其实搜错关键字了hh<br />
歪打正着给博客静态资源压缩了<br />
参考这篇<a
href="https://blog.csdn.net/yueyue200830/article/details/104470646">博客</a>
没想到主页文章折叠在 <code>next</code> 的 <code>_config.yml</code>
文件里就有<br />
不过貌似优先级比较低？在我的<code>markdown</code>生成目录的文章里，如果填在
<code>&lt;!--TOC--&gt;</code> 标签之前就无法折叠了。。。</p>
<h1 id="next文章评论区">next文章评论区</h1>
<p>看了下 <code>next</code> 的 <code>_config.yml</code> 文件<br />
额额最终还是采用了 <code>waline</code>
搭建个人博客（<code>gitalk</code>折腾了半天一直会报
<code>e.toLowerCase is not a function</code>的错，他在GitHub的issue下面是有相关问题的，但是我按照其他人给出的解决方案没有搞定，只能转战
<code>waline</code> 了<br />
waline的参考手册还是很详细的，按照步骤很方便快速地部署完成了</p>
<h2 id="vercel-部署">vercel 部署</h2>
<p>采用了 <code>vercel</code> 自动部署 <code>waline</code><br />
参考文档如下： - <a
href="https://waline.js.org/guide/deploy/vercel.html#%E5%A6%82%E4%BD%95%E9%83%A8%E7%BD%B2">Waline</a></p>
<p>管理界面地址为 https://[your app address]/ui<br />
或者进入你的app中，看页面上方的导航标签有管理可以进入。。。<br />
但是有一个问题就是通过vercel部署访问的速度实在太慢了。。。<br />
之后考虑看看有没有其他办法能够部署到国内</p>
<h2 id="waline-dark">waline dark</h2>
<p>配置好的 waline 是不支持黑夜模式的，看了官方文档但是我没有找到 client
文件，所以直接修改 css 作罢<br />
随后考虑修改 <code>_config.yml</code>
文件中的字段看是否能够修改（？字段几乎都不起作用，奇怪）<br />
最后考虑到之前做的 dark mode按钮，那个是直接覆盖页面的
css，应该也可以修改评论区的css<br />
一查发现佬的文章里已经做了说明。 &gt; 由于暗黑模式切换插件依赖了
Darkmode.js，如果插件不生效，这很有可能是 Darkmode.js 的 CDN
资源失效了（在国内访问被墙）。<br />
&gt; 此时，建议使用暗黑模式切换插件的 libUrl 配置参数来指定可用的 CDN
资源链接，如下所示： &gt; - 使用 Unpkg 免费提供的 CDN 资源 &gt;
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">darkmode_js:</span><br><span class="line"> ...</span><br><span class="line"> libUrl: &#x27;https://unpkg.com/darkmode-js@1.5.7/lib/darkmode-js.min.js&#x27;</span><br></pre></td></tr></table></figure> &gt; - 使用 Jsdelivr 免费提供的 CDN 资源 &gt;
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">darkmode_js:</span><br><span class="line"> ...</span><br><span class="line"> libUrl: &#x27;https://cdn.jsdelivr.net/npm/darkmode-js@1.5.7/lib/darkmode-js.min.js&#x27;</span><br></pre></td></tr></table></figure></p>
<p>按照佬的方案轻松解决 ^ ^</p>
<h2 id="waline加速">waline加速</h2>
<p>由于<code>vercel</code>在国内被dns污染了所以访问起来不挂梯子是看不到评论区的<br />
参考了网上的一些方法，刚好域名备案下来了，搞一下waline的加速<br />
<br> ！注意，这个操作是要基于已有域名的情况下才能继续进行<br />
我们要做的就是<strong>用已有的博客域名去免费申请一个子域名来代替vercel.app那个被污染的域名。</strong></p>
<ol type="1">
<li>进入域名控制台，找到你的博客域名，添加解析记录
<ol type="1">
<li>记录类型选择 <code>CNAME</code></li>
<li>主机记录填 <code>comment</code>
(可以自定，这一步完成你的二级域名就是 comment + 你的一级域名)</li>
<li>添加记录值为 <code>cname.vercel-dns.com</code></li>
</ol></li>
</ol>
<p>(比如我的域名是 <code>szf.cool</code>，那么操作完，我的二级域名应该是
<code>comment.szf.cool</code>)<br />
<br> 2.
进入vercel控制台，进入你的项目，在<code>setting</code>-<code>Domains</code>下，在输入框中输入你的二级域名
=&gt; add添加<br />
3.
同时修改next的config文件，更换一下<code>serverURL</code>字段即可<br />
4. 这样就可以了，随后部署测试一下评论区是否能够正常访问即可</p>
<p>额额但是在博客的评论区里，power by waline的版本是2.x。。。<br />
在管理页面是 3.x 版本，目前还没出问题，不至于以后会不会有问题。。。</p>
<h1 id="文件跳过渲染">文件跳过渲染</h1>
<p>修改站点配置文件中的 <code>skip_render</code> 配置项。<br />
只有 <code>source</code> 目录下的文件才会发布，因此 Hexo 只渲染 source
目录下的文件。<code>skip_render</code> 参数设置的路径是相对于
<code>source</code> 目录的路径。<br />
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">#单个文件</span><br><span class="line">skip_render: hello.html</span><br><span class="line"></span><br><span class="line">#单个文件夹下全部文件</span><br><span class="line">skip_render: test/* </span><br><span class="line"></span><br><span class="line">#单个文件夹下指定类型文件</span><br><span class="line">skip_render: test/*.md  </span><br><span class="line"></span><br><span class="line">#单个文件夹下全部文件以及子目录</span><br><span class="line">skip_render: test/**  </span><br><span class="line"></span><br><span class="line">#跳过多个目录，或者多个文件</span><br><span class="line">skip_render: [&#x27;*.html&#x27;, demo/**, test/*]</span><br></pre></td></tr></table></figure></p>
<h1 id="文章置顶功能">文章置顶功能</h1>
<p>参考这篇<a
href="https://blog.csdn.net/stormdony/article/details/86745805">博客</a><br />
首先修改本地的仓库<br />
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ npm uninstall hexo-generator-index --save</span><br><span class="line">$ npm install hexo-generator-index-pin-top --save</span><br></pre></td></tr></table></figure></p>
<p>然后就可以在文章的顶部信息修改 <code>top</code> 字段了。。。<br />
同时要设置一下置顶的标签 hh</p>
<p>我是在<code>\next\layout\_marco\post.nijk</code> 文件下的
<code>post-meta-container</code>标签中添加的相关代码<br />
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&#123;% if post.top %&#125;</span><br><span class="line">  &lt;i class=&quot;fa fa-thumb-tack&quot;&gt;&lt;/i&gt;</span><br><span class="line">  &lt;font color=7D26CD&gt;置顶&lt;/font&gt;</span><br><span class="line">  &lt;span class=&quot;post-meta-divider&quot;&gt;|&lt;/span&gt;</span><br><span class="line">&#123;% endif %&#125;</span><br></pre></td></tr></table></figure> 注意缩进 ^ ^<br />
但是这个紫色的颜色不好看，我修改了 font color的值</p>
<h1 id="更新markdown渲染">更新markdown渲染</h1>
<p>额额看花书洋洋洒洒写了点文字，结果没想到markdown的渲染好像不是很行，于是按照教程重新折腾了一下markdown的渲染。。。<br />
教程在<a
href="https://blog.csdn.net/qq_42951560/article/details/123596899">这里</a><br />
笔者又找到一个markdown的依赖包<a
href="https://blog.csdn.net/qq_36667170/article/details/105846999">教程</a></p>
]]></content>
      <categories>
        <category>hexo</category>
      </categories>
      <tags>
        <tag>hexo</tag>
        <tag>nexT</tag>
      </tags>
  </entry>
  <entry>
    <title>markdown生成目录</title>
    <url>/2024/04/09/markdown%E7%94%9F%E6%88%90%E7%9B%AE%E5%BD%95/</url>
    <content><![CDATA[<!-- TOC -->
<!-- vscode-markdown-toc -->
<ul>
<li><a href="#test">test</a></li>
<li><a
href="#VSCMarkdownTOC">利用VSC的插件<code>Markdown TOC</code>生成目录</a></li>
<li><a href="#test-1">test</a>
<ul>
<li><a href="#test1">test1</a> <!-- vscode-markdown-toc-config
  numbering=false
  autoSave=true
  /vscode-markdown-toc-config --> <!-- /TOC --></li>
</ul></li>
</ul>
<span id="more"></span>
<h1 id="在hexo中开启侧边栏文章目录">在hexo中开启侧边栏文章目录</h1>
<p>看了网上的一些帖子，好像因为 <code>next</code> 版本不匹配导致<br />
其实好像不用大费周章寻找
<code>_custom.styl</code>（至少我是这样。。。）<br />
只需要在<code>_config.md</code> 下修改 toc 词条即可<br />
这是我的配置<br />
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># Table of Contents in the Sidebar</span><br><span class="line"># Front-matter variable (nonsupport wrap expand_all).</span><br><span class="line">toc:</span><br><span class="line">  enable: true</span><br><span class="line">  # Automatically add list number to toc.</span><br><span class="line">  number: true</span><br><span class="line">  # If true, all words will placed on next lines if header width longer then sidebar width.</span><br><span class="line">  wrap: true</span><br><span class="line">  # If true, all level of TOC in a post will be displayed, rather than the activated part of it.</span><br><span class="line">  expand_all: true</span><br><span class="line">  # Maximum heading depth of generated toc.</span><br><span class="line">  max_depth: 3</span><br></pre></td></tr></table></figure> 按照英文指示自行按照喜好修改参数即可hh</p>
<h2 id="test"><a name='test'></a>test</h2>
<blockquote>
<p>test for generate TOC</p>
</blockquote>
<h1 id="markdown中插入目录">markdown中插入目录</h1>
<h2
id="利用vsc的插件markdown-toc生成目录"><a name='VSCMarkdownTOC'></a>利用VSC的插件<code>Markdown TOC</code>生成目录</h2>
<p>直接在VSC的插件里搜索
<code>Markdown TOC</code>，第一个下载最高的插件下载<br />
根据插件给出的使用方法<br />
&gt; 1. <code>ctrl + shift + p</code> 呼出面板 &gt; 2.
将光标移动到你要生成目录的地方 &gt; 3. 输入 <code>Generate</code>
找到命令 <code>Generate TOC for markdown</code> &gt; 4.
选择该命令就可以了。。。 &gt; 5. 可以在他注释的地方修改参数 &gt;
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">numbering=false # 为true则开启标签目录带序号</span><br><span class="line">autoSave=true   # 为true生成目录时自动保存文件</span><br></pre></td></tr></table></figure></p>
<p>不过好像这个插件会自动将你md文件中的标签改成 html 的格式<br />
但是初次生成的时候，好像默认两个参数都是true，我也没找到在哪里修改他的配置文件，所以就怪怪的要生成两次，先生成一次然后修改参数，删掉原来的目录（保存注释），再生成一次他就会按照参数生成一份</p>
<p><br></p>
<p>而且感觉这东西好像不太实用呢，我需要将他生成的目录先放到hexo生成的两个<code>---</code>后面，然后这个插件为啥不支持生成一级文件。。。<br />
此文仅做个记录以后不用了。。。</p>
<h2 id="test-1"><a name='test-1'></a>test</h2>
<blockquote>
<p>test for generate TOC</p>
</blockquote>
<h3 id="test1"><a name='test1'></a>test1</h3>
<blockquote>
<p>test for generate TOC</p>
</blockquote>
<h2 id="typora">typora</h2>
<p>。。。才发现 typora能自动生成目录啊<br />
<code>[TOC]</code>即可。。。<br />
有点 🤡 了呀</p>
]]></content>
      <categories>
        <category>markdown</category>
      </categories>
      <tags>
        <tag>hexo</tag>
        <tag>markdown</tag>
      </tags>
  </entry>
  <entry>
    <title>hexo 主题更换</title>
    <url>/2024/04/07/hexo-%E4%B8%BB%E9%A2%98%E6%9B%B4%E6%8D%A2/</url>
    <content><![CDATA[<p>今天折腾一下 hexo 的主题变换 hh</p>
<span id="more"></span>
<p>翻了一下<a
href="https://hexo.io/themes/">hexo主题</a>，看到几个我自己觉得比较简洁还不错的<br />
- <a
href="https://github.com/hooozen/hexo-theme-tranquility">Tranquility</a>
- <a href="https://github.com/Lhcfl/hexo-theme-anatolo">Anatolo</a> - <a
href="https://github.com/jerryc127/hexo-theme-butterfly">butterfly</a> -
... 之后再看看吧 hhh</p>
<hr />
<blockquote>
<p>安装的过程参考了这篇<a
href="https://zhuanlan.zhihu.com/p/618864711">文章</a></p>
</blockquote>
<h1 id="安装-next">安装 next</h1>
<p>在根目录下执行 <figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">git clone https://github.com/next-theme/hexo-theme-next themes/next</span><br></pre></td></tr></table></figure></p>
<p>下载完成后进入 <code>_config.yml</code> 文件修改
<code>theme</code>，输入 next 即可<br />
然后 hexo 三连就能看到主页主题已经发生变化了</p>
<p>版本查看在
<code>/themes/next/package.json</code>文件的version词条里</p>
<h2 id="安装插件">安装插件</h2>
<h3 id="博客信息">博客信息</h3>
<p>此为在根目录下的总的博客信息，包括
<code>title</code>、<code>subtitle</code> 等等内容</p>
<h3 id="配置-next-主页">配置 next 主页</h3>
<p>进 <code>next</code> 的 <code>_config.yml</code>
文件进行修改注释是不行的，因为他找不到相应的路径<br />
所以要先在根目录下建立相应的文件夹然后再去修改 <code>next</code>
的配置文件</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">hexo new page [tags名称] # 比如和注释里相同的 about、tags之类</span><br></pre></td></tr></table></figure>
<p>同理创建其他菜单，然后在 <code>next</code>
的配置文件中进行修改即可...<br />
可以在source里面找到菜单，其实里面就是一些 md
文件，也可以自己进行修改添加内容等</p>
<h3 id="搜索功能">搜索功能</h3>
<p>添加搜索功能与预计阅读时间<br />
需要安装 <code>symbols-count-time</code> 插件<br />
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">npm install hexo-symbols-count-time</span><br></pre></td></tr></table></figure></p>
<h3 id="账户头像">账户头像</h3>
<p>首先要将图片放在 <code>/next/source/images</code> 下<br />
然后修改 <code>next</code>
的配置文件，更改avatar的图片路径为刚刚的路径即可<br />
- rounded 是图片是否被圆形切割 - rotated 是头像是否随鼠标旋转</p>
<h3 id="修改文章间的分割线">修改文章间的分割线</h3>
<p>找到文件
<code>themes/next/source/css/_common/components/post/post-footer.styl</code>
进入修改如下内容 <figure class="highlight css"><table><tr><td class="code"><pre><span class="line"><span class="selector-class">.post-eof</span> &#123;</span><br><span class="line">  <span class="attribute">background</span>: $grey-light;</span><br><span class="line">  <span class="attribute">height</span>: <span class="number">3px</span>;</span><br><span class="line">  <span class="attribute">margin</span>: $post-eof-margin-top auto $post-eof-margin-bottom;</span><br><span class="line">  <span class="attribute">text-align</span>: center;</span><br><span class="line">  <span class="attribute">width</span>: <span class="number">100%</span>;</span><br><span class="line"></span><br><span class="line">  <span class="selector-class">.post-block</span><span class="selector-pseudo">:last-of-type</span> &amp; &#123;</span><br><span class="line">    <span class="attribute">display</span>: none;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure> 可以自行根据喜好决定参数大小</p>
<h3 id="next的目录和标签">nexT的目录和标签</h3>
<p>按照之前的步骤，<code>hexo new page</code> 完之后，要在
<code>tags</code> 和 <code>categories</code> 的 <code>index.md</code>
里修改一下 <code>type</code><br />
就是在<code>date</code>下加上<code>type: "tags"</code></p>
<p>以 <code>tags</code> 为例</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">---</span><br><span class="line">title: tags</span><br><span class="line">date : xxxx</span><br><span class="line">type : &quot;tags&quot;</span><br><span class="line">---</span><br></pre></td></tr></table></figure>
<p>然后回到<code>_post</code>要推送的笔记下面，在相应文章的标签上加上<code>tags</code>即可<br />
如下所示: <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">---</span><br><span class="line">title: xxxx</span><br><span class="line">date: xxxx</span><br><span class="line">categories: hexo</span><br><span class="line">tags: # 标签（多标签）</span><br><span class="line">- hexo</span><br><span class="line">- nexT</span><br><span class="line">---</span><br></pre></td></tr></table></figure></p>
<p>然后 <code>hexo</code> 三连即可</p>
]]></content>
      <categories>
        <category>hexo</category>
      </categories>
      <tags>
        <tag>hexo</tag>
        <tag>nexT</tag>
      </tags>
  </entry>
  <entry>
    <title>hexo本地博客+云服务器个人博客搭建</title>
    <url>/2024/04/06/hexo%E6%9C%AC%E5%9C%B0%E5%8D%9A%E5%AE%A2+%E4%BA%91%E6%9C%8D%E5%8A%A1%E5%99%A8%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA/</url>
    <content><![CDATA[<p>记录一下本地搭建 <code>hexo</code>
个人博客并部署到云服务器的过程。。。<br />
本地环境是 <code>win11</code> ，云服务器是
<code>Ubuntu 22.04LTS</code></p>
<p>大概的流程是本地写笔记，推送到云服务器然后部署，展示给大家<br />
所以当然也可以用 <code>GitHub.io</code>
去进行部署，这是取决于各位的。。。</p>
<span id="more"></span>
<h1 id="node.js-git">Node.js &amp;&amp; git</h1>
<p><code>Node.js</code> 和 <code>git</code> 是 <code>hexo</code>
安装前的必备插件，所以要提前安装一下这两个，基本上，进官网直接下载一直
next 安装即可。。。</p>
<h2 id="git">git</h2>
<p><a href="https://git-scm.com/download/win">git下载地址</a><br />
可以通过 cmd 命令 <code>git -v</code> 查看 <code>git</code>
版本判断是否成功安装<br />
按需下载即可</p>
<h2 id="node.js">Node.js</h2>
<p><a href="https://nodejs.cn/download/">Nodejs下载地址</a><br />
同样按需下载，没有特殊要求无脑继续就可以了。。<br />
可以在 cmd 窗口中通过 <code>node -v</code> 和 <code>npm -v</code>
命令查看版本（查看是否成功安装了 node）<br />
注意 <code>node</code> 自带 <code>npm</code>
,但是版本和下载路径可能不满足大家的需求</p>
<h3 id="npm更换淘宝镜像源">npm更换淘宝镜像源</h3>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment">#最新地址 淘宝 NPM 镜像站喊你切换新域名啦!</span></span><br><span class="line">npm config <span class="built_in">set</span> registry https://registry.npmmirror.com</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看当前源</span></span><br><span class="line">npm config get registry </span><br><span class="line"></span><br><span class="line"><span class="comment"># 恢复官方源</span></span><br><span class="line">npm config <span class="built_in">set</span> registry https://registry.npmjs.org</span><br></pre></td></tr></table></figure>
<h3 id="npm更新">npm更新</h3>
<p><a
href="https://juejin.cn/post/7065534944101007391">搜索词条的第一个</a><br />
这里通过 <code>n</code> 包对 <code>Nodejs</code> 进行管理
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 安装</span></span><br><span class="line">npm install -g n</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看版本</span></span><br><span class="line">n lts <span class="comment"># 长期支持版本</span></span><br><span class="line">n latest <span class="comment"># 最新版本</span></span><br></pre></td></tr></table></figure></p>
<p>其他命令就不展示了，善用搜索引擎 hh</p>
<h1 id="配置-hexo">配置 hexo</h1>
<p>在命令行中输入<br />
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">npm install -g hexo-cli</span><br></pre></td></tr></table></figure></p>
<p>找到你要写博客的父目录，比如你想将文件夹放在桌面就取到桌面的路径下</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">hexo init &lt;folder&gt; // folder为你的博客目录名字</span><br><span class="line"><span class="built_in">cd</span> &lt;folder&gt; // 进入该目录</span><br><span class="line">npm install // 安装依赖</span><br></pre></td></tr></table></figure>
<p>到此，本地博客就已经搭建完成了</p>
<h1 id="将本地博客部署到云服务器中">将本地博客部署到云服务器中</h1>
<p>既然是部署到云服务器，当然需要 <code>ssh</code> 连接，我用的 cmd
下的ssh连接<br />
貌似 win 下已经自带了 openssh
吧，我不太清楚hh，还请有心人自行检索搜索引擎 ^ ^</p>
<h2 id="git-安装">git 安装</h2>
<p>这里我用的阿里云的服务器，他们给的镜像里已经装有了 <code>git</code>
所以就没有安装了。。<br />
既然需要同步，那么自然不能每次都要登录 <code>git</code>
，所以要配置一下免密登录</p>
<h3 id="创建用户并设置密码">1. 创建用户并设置密码</h3>
<p>下面的 <code>username</code>
设置为你喜欢的用户名，并设置一个属于这个账户的密码 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">adduser [username]</span><br><span class="line">passwd [username]</span><br></pre></td></tr></table></figure>
可能不是这个顺序，但是注意看命令行给的提示，看心情补充额外信息吧，只要有用户名密码即可
hh</p>
<h3 id="分配用户权限">2. 分配用户权限</h3>
<p>如果没有 wheel 组就先建立 wheel 组（没有的话直接添加会报错的）
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 建立 wheel 组</span></span><br><span class="line">addgroup wheel</span><br><span class="line"></span><br><span class="line"><span class="comment"># 添加刚刚建立的用户到用户组</span></span><br><span class="line">usermod [username] -G wheel</span><br></pre></td></tr></table></figure></p>
<h3 id="本地创建密钥">3. 本地创建密钥</h3>
<p>！！！ 注意是在本地 <code>win11</code>
创建密钥，相当于身份证了，就可以实现免密登录了<br />
在本机终端中输入 <figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">ssh-keygen -t rsa</span><br></pre></td></tr></table></figure> 然后无脑回车，得到一个新的密钥
一般来说在你创建的时候命令行会提示你密钥文件的路径，windows电脑一般是C:.ssh（不确定可以百度一下
hh）</p>
<p>复制id_rsa.pub文件中的内容备用。</p>
<h3 id="配置公钥">4. 配置公钥</h3>
<p>回到服务器端，用 <code>su</code> 命令切到你创建的那个用户中
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">su - [username]</span><br></pre></td></tr></table></figure></p>
<p>创建 <code>.ssh</code> 文件夹 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">cd</span> ~</span><br><span class="line"><span class="built_in">mkdir</span> .ssh</span><br></pre></td></tr></table></figure></p>
<p>利用 <code>vi</code> 或 <code>vim</code> 新建
<code>authorized_keys</code> 文件 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">vim .ssh/authorized_keys</span><br><span class="line"></span><br><span class="line"><span class="comment"># 或者</span></span><br><span class="line">vi .ssh/authorized_keys</span><br></pre></td></tr></table></figure> 将刚刚
<code>id_rsa.pub</code> 公钥中的内容，复制粘贴到文件里，保存退出。</p>
<h2 id="git-仓库配置">git 仓库配置</h2>
<p>退出用户登录 <figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">su root</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">按照提示输入你的密码...</span></span><br></pre></td></tr></table></figure></p>
<p>这时应该可以看到是 <code>root</code> 用户，创建 <code>git</code>
目录，并修改目录的所有权和用户权限 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">mkdir</span> /home/git/</span><br><span class="line"><span class="built_in">chown</span> -R [username]:[username] /home/git/   <span class="comment"># 是你刚刚创建的用户名</span></span><br><span class="line"><span class="built_in">chmod</span> -R 755 /home/git/</span><br></pre></td></tr></table></figure></p>
<p>进入目录，建立git仓库，修改权限 <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">cd /home/git/</span><br><span class="line">git init --bare blog.git</span><br><span class="line">chown [username]:[username] -R blog.git</span><br></pre></td></tr></table></figure></p>
<p>新建钩子文件，但是要看你具体的 git 版本 我的是
<code>post-update.xxx</code>（文件类型忘了，配置时忘了记录）<br />
需要进入修改添加两行内容 <figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">vim /home/git/blog.git/hooks/post (然后这里直接按 tab 补全补出来)</span><br></pre></td></tr></table></figure></p>
<p>进入文件后添加 <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">#!/bin/bash</span><br><span class="line">git --work-tree=/home/blog --git-dir=/home/git/blog.git checkout -f</span><br></pre></td></tr></table></figure></p>
<p>不着急退出，看一下上面的注释，我的文件里写着要修改文件名为
<code>post-update</code> 才能生效，具体问题具体分析吧，用 rm
命令重命名好了，然后<code>wq</code>保存退出<br />
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">rm ./post- (tab 补全)  ./post-update</span><br></pre></td></tr></table></figure></p>
<p>然后修改文件权限 <figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">chmod +x /home/git/blog.git/hooks/post-update</span><br></pre></td></tr></table></figure></p>
<h2 id="配置-nginx">配置 nginx</h2>
<p>安装 <code>nginx</code></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">apt-install nginx </span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">同样可以查看版本判断是否安装成功</span></span><br><span class="line">nginx -v</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">设置开机启动</span></span><br><span class="line">systemctl enable nginx.service</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">查看 nginx 状态</span></span><br><span class="line">systemctl status nginx.service</span><br></pre></td></tr></table></figure>
<p>修改nginx的默认配置，其中cd后边就是刚刚查到的安装位置，每个人可能都不一样
<code>/usr/local/nginx/conf/nginx.conf</code><br />
我的安装位置是<code>/etc/nginx/nginx.conf</code><br />
在网上我看到两种说法，一种修改 default一种修改
conf，但我个人觉得还是修改conf吧。。。<br />
<a href="https://zhuanlan.zhihu.com/p/158678677">配置 conf
文件</a><br />
<a
href="https://blog.captainz.cc/posts/hexo_nginx.html#%E9%85%8D%E7%BD%AE-Nginx-%E6%89%98%E7%AE%A1%E6%96%87%E4%BB%B6%E7%9B%AE%E5%BD%95">配置
default 文件</a></p>
<p>配置完重启下 <code>nginx</code> <figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">service nginx restart</span><br></pre></td></tr></table></figure></p>
<h2 id="更改本地-hexo-配置文件">更改本地 hexo 配置文件</h2>
<p>打开你<strong>本地</strong>的hexo博客所在文件里面的配置文件_config.yml<br />
应该在最后的位置 <figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">deploy:</span><br><span class="line">  type: git</span><br><span class="line">  repository: [username]@[公网ip]:[/home/git/blog.git]   #用户名@服务器Ip:git仓库位置</span><br><span class="line">  branch: master</span><br></pre></td></tr></table></figure></p>
<p>在本地hexo博客根目录下，打开终端，部署<br />
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">hexo clean</span><br><span class="line">hexo g</span><br><span class="line">hexo d</span><br></pre></td></tr></table></figure></p>
<p>然后访问服务器的 IP 地址应该就可以看到 hexo
已经部署完成了。。。<br />
后续就是善用搜索引擎的过程了。。。</p>
<h1 id="域名配置">域名配置</h1>
<p>但是目前访问博客的时候有个问题，就是网址栏一直有个不安全，这是由于通过http访问<br />
我们需要将其改为 https 访问(https = http + ssl)<br />
所以首先要申请 ssl 证书，然后再去nginx里配置一下<br />
1. 去服务商拿ssl免费证书<br />
我是阿里的服务器，<a
href="https://yundun.console.aliyun.com/?spm=5176.7968328.J_8413632810.1.62ae685bLZTqIi&amp;p=cas&amp;showBuy=1#/certExtend/free/cn-hangzhou">地址</a>在这里<br />
2. 选择免费证书，然后一路系统配置申领就好了<br />
3. 申领结束点击下载 nginx 版本<br />
4. 将证书传到服务器的指定位置(!需要记住)<br />
我放在了 <code>/etc/nginx/conf.d</code>文件夹下 5. 修改
<code>nginx.conf</code> 配置文件内容<br />
注意如果在 conf 文件下修改，要在 http
的域内修改，所以要排查一下"{}"是否闭合，防止漏了hh<br />
添加或者修改<code>server</code>域<br />
这是https的433端口配置<br />
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">server &#123;</span><br><span class="line"><span class="meta prompt_">  #</span><span class="language-bash">HTTPS的默认访问端口443。</span></span><br><span class="line"><span class="meta prompt_">  #</span><span class="language-bash">如果未在此处配置HTTPS的默认访问端口，可能会造成Nginx无法启动。</span></span><br><span class="line">  listen 443 ssl;</span><br><span class="line"><span class="meta prompt_">  #</span><span class="language-bash">填写证书绑定的域名</span></span><br><span class="line">  server_name domain_name;</span><br><span class="line"><span class="meta prompt_">  #</span><span class="language-bash">填写证书文件名称</span></span><br><span class="line">  ssl_certificate cert/domain_name.pem;</span><br><span class="line"><span class="meta prompt_">  #</span><span class="language-bash">填写证书私钥文件名称</span></span><br><span class="line">  ssl_certificate_key cert/domain_name.key;</span><br><span class="line">  ssl_session_cache shared:SSL:1m;</span><br><span class="line">  ssl_session_timeout 5m;</span><br><span class="line"><span class="meta prompt_">  #</span><span class="language-bash">自定义设置使用的TLS协议的类型以及加密套件（以下为配置示例，请您自行评估是否需要配置）</span></span><br><span class="line"><span class="meta prompt_">  #</span><span class="language-bash">TLS协议版本越高，HTTPS通信的安全性越高，但是相较于低版本TLS协议，高版本TLS协议对浏览器的兼容性较差。</span></span><br><span class="line">  ssl_ciphers ECDHE-RSA-AES128-GCM-SHA256:ECDHE:ECDH:AES:HIGH:!NULL:!aNULL:!MD5:!ADH:!RC4;</span><br><span class="line">  ssl_protocols TLSv1.1 TLSv1.2 TLSv1.3;</span><br><span class="line"><span class="meta prompt_">  #</span><span class="language-bash">表示优先使用服务端加密套件。默认开启</span></span><br><span class="line">  ssl_prefer_server_ciphers on;</span><br><span class="line">  location / &#123;</span><br><span class="line">        #配置域名最终向后端发送请求的url地址</span><br><span class="line">        proxy_pass http://127.0.0.1:8008;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<pre><code>这是http的80端口配置

<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">server &#123;</span><br><span class="line">      # 配置阿里云域名和监听的80端口</span><br><span class="line">      listen       80;</span><br><span class="line">      server_name  domain_name;</span><br><span class="line">      #charset koi8-r;</span><br><span class="line">      #access_log  logs/host.access.log  main;</span><br><span class="line"></span><br><span class="line">      #将所有HTTP请求通过rewrite指令重定向到HTTPS。</span><br><span class="line">      rewrite ^(.*)$ https://$host$1;</span><br><span class="line">      location / &#123;</span><br><span class="line">          #配置域名最终向后端发送请求的url地址</span><br><span class="line">          proxy_pass http://127.0.0.1:8008;</span><br><span class="line">      &#125;</span><br><span class="line">      #error_page  404              /404.html;</span><br><span class="line">      # redirect server error pages to the static page /50x.html</span><br><span class="line">      #</span><br><span class="line">      error_page   500 502 503 504  /50x.html;</span><br><span class="line">      location = /50x.html &#123;</span><br><span class="line">          root   html;</span><br><span class="line">      &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure></code></pre>
<ol start="6" type="1">
<li>配置完依次执行命令 <code>service nginx restart</code> 命令和
<code>nginx -t</code>(或<code>systemctl status nginx.service</code>)命令，查看nginx的运行状态，如果是successful，那么就可以访问一下你的域名，网址栏的<code>不安全</code>字段是否消失并变为小锁了，如果是说明配置完成</li>
</ol>
<h2 id="一次的error记录">一次🤡的Error记录。。。</h2>
<p>但是很抱歉，笔者按照上述流程没有配置成功，在配置转发端口的时候，访问遇到了502
error。。。<br />
然后取消转发配置，能发现ssl证书成功配置<br />
接下来排查一下为啥转发失败了 ^ ^</p>
<hr />
<p>先看一下 <code>proxy_pass</code> 设置，遇到两个问题 - 设置为
<code>127.0.0.1</code> 报502错误 - 设置为 <code>公网IP</code> 报 504
错误</p>
<p>理一下，首先，ssl证书配置成功，说明 443 端口是可以正常访问的<br />
我们的配置思路是，如果通过 http 的 80 端口访问，那么强制改写为 443 走
https 端口访问，然后443端口转发到4000的hexo博客端口</p>
<p>好，那么先排查 80 端口是否能正常访问<br />
&gt; 要先安装 telnet<br />
&gt;
在控制面板-程序-程序和功能-（侧边的）启动或关闭Windows功能-telnet勾选<br />
&gt; 等他安装完即可</p>
<p>在shell中 <code>telnet ip port</code> 如果出现空白页面即可<br />
&gt; 靠，注意是空格不是冒号！！！</p>
<p>ok，80端口能ping通（试了下 443
端口同样能ping通，但是4000端口ping不通<br />
好，那么问题就在4000端口了</p>
<hr />
<p>查看 4000 端口的进程<br />
<code>netstat -nplt</code><br />
发现根本就没有4000端口？！<br />
看到这里突然恍然大悟了，这下🤡了呀<br />
原来咱们部署的原理是，在本地deploy的时候，通过git
hook将文件自动push到服务器的仓库中<br />
然后访问80端口，配置nginx将root修改到博客的静态资源处实现访问。。。<br />
这下是彻底理解咱们的整体部署是在干什么了。。。<br />
哎，蒙头按照教程配置还是一知半解<br />
我也将这次🤡记录在博客上，供大家参考</p>
<hr />
<p>接下来修改 nginx 的配置文件就很清晰了。。。<br />
因为我的博客地址是 <code>/home/blog/</code><br />
直接去conf文件下，首先修改 root 字段的地址<br />
然后 index 字段看一眼没问题，ok 重启，登网站，完美。。。</p>
<p>现在的逻辑是：通过域名访问 80 端口-&gt;转发到 433，433去 root 下面搜
index 给出的文件名<br />
&gt; 终于解决了hhhh</p>
<h1 id="图床搭建">图床搭建</h1>
<p>写博客自然不能少了图片，但是放在 hexo
的文件下，每次推送打包的过程实在是受不了，所以还是选择将图片放在服务器上，搭一个图床好了。。。<br />
参考这篇<a
href="https://blog.csdn.net/m0_51321469/article/details/127340237">文章</a><br />
&gt; 主要的原理是利用 PicGo推送到阿里云的服务器，然后 typora
修改图片索引为 url
地址，这样加载的时候就可以直接在服务器的存储地址上进行搜索，不会占用本地资源了。。。</p>
<h2 id="oss存储">OSS存储</h2>
<p>本文用的是阿里云的OSS存储作为图片的放置，要花点小钱
T.T买个资源包<br />
首先在<a
href="https://ecs.console.aliyun.com/server/region/cn-shanghai#/">阿里云的控制台</a>搜索oss，进入oss控制台，然后点击左侧概览下面的<code>Bucket列表</code>创建一个bucket
- 名称自定 - 地域选择离自己近的就好 -
注意读写权限要是公共读！不然博客无法去读取图片资源</p>
<p>创建完成后，点击下面的资源包搞一个便宜点的资源包（有券的话应该免费 ^
^）<br />
资源购买完成后开始创建类似于私钥的东西<br />
在一开始给出的<a
href="https://ecs.console.aliyun.com/server/region/cn-shanghai#/">控制台</a>中，鼠标移动到右上角的头像下，有个
<code>AccessKey管理</code>，点击进入有弹窗提示，这里选择稍微安全点的子用户（因为拿到你的<code>AccessKey</code>可以控制阿里云账户），然后创建用户
- 登录名称自定 - 显示名称自定 -
访问方式要勾选<code>OpenAPI调用访问</code>！！！</p>
<p>创建完成后给他添加上OSS的权限让这个子用户能够访问OSS（直接点击添加权限，在列表里找一下带OSS的词条即可）<br />
页面先不急关，待会要复制<code>ID</code>和<code>Key</code></p>
<h2 id="picgo配置">PicGo配置</h2>
<p>下载地址： - <a
href="https://github.com/Molunerfinn/PicGo/releases">github</a> - <a
href="https://mirrors.sdu.edu.cn/github-release/Molunerfinn_PicGo/v2.3.1/">山大镜像</a></p>
<p>额额如果是windows直接找到 x64.exe后缀的文件下载安装即可<br />
启动picgo，找到图床设置，选择阿里云OSS： - keyid就是刚刚子用户的id</p>
<ul>
<li><p>keysecret就是他的key</p></li>
<li><p>bucket是bucket名字</p></li>
<li><p>存储区域在</p>
<p><img
src="https://szfmsmdx.oss-cn-hangzhou.aliyuncs.com/blog_pic/202404251802510.png" /></p></li>
<li><p>存储路径就是你这个bucket下的文件夹，如果没有的话直接就存在bucket里面了</p></li>
</ul>
<p>设置完成可以传一个图片给picgo，然后去阿里的oss刷新一下看看是否成功上传上去了</p>
<h2 id="typora配置">typora配置</h2>
<p>typora下载的资源应该网上一大堆，这个各位善用搜索引擎哈 ^ ^<br />
进入typora点击<code>文件</code>-<code>偏好设置</code>-<code>图像</code></p>
<p><img
src="https://szfmsmdx.oss-cn-hangzhou.aliyuncs.com/blog_pic/202404251800449.png" /></p>
<p>注意要选择上传图片啊！！！</p>
<p>不然好像不行</p>
<p>修改成你自己的picgo的配置即可，然后验证一下看是否能够成功上传即可</p>
<hr />
<p>好了，接下来就是 hexo 三连，测试一下hexo是否能够访问即可<br />
到这里，一个博客总算是大部分都搭好咯（应该吧 hhh</p>
]]></content>
      <categories>
        <category>hexo</category>
      </categories>
      <tags>
        <tag>hexo</tag>
        <tag>git</tag>
        <tag>nodejs</tag>
        <tag>nginx</tag>
      </tags>
  </entry>
</search>
